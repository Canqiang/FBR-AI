{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T02:59:09.527624Z",
     "start_time": "2025-07-31T02:59:08.295408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.causal_inference.causal_engine import FBRUSCausalAnalyzer,logger\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ],
   "id": "3c3a123e3e2111b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T02:59:40.010890Z",
     "start_time": "2025-07-31T02:59:16.067453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ä¸»ç¨‹åº\n",
    "# def main():\n",
    "\"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "# å®šä¹‰é—¨åº—ä½ç½®ï¼ˆç¾å›½ä¸»è¦åŸå¸‚çš„é‚®ç¼–ï¼‰\n",
    "store_locations = [\n",
    "    {'name': 'FBR NYC Manhattan', 'postal_code': '10001'},     # çº½çº¦æ›¼å“ˆé¡¿\n",
    "    {'name': 'FBR LA Downtown', 'postal_code': '90012'},       # æ´›æ‰çŸ¶å¸‚ä¸­å¿ƒ\n",
    "    {'name': 'FBR Chicago Loop', 'postal_code': '60601'},      # èŠåŠ å“¥\n",
    "    {'name': 'FBR SF Financial', 'postal_code': '94105'},      # æ—§é‡‘å±±é‡‘èåŒº\n",
    "    {'name': 'FBR Miami Beach', 'postal_code': '33139'}        # è¿ˆé˜¿å¯†æµ·æ»©\n",
    "]\n",
    "\n",
    "# åˆ›å»ºåˆ†æå™¨\n",
    "analyzer = FBRUSCausalAnalyzer(store_locations)\n",
    "\n",
    "try:\n",
    "    # 1. åŠ è½½æˆ–ç”Ÿæˆé”€å”®æ•°æ®\n",
    "    sales_data = analyzer.load_sales_data()  # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®\n",
    "    logger.info(f\"é”€å”®æ•°æ®å½¢çŠ¶: {sales_data.shape}\")\n",
    "\n",
    "    # 2. è·å–å¤©æ°”æ•°æ®\n",
    "    weather_data = analyzer.fetch_weather_data()\n",
    "    logger.info(f\"å¤©æ°”æ•°æ®å½¢çŠ¶: {weather_data.shape if weather_data is not None else 'None'}\")\n",
    "\n",
    "    # 3. è·å–èŠ‚å‡æ—¥æ•°æ®\n",
    "    holiday_data = analyzer.fetch_holiday_data()\n",
    "    logger.info(f\"èŠ‚å‡æ—¥æ•°æ®å½¢çŠ¶: {holiday_data.shape if holiday_data is not None else 'None'}\")\n",
    "\n",
    "    # 4. åˆå¹¶æ‰€æœ‰æ•°æ®\n",
    "    merged_data = analyzer.merge_all_data()\n",
    "    logger.info(f\"åˆå¹¶æ•°æ®å½¢çŠ¶: {merged_data.shape}\")\n",
    "\n",
    "    # 5. æ‰§è¡Œå› æœåˆ†æ\n",
    "    causal_results = analyzer.perform_causal_analysis()\n",
    "\n",
    "    # 6. æ‰§è¡Œåäº‹å®åˆ†æ\n",
    "    counterfactual_results = analyzer.perform_counterfactual_analysis()\n",
    "\n",
    "    # 7. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š\n",
    "    analyzer.visualize_results(causal_results, counterfactual_results)\n",
    "\n",
    "    # 8. æ‰“å°ä¸€äº›å…³é”®ç»“æœ\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FBRç¾å›½é—¨åº—å› æœåˆ†æ - å…³é”®å‘ç°\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # æ‰“å°ä¿ƒé”€æ•ˆæœ\n",
    "    if 'promotion_effect' in causal_results and 'average_effect' in causal_results['promotion_effect']:\n",
    "        print(f\"\\nğŸ“Š ä¿ƒé”€æ•ˆæœï¼š{causal_results['promotion_effect']['interpretation']}\")\n",
    "\n",
    "    # æ‰“å°æœ€ä¼˜æ¡ä»¶\n",
    "    if 'optimal_conditions' in counterfactual_results:\n",
    "        print(f\"\\nğŸ¯ æœ€ä¼˜é”€å”®æ¡ä»¶å°†å¸¦æ¥ï¼š${counterfactual_results['optimal_conditions']['expected_sales']:.0f} çš„é”€å”®é¢\")\n",
    "\n",
    "    # æ‰“å°ä¸»è¦é£é™©\n",
    "    if 'risk_scenarios' in counterfactual_results and counterfactual_results['risk_scenarios']:\n",
    "        top_risk = counterfactual_results['risk_scenarios'][0]\n",
    "        print(f\"\\nâš ï¸  æœ€å¤§é£é™©ï¼š{top_risk['scenario']} - é¢„è®¡æŸå¤± ${top_risk['predicted_loss']:.0f}\")\n",
    "\n",
    "    print(\"\\nâœ… åˆ†æå®Œæˆï¼\")\n",
    "    print(\"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶ï¼š\")\n",
    "    print(\"   - å¯è§†åŒ–æŠ¥å‘Š: fbr_us_causal_analysis_report.html\")\n",
    "    print(\"   - æ–‡å­—æŠ¥å‘Š: fbr_us_causal_analysis_report.txt\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# return analyzer, causal_results, counterfactual_results\n",
    "#\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     # è¿è¡Œåˆ†æ\n",
    "#     analyzer, causal_results, counterfactual_results = main()"
   ],
   "id": "e180f1658911d664",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 10:59:16,074 - INFO - åŠ è½½é”€å”®æ•°æ®...\n",
      "2025-07-31 10:59:16,076 - INFO - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...\n",
      "2025-07-31 10:59:16,086 - INFO - ç”Ÿæˆäº† 1830 æ¡é”€å”®è®°å½•\n",
      "2025-07-31 10:59:16,087 - INFO - é”€å”®æ•°æ®å½¢çŠ¶: (1830, 10)\n",
      "2025-07-31 10:59:16,087 - INFO - è·å–å¤©æ°”æ•°æ®...\n",
      "2025-07-31 10:59:16,090 - INFO - Generating weather features for 5 postal codes.\n",
      "2025-07-31 10:59:25,390 - INFO - Finished creating weather features. Total rows: 1830\n",
      "2025-07-31 10:59:25,401 - INFO - è·å–äº† 1830 æ¡å¤©æ°”è®°å½•\n",
      "2025-07-31 10:59:25,401 - INFO - å¤©æ°”æ•°æ®å½¢çŠ¶: (1830, 18)\n",
      "2025-07-31 10:59:25,402 - INFO - è·å–èŠ‚å‡æ—¥æ•°æ®...\n",
      "2025-07-31 10:59:25,405 - INFO - Generating holiday features for 5 postal codes.\n",
      "2025-07-31 10:59:25,537 - INFO - Finished creating holiday features. Total rows: 1830\n",
      "2025-07-31 10:59:25,540 - INFO - è·å–äº† 1830 æ¡èŠ‚å‡æ—¥è®°å½•\n",
      "2025-07-31 10:59:25,541 - INFO - èŠ‚å‡æ—¥æ•°æ®å½¢çŠ¶: (1830, 9)\n",
      "2025-07-31 10:59:25,541 - INFO - åˆå¹¶æ•°æ®...\n",
      "2025-07-31 10:59:25,549 - INFO - åˆå¹¶åæ•°æ®é›†åŒ…å« 1830 æ¡è®°å½•ï¼Œ32 ä¸ªç‰¹å¾\n",
      "2025-07-31 10:59:25,549 - INFO - åˆå¹¶æ•°æ®å½¢çŠ¶: (1830, 32)\n",
      "2025-07-31 10:59:25,550 - INFO - å¼€å§‹å› æœåˆ†æ...\n",
      "2025-07-31 10:59:25,550 - INFO - åˆ†æå¤©æ°”å› æœæ•ˆåº”...\n",
      "2025-07-31 10:59:25,551 - WARNING - é›¨å¤©æ•°æ®ä¸è¶³ï¼Œè·³è¿‡é›¨å¤©åˆ†æ\n",
      "2025-07-31 10:59:25,551 - INFO - åˆ†æä¿ƒé”€å› æœæ•ˆåº”...\n",
      "2025-07-31 10:59:25,552 - WARNING - Causal Graph not provided. DoWhy will construct a graph based on data inputs.\n",
      "2025-07-31 10:59:25,552 - INFO - Model to find the causal effect of treatment ['has_promotion'] on outcome ['sales_revenue']\n",
      "2025-07-31 10:59:25,553 - WARNING - There are an additional 27 variables in the dataset that are not in the graph. Variable names are: '['date', 'days_since_last_holiday', 'days_until_next_holiday', 'discount_rate', 'holiday_name', 'inventory_level', 'is_extreme_temp', 'is_extreme_weather', 'is_major_holiday', 'is_shopping_season', 'is_snowy', 'is_weekend', 'is_windy', 'orders_count', 'postal_code', 'precipitation_sum', 'snowfall_sum', 'state', 'store_name', 'temp_avg_f', 'temp_max_f', 'temp_min_f', 'temperature_2m_max', 'temperature_2m_min', 'temperature_avg', 'weather_code', 'wind_speed_10m_max']'\n",
      "2025-07-31 10:59:25,554 - INFO - Causal effect can be identified.\n",
      "2025-07-31 10:59:25,555 - INFO - Instrumental variables for treatment and outcome:[]\n",
      "2025-07-31 10:59:25,555 - INFO - Frontdoor variables for treatment and outcome:[]\n",
      "2025-07-31 10:59:25,556 - INFO - Number of general adjustment sets found: 1\n",
      "2025-07-31 10:59:25,556 - INFO - Causal effect can be identified.\n",
      "2025-07-31 10:59:25,557 - INFO - propensity_score_matching\n",
      "2025-07-31 10:59:25,558 - INFO - INFO: Using Propensity Score Matching Estimator\n",
      "2025-07-31 10:59:25,566 - INFO - b: sales_revenue~has_promotion+is_holiday+is_rainy+day_of_week\n",
      "2025-07-31 10:59:25,583 - INFO - åˆ†æèŠ‚å‡æ—¥å› æœæ•ˆåº”...\n",
      "2025-07-31 10:59:25,590 - INFO - åˆ†æç»„åˆå› æœæ•ˆåº”...\n",
      "2025-07-31 10:59:25,607 - INFO - å¼€å§‹åäº‹å®åˆ†æ...\n",
      "2025-07-31 10:59:39,939 - INFO - ç”Ÿæˆå¯è§†åŒ–...\n",
      "2025-07-31 10:59:40,009 - INFO - å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜è‡³ fbr_us_causal_analysis_report.html\n",
      "2025-07-31 10:59:40,009 - INFO - æ–‡å­—æŠ¥å‘Šå·²ä¿å­˜è‡³ fbr_us_causal_analysis_report.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "åˆ†æå®Œæˆï¼\n",
      "==================================================\n",
      "\n",
      "# FBRç¾å›½é—¨åº—é”€å”®æ•°æ®å› æœåˆ†ææŠ¥å‘Š\n",
      "\n",
      "ç”Ÿæˆæ—¶é—´ï¼š2025-07-31 10:59:40\n",
      "\n",
      "## ä¸€ã€æ‰§è¡Œæ‘˜è¦\n",
      "\n",
      "æœ¬æŠ¥å‘ŠåŸºäºFBRç¾å›½é—¨åº—çš„é”€å”®æ•°æ®ï¼Œç»“åˆå¤©æ°”å’ŒèŠ‚å‡æ—¥ä¿¡æ¯ï¼Œè¿›è¡Œäº†æ·±å…¥çš„å› æœåˆ†æã€‚\n",
      "\n",
      "### é—¨åº—è¦†ç›–ï¼š\n",
      "- FBR NYC Manhattan (é‚®ç¼–: 10001)\n",
      "- FBR LA Downtown (é‚®ç¼–: 90012)\n",
      "- FBR Chicago Loop (é‚®ç¼–: 60601)\n",
      "- FBR SF Financial (é‚®ç¼–: 94105)\n",
      "- FBR Miami Beach (é‚®ç¼–: 33139)\n",
      "\n",
      "### å…³é”®å‘ç°ï¼š\n",
      "\n",
      "\n",
      "2. **ä¿ƒé”€æ•ˆæœ**ï¼š\n",
      "   - ä¿ƒé”€å¹³å‡æå‡é”€å”®é¢ $10504ï¼ŒROIçº¦ä¸º -14.6%\n",
      "   - å¼‚è´¨æ€§æ•ˆåº”ï¼š\n",
      "     * å·¥ä½œæ—¥: $7389\n",
      "     * å‘¨æœ«: $8609\n",
      "     * æ™´å¤©: $7517\n",
      "\n",
      "3. **èŠ‚å‡æ—¥æ•ˆåº”**ï¼š\n",
      "   - èŠ‚å‡æ—¥æ•´ä½“æå‡é”€å”®é¢ $nan\n",
      "   - è´­ç‰©å­£æ•ˆåº”ï¼šè´­ç‰©å­£é”€å”®é¢å¹³å‡å‡å°‘ $nan\n",
      "\n",
      "## äºŒã€åäº‹å®åˆ†æ\n",
      "\n",
      "### åœºæ™¯åˆ†æï¼š\n",
      "\n",
      "**å®Œç¾ä¿ƒé”€æ—¥ï¼šèŠ‚å‡æ—¥+å¥½å¤©æ°”+ä¿ƒé”€**\n",
      "- é¢„æµ‹é”€å”®é¢ï¼š$57491\n",
      "- ç›¸æ¯”å¹³å‡å€¼ï¼š$+2997\n",
      "\n",
      "**æœ€å·®æƒ…å†µï¼šæç«¯å¤©æ°”+æ— ä¿ƒé”€**\n",
      "- é¢„æµ‹é”€å”®é¢ï¼š$50695\n",
      "- ç›¸æ¯”å¹³å‡å€¼ï¼š$-3798\n",
      "\n",
      "**é›¨å¤©ä¿ƒé”€ç­–ç•¥**\n",
      "- é¢„æµ‹é”€å”®é¢ï¼š$57491\n",
      "- ç›¸æ¯”å¹³å‡å€¼ï¼š$+2997\n",
      "\n",
      "**é»‘è‰²æ˜ŸæœŸäº”åœºæ™¯**\n",
      "- é¢„æµ‹é”€å”®é¢ï¼š$58168\n",
      "- ç›¸æ¯”å¹³å‡å€¼ï¼š$+3675\n",
      "\n",
      "**æ™®é€šå·¥ä½œæ—¥**\n",
      "- é¢„æµ‹é”€å”®é¢ï¼š$49648\n",
      "- ç›¸æ¯”å¹³å‡å€¼ï¼š$-4845\n",
      "\n",
      "### æœ€ä¼˜è¿è¥æ¡ä»¶ï¼š\n",
      "- é¢„æœŸé”€å”®é¢ï¼š$69857\n",
      "- å»ºè®®ï¼š\n",
      "  * å®æ–½ä¿ƒé”€æ´»åŠ¨\n",
      "  * å……åˆ†åˆ©ç”¨èŠ‚å‡æ—¥æµé‡\n",
      "  * æœ€ä½³é”€å”®æ—¥æ˜¯å‘¨å…­\n",
      "  * ç†æƒ³æ¸©åº¦çº¦70Â°F\n",
      "\n",
      "### é£é™©åœºæ™¯é¢„è­¦ï¼š\n",
      "\n",
      "- **è¿ç»­é›¨å¤©+å·¥ä½œæ—¥**\n",
      "  * é¢„è®¡æŸå¤±ï¼š$4845\n",
      "  * é£é™©ç­‰çº§ï¼šä¸­\n",
      "  * ç¼“è§£æªæ–½ï¼šé›¨å¤©ä¸“å±ä¼˜æƒ ã€åŠ å¼ºçº¿ä¸Šæ¨å¹¿ã€æ”¹å–„åº—å†…ä½“éªŒ\n",
      "\n",
      "- **æç«¯é«˜æ¸©(>100Â°F)+æ— ä¿ƒé”€**\n",
      "  * é¢„è®¡æŸå¤±ï¼š$3798\n",
      "  * é£é™©ç­‰çº§ï¼šä¸­\n",
      "  * ç¼“è§£æªæ–½ï¼šåŠ å¼ºç©ºè°ƒã€æä¾›å†·é¥®ä¼˜æƒ ã€å»¶é•¿è¥ä¸šæ—¶é—´è‡³æ™šä¸Š\n",
      "\n",
      "- **æš´é£é›ªå¤©æ°”**\n",
      "  * é¢„è®¡æŸå¤±ï¼š$3798\n",
      "  * é£é™©ç­‰çº§ï¼šä¸­\n",
      "  * ç¼“è§£æªæ–½ï¼šæå‰å¤‡è´§ã€åŠ å¼ºå¤–å–æœåŠ¡ã€å‘˜å·¥å®‰å…¨ä¿éšœ\n",
      "\n",
      "\n",
      "## ä¸‰ã€ä¸šåŠ¡å»ºè®®\n",
      "\n",
      "### åŸºäºå› æœåˆ†æçš„è¡ŒåŠ¨å»ºè®®ï¼š\n",
      "\n",
      "1. **å¤©æ°”åº”å¯¹ç­–ç•¥**ï¼š\n",
      "   - å»ºç«‹å¤©æ°”ç›‘æµ‹é¢„è­¦ç³»ç»Ÿï¼Œæå‰3-5å¤©é¢„æµ‹é”€å”®è¶‹åŠ¿\n",
      "   - é›¨å¤©å’Œæç«¯å¤©æ°”æ—¶ï¼ŒåŠ å¼ºå¤–å–/é…é€æœåŠ¡\n",
      "   - åœ¨æœ€ä½³æ¸©åº¦åŒºé—´ï¼ˆèˆ’é€‚å¤©æ°”ï¼‰æ—¶ï¼Œå¯ä»¥ä¸¾åŠæˆ·å¤–ä¿ƒé”€æ´»åŠ¨\n",
      "\n",
      "2. **ä¿ƒé”€ä¼˜åŒ–**ï¼š\n",
      "   - åŸºäºROIåˆ†æï¼Œåœ¨å‘¨æœ«å’ŒèŠ‚å‡æ—¥åŠ å¤§ä¿ƒé”€åŠ›åº¦\n",
      "   - é›¨å¤©ä¿ƒé”€å¯ä»¥æœ‰æ•ˆç¼“è§£å¤©æ°”å¸¦æ¥çš„è´Ÿé¢å½±å“\n",
      "   - å»ºè®®å»ºç«‹åŠ¨æ€ä¿ƒé”€ç³»ç»Ÿï¼Œæ ¹æ®å¤©æ°”å’Œå®¢æµè‡ªåŠ¨è°ƒæ•´\n",
      "\n",
      "3. **èŠ‚å‡æ—¥è¿è¥**ï¼š\n",
      "   - é‡ç‚¹å‡†å¤‡ä¸»è¦èŠ‚å‡æ—¥ï¼ˆæ„Ÿæ©èŠ‚ã€åœ£è¯èŠ‚ç­‰ï¼‰çš„åº“å­˜\n",
      "   - è´­ç‰©å­£ï¼ˆé»‘è‰²æ˜ŸæœŸäº”ã€ç½‘ç»œæ˜ŸæœŸä¸€ï¼‰éœ€è¦ç‰¹åˆ«å‡†å¤‡\n",
      "   - é’ˆå¯¹è¡¨ç°è¾ƒå·®çš„èŠ‚å‡æ—¥ï¼Œè€ƒè™‘ç‰¹æ®Šè¥é”€ç­–ç•¥\n",
      "\n",
      "4. **é£é™©ç®¡ç†**ï¼š\n",
      "   - å»ºç«‹æç«¯å¤©æ°”åº”æ€¥é¢„æ¡ˆ\n",
      "   - ä¼˜åŒ–åº“å­˜ç®¡ç†ï¼Œé¿å…å› å¤©æ°”å¯¼è‡´çš„æŸå¤±\n",
      "   - è€ƒè™‘å¤©æ°”ä¿é™©ç­‰é£é™©å¯¹å†²å·¥å…·\n",
      "\n",
      "## å››ã€æ•°æ®è¯´æ˜\n",
      "\n",
      "- åˆ†ææ—¶é—´èŒƒå›´ï¼šæœ€è¿‘365å¤©\n",
      "- æ•°æ®è®°å½•æ•°ï¼š{len(self.merged_data) if self.merged_data is not ...\n",
      "\n",
      "============================================================\n",
      "FBRç¾å›½é—¨åº—å› æœåˆ†æ - å…³é”®å‘ç°\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä¿ƒé”€æ•ˆæœï¼šä¿ƒé”€å¹³å‡æå‡é”€å”®é¢ $10504ï¼ŒROIçº¦ä¸º -14.6%\n",
      "\n",
      "ğŸ¯ æœ€ä¼˜é”€å”®æ¡ä»¶å°†å¸¦æ¥ï¼š$69857 çš„é”€å”®é¢\n",
      "\n",
      "âš ï¸  æœ€å¤§é£é™©ï¼šè¿ç»­é›¨å¤©+å·¥ä½œæ—¥ - é¢„è®¡æŸå¤± $4845\n",
      "\n",
      "âœ… åˆ†æå®Œæˆï¼\n",
      "ğŸ“„ æŠ¥å‘Šæ–‡ä»¶ï¼š\n",
      "   - å¯è§†åŒ–æŠ¥å‘Š: fbr_us_causal_analysis_report.html\n",
      "   - æ–‡å­—æŠ¥å‘Š: fbr_us_causal_analysis_report.txt\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:00:24.928052Z",
     "start_time": "2025-07-31T03:00:24.909982Z"
    }
   },
   "cell_type": "code",
   "source": "sales_data",
   "id": "406d612ae61e4004",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           date         store_name postal_code  sales_revenue  \\\n",
       "0    2024-07-31 10:59:16.076790  FBR NYC Manhattan       10001   52483.570765   \n",
       "1    2024-08-01 10:59:16.076790  FBR NYC Manhattan       10001   57006.632090   \n",
       "2    2024-08-02 10:59:16.076790  FBR NYC Manhattan       10001   53582.658807   \n",
       "3    2024-08-03 10:59:16.076790  FBR NYC Manhattan       10001   77327.500690   \n",
       "4    2024-08-04 10:59:16.076790  FBR NYC Manhattan       10001   59654.862248   \n",
       "...                         ...                ...         ...            ...   \n",
       "1825 2025-07-27 10:59:16.076790    FBR Miami Beach       33139   46710.318381   \n",
       "1826 2025-07-28 10:59:16.076790    FBR Miami Beach       33139   54410.679070   \n",
       "1827 2025-07-29 10:59:16.076790    FBR Miami Beach       33139   55641.624020   \n",
       "1828 2025-07-30 10:59:16.076790    FBR Miami Beach       33139   45789.909738   \n",
       "1829 2025-07-31 10:59:16.076790    FBR Miami Beach       33139   48882.637474   \n",
       "\n",
       "      orders_count  has_promotion  discount_rate  inventory_level  \\\n",
       "0       602.106902              0       0.000000         0.542270   \n",
       "1       711.497004              1       0.228716         0.982927   \n",
       "2       665.164544              0       0.000000         0.748680   \n",
       "3       980.517886              1       0.246318         0.875736   \n",
       "4       782.664900              0       0.000000         0.392767   \n",
       "...            ...            ...            ...              ...   \n",
       "1825    591.819913              0       0.000000         0.984946   \n",
       "1826    698.766808              1       0.196143         0.711903   \n",
       "1827    703.427774              0       0.000000         0.689866   \n",
       "1828    566.120328              0       0.000000         0.759057   \n",
       "1829    595.113190              0       0.000000         0.647917   \n",
       "\n",
       "      day_of_week  is_weekend  \n",
       "0               2       False  \n",
       "1               3       False  \n",
       "2               4       False  \n",
       "3               5        True  \n",
       "4               6        True  \n",
       "...           ...         ...  \n",
       "1825            6        True  \n",
       "1826            0       False  \n",
       "1827            1       False  \n",
       "1828            2       False  \n",
       "1829            3       False  \n",
       "\n",
       "[1830 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>sales_revenue</th>\n",
       "      <th>orders_count</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>inventory_level</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-31 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>52483.570765</td>\n",
       "      <td>602.106902</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542270</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-01 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>57006.632090</td>\n",
       "      <td>711.497004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228716</td>\n",
       "      <td>0.982927</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-02 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>53582.658807</td>\n",
       "      <td>665.164544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.748680</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-03 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>77327.500690</td>\n",
       "      <td>980.517886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.246318</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-04 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>59654.862248</td>\n",
       "      <td>782.664900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392767</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2025-07-27 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>46710.318381</td>\n",
       "      <td>591.819913</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984946</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>2025-07-28 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>54410.679070</td>\n",
       "      <td>698.766808</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196143</td>\n",
       "      <td>0.711903</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>2025-07-29 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>55641.624020</td>\n",
       "      <td>703.427774</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689866</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>2025-07-30 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>45789.909738</td>\n",
       "      <td>566.120328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.759057</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>2025-07-31 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>48882.637474</td>\n",
       "      <td>595.113190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1830 rows Ã— 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:09:24.733209Z",
     "start_time": "2025-07-31T03:09:24.714555Z"
    }
   },
   "cell_type": "code",
   "source": "holiday_data.groupby(\"is_holiday\").count()",
   "id": "e8f6bdff132dfdd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            date  postal_code  holiday_name  days_since_last_holiday  \\\n",
       "is_holiday                                                             \n",
       "False       1765         1765          1765                     1765   \n",
       "True          65           65            65                       65   \n",
       "\n",
       "            days_until_next_holiday  state  is_major_holiday  \\\n",
       "is_holiday                                                     \n",
       "False                          1765   1765              1765   \n",
       "True                             65     65                65   \n",
       "\n",
       "            is_shopping_season  \n",
       "is_holiday                      \n",
       "False                     1765  \n",
       "True                        65  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>days_since_last_holiday</th>\n",
       "      <th>days_until_next_holiday</th>\n",
       "      <th>state</th>\n",
       "      <th>is_major_holiday</th>\n",
       "      <th>is_shopping_season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_holiday</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:10:06.434299Z",
     "start_time": "2025-07-31T03:10:06.414106Z"
    }
   },
   "cell_type": "code",
   "source": "holiday_data.groupby(\"is_shopping_season\").count()",
   "id": "4a1423b45c1b46cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    date  postal_code  is_holiday  holiday_name  \\\n",
       "is_shopping_season                                                \n",
       "0                   1817         1817        1817          1817   \n",
       "1                     13           13          13            13   \n",
       "\n",
       "                    days_since_last_holiday  days_until_next_holiday  state  \\\n",
       "is_shopping_season                                                            \n",
       "0                                      1817                     1817   1817   \n",
       "1                                        13                       13     13   \n",
       "\n",
       "                    is_major_holiday  \n",
       "is_shopping_season                    \n",
       "0                               1817  \n",
       "1                                 13  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>days_since_last_holiday</th>\n",
       "      <th>days_until_next_holiday</th>\n",
       "      <th>state</th>\n",
       "      <th>is_major_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_shopping_season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:20:41.075143Z",
     "start_time": "2025-07-31T03:20:41.071456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.repositories import OrderRepository,CustomerRepository,AnalyticsRepository\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "56b4dbcfd08ddcf6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:20:51.097506Z",
     "start_time": "2025-07-31T03:20:51.093422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "order_repository = OrderRepository()\n",
    "customer_repository = CustomerRepository()\n",
    "analytics_repository = AnalyticsRepository()"
   ],
   "id": "5fc21558d61e10b8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:27:31.306713Z",
     "start_time": "2025-07-31T03:27:29.877968Z"
    }
   },
   "cell_type": "code",
   "source": "order_items = order_repository.get_daily_sales(start_date=datetime.now() - timedelta(days=1), end_date=datetime.now())",
   "id": "8f173aa703c90ae2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:28:06.279214Z",
     "start_time": "2025-07-31T03:28:06.250634Z"
    }
   },
   "cell_type": "code",
   "source": "order_items.head()",
   "id": "b10cac2c8eca7bf3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date    location_id  order_count  customer_count total_revenue  \\\n",
       "0 2025-07-30  LVTJHAGAMD2ZY           67              55        999.53   \n",
       "1 2025-07-30  LAFZJ3CHGB11Z           45              28        807.16   \n",
       "2 2025-07-30  LWDYYDAMG4AH5           40              30        575.71   \n",
       "3 2025-07-30  LMF86PJ371T9T          248             157       6948.06   \n",
       "4 2025-07-30  L3T37TFC0S7WB          121              93       1650.83   \n",
       "\n",
       "   avg_order_value  new_customer_count  repeat_customer_count  \n",
       "0        11.622442                  28                     27  \n",
       "1        10.482597                  11                     17  \n",
       "2         9.757797                  14                     16  \n",
       "3        14.942065                  38                    119  \n",
       "4         9.597849                  36                     57  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location_id</th>\n",
       "      <th>order_count</th>\n",
       "      <th>customer_count</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>new_customer_count</th>\n",
       "      <th>repeat_customer_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LVTJHAGAMD2ZY</td>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "      <td>999.53</td>\n",
       "      <td>11.622442</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LAFZJ3CHGB11Z</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>807.16</td>\n",
       "      <td>10.482597</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LWDYYDAMG4AH5</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>575.71</td>\n",
       "      <td>9.757797</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LMF86PJ371T9T</td>\n",
       "      <td>248</td>\n",
       "      <td>157</td>\n",
       "      <td>6948.06</td>\n",
       "      <td>14.942065</td>\n",
       "      <td>38</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>L3T37TFC0S7WB</td>\n",
       "      <td>121</td>\n",
       "      <td>93</td>\n",
       "      <td>1650.83</td>\n",
       "      <td>9.597849</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T06:05:22.899683Z",
     "start_time": "2025-07-31T06:05:14.313278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "å› æœæ¨æ–­æ•°æ®å‡†å¤‡è„šæœ¬\n",
    "åˆ©ç”¨ç°æœ‰çš„connectorså’Œrepositorieså‡†å¤‡å› æœæ¨æ–­æ‰€éœ€çš„æ•°æ®\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "# project_root = Path(__file__).parent.parent\n",
    "# sys.path.append(str(project_root))\n",
    "\n",
    "from src.data.repositories import OrderRepository, CustomerRepository\n",
    "from src.data.connectors import ClickHouseConnector\n",
    "from config.settings import Settings\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CausalDataPreparator:\n",
    "    \"\"\"å› æœæ¨æ–­æ•°æ®å‡†å¤‡å™¨\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = Settings()\n",
    "        self.ch_connector = ClickHouseConnector()\n",
    "        self.order_repo = OrderRepository()\n",
    "        self.customer_repo = CustomerRepository()\n",
    "\n",
    "    def prepare_causal_dataset(self, days_back: int = 180) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        å‡†å¤‡å› æœæ¨æ–­æ•°æ®é›†\n",
    "\n",
    "        Args:\n",
    "            days_back: å›æº¯å¤©æ•°ï¼Œé»˜è®¤180å¤©\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: å‡†å¤‡å¥½çš„æ•°æ®é›†\n",
    "        \"\"\"\n",
    "        logger.info(f\"å¼€å§‹å‡†å¤‡å› æœæ¨æ–­æ•°æ®é›†ï¼Œå›æº¯{days_back}å¤©\")\n",
    "\n",
    "        try:\n",
    "            # 1. è·å–åŸºç¡€æ•°æ®\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=days_back)\n",
    "\n",
    "            # è·å–é”€å”®æ•°æ®\n",
    "            logger.info(\"è·å–é”€å”®æ•°æ®...\")\n",
    "            sales_data = self._get_sales_data(start_date, end_date)\n",
    "\n",
    "            if sales_data.empty:\n",
    "                logger.error(\"æ— æ³•è·å–é”€å”®æ•°æ®ï¼Œæµç¨‹ç»ˆæ­¢\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # è·å–å®¢æˆ·æ•°æ®\n",
    "            logger.info(\"è·å–å®¢æˆ·æ•°æ®...\")\n",
    "            customer_data = self._get_customer_data()\n",
    "\n",
    "            # è·å–å•†å“æ•°æ®\n",
    "            logger.info(\"è·å–å•†å“æ•°æ®...\")\n",
    "            item_data = self._get_item_data()\n",
    "\n",
    "            # 2. æ•°æ®åˆå¹¶å’Œæ¸…ç†\n",
    "            logger.info(\"åˆå¹¶å’Œæ¸…ç†æ•°æ®...\")\n",
    "            merged_data = self._merge_and_clean_data(sales_data, customer_data, item_data)\n",
    "\n",
    "            if merged_data.empty:\n",
    "                logger.error(\"æ•°æ®åˆå¹¶åä¸ºç©ºï¼Œæµç¨‹ç»ˆæ­¢\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # 3. åˆ›å»ºå¤„ç†å˜é‡\n",
    "            logger.info(\"åˆ›å»ºå¤„ç†å˜é‡...\")\n",
    "            try:\n",
    "                merged_data = self._create_treatment_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"åˆ›å»ºå¤„ç†å˜é‡å¤±è´¥: {e}\")\n",
    "                # åˆ›å»ºæœ€åŸºæœ¬çš„å¤„ç†å˜é‡\n",
    "                merged_data['has_promotion'] = 0\n",
    "                merged_data['has_discount'] = 0\n",
    "                merged_data['vip_treatment'] = 0\n",
    "\n",
    "            # 4. åˆ›å»ºæ··æ·†å˜é‡\n",
    "            logger.info(\"åˆ›å»ºæ··æ·†å˜é‡...\")\n",
    "            try:\n",
    "                merged_data = self._create_confounding_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"åˆ›å»ºæ··æ·†å˜é‡å¤±è´¥: {e}\")\n",
    "                # åˆ›å»ºæœ€åŸºæœ¬çš„æ··æ·†å˜é‡\n",
    "                merged_data['is_weekend'] = 0\n",
    "                merged_data['hour_of_day'] = 12\n",
    "                merged_data['location_encoded'] = 0\n",
    "                merged_data['category_encoded'] = 0\n",
    "\n",
    "            # 5. åˆ›å»ºç»“æœå˜é‡\n",
    "            logger.info(\"åˆ›å»ºç»“æœå˜é‡...\")\n",
    "            try:\n",
    "                merged_data = self._create_outcome_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"åˆ›å»ºç»“æœå˜é‡å¤±è´¥: {e}\")\n",
    "                # ç¡®ä¿è‡³å°‘æœ‰åŸºæœ¬çš„ç»“æœå˜é‡\n",
    "                if 'revenue' in merged_data.columns:\n",
    "                    merged_data['log_revenue'] = np.log1p(merged_data['revenue'])\n",
    "\n",
    "            # 6. æœ€ç»ˆæ•°æ®éªŒè¯\n",
    "            logger.info(\"æ•°æ®éªŒè¯...\")\n",
    "            validated_data = self._validate_data(merged_data)\n",
    "\n",
    "            if not validated_data.empty:\n",
    "                logger.info(f\"æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…±{len(validated_data)}æ¡è®°å½•\")\n",
    "            else:\n",
    "                logger.error(\"æ•°æ®éªŒè¯åä¸ºç©º\")\n",
    "\n",
    "            return validated_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _get_sales_data(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"è·å–é”€å”®æ•°æ®\"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ç°æœ‰çš„repositoryæ–¹æ³•\n",
    "            sales_data = self.order_repo.get_daily_sales(start_date, end_date)\n",
    "\n",
    "            # æ‰“å°è°ƒè¯•ä¿¡æ¯\n",
    "            if not sales_data.empty:\n",
    "                logger.info(f\"ä»repositoryè·å–åˆ°æ•°æ®ï¼Œå½¢çŠ¶: {sales_data.shape}\")\n",
    "                logger.info(f\"åˆ—å: {list(sales_data.columns)}\")\n",
    "                sales_data = self._standardize_column_names(sales_data)\n",
    "            else:\n",
    "                logger.info(\"Repositoryè¿”å›ç©ºæ•°æ®ï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢...\")\n",
    "                # å°è¯•å¤šç§å¯èƒ½çš„æŸ¥è¯¢\n",
    "                sales_data = self._try_direct_queries(start_date, end_date)\n",
    "\n",
    "            return sales_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è·å–é”€å”®æ•°æ®å¤±è´¥: {e}\")\n",
    "            logger.info(\"å°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...\")\n",
    "            return self._create_sample_data(start_date, end_date)\n",
    "\n",
    "    def _try_direct_queries(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"å°è¯•ä¸åŒçš„ç›´æ¥æŸ¥è¯¢æ–¹å¼\"\"\"\n",
    "\n",
    "        # æŸ¥è¯¢1ï¼šå®Œæ•´å­—æ®µæŸ¥è¯¢\n",
    "        queries = [\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "                customer_id,\n",
    "                item_name,\n",
    "                category_name,\n",
    "                location_id,\n",
    "                order_final_total_amt as revenue,\n",
    "                quantity,\n",
    "                toDate(created_at_pt) as date,\n",
    "                toHour(created_at_pt) as hour\n",
    "            FROM dw.fact_order_item_variations\n",
    "            WHERE created_at_pt >= '%(start_date)s'\n",
    "                AND created_at_pt < '%(end_date)s'\n",
    "                AND pay_status = 'COMPLETED'\n",
    "            LIMIT 1000\n",
    "            \"\"\",\n",
    "\n",
    "            # æŸ¥è¯¢2ï¼šç®€åŒ–å­—æ®µæŸ¥è¯¢\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM dw.fact_order_item_variations\n",
    "            WHERE created_at_pt >= '%(start_date)s'\n",
    "                AND created_at_pt < '%(end_date)s'\n",
    "            LIMIT 10\n",
    "            \"\"\",\n",
    "\n",
    "            # æŸ¥è¯¢3ï¼šè¡¨ç»“æ„æŸ¥è¯¢\n",
    "            \"\"\"\n",
    "            DESCRIBE TABLE dw.fact_order_item_variations\n",
    "            \"\"\"\n",
    "        ]\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "            try:\n",
    "                logger.info(f\"å°è¯•æŸ¥è¯¢{i+1}...\")\n",
    "                formatted_query = query % {\n",
    "                    'start_date': start_date.strftime('%Y-%m-%d'),\n",
    "                    'end_date': end_date.strftime('%Y-%m-%d')\n",
    "                }\n",
    "\n",
    "                result = self.ch_connector.query_df(formatted_query)\n",
    "\n",
    "                if not result.empty:\n",
    "                    logger.info(f\"æŸ¥è¯¢{i+1}æˆåŠŸï¼Œå½¢çŠ¶: {result.shape}\")\n",
    "                    logger.info(f\"åˆ—å: {list(result.columns)}\")\n",
    "\n",
    "                    if i == 2:  # è¡¨ç»“æ„æŸ¥è¯¢\n",
    "                        logger.info(\"è¡¨ç»“æ„:\")\n",
    "                        for _, row in result.iterrows():\n",
    "                            logger.info(f\"  {row}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        return self._standardize_column_names(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"æŸ¥è¯¢{i+1}å¤±è´¥: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def _standardize_column_names(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ ‡å‡†åŒ–åˆ—å\"\"\"\n",
    "        # åˆ—åæ˜ å°„å­—å…¸\n",
    "        column_mapping = {\n",
    "            # customerç›¸å…³\n",
    "            'customerId': 'customer_id',\n",
    "            'customer': 'customer_id',\n",
    "            'user_id': 'customer_id',\n",
    "            'userId': 'customer_id',\n",
    "\n",
    "            # itemç›¸å…³\n",
    "            'itemName': 'item_name',\n",
    "            'product_name': 'item_name',\n",
    "            'productName': 'item_name',\n",
    "\n",
    "            # categoryç›¸å…³\n",
    "            'categoryName': 'category_name',\n",
    "            'category': 'category_name',\n",
    "\n",
    "            # locationç›¸å…³\n",
    "            'locationId': 'location_id',\n",
    "            'location': 'location_id',\n",
    "            'store_id': 'location_id',\n",
    "\n",
    "            # revenueç›¸å…³\n",
    "            'total_amount': 'revenue',\n",
    "            'amount': 'revenue',\n",
    "            'order_amount': 'revenue',\n",
    "            'order_final_total_amt': 'revenue',\n",
    "\n",
    "            # dateç›¸å…³\n",
    "            'order_date': 'date',\n",
    "            'created_date': 'date',\n",
    "            'purchase_date': 'date'\n",
    "        }\n",
    "\n",
    "        # åº”ç”¨æ˜ å°„\n",
    "        data = data.rename(columns=column_mapping)\n",
    "\n",
    "        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰customer_idï¼Œå°è¯•åˆ›å»º\n",
    "        if 'customer_id' not in data.columns:\n",
    "            # å¯»æ‰¾å¯èƒ½çš„IDåˆ—\n",
    "            id_columns = [col for col in data.columns if 'id' in col.lower()]\n",
    "            if id_columns:\n",
    "                logger.info(f\"æœªæ‰¾åˆ°customer_idï¼Œä½¿ç”¨ {id_columns[0]} ä½œä¸ºcustomer_id\")\n",
    "                data['customer_id'] = data[id_columns[0]]\n",
    "            else:\n",
    "                logger.warning(\"æœªæ‰¾åˆ°ä»»ä½•å®¢æˆ·IDåˆ—ï¼Œåˆ›å»ºè™šæ‹Ÿcustomer_id\")\n",
    "                # åˆ›å»ºè™šæ‹Ÿcustomer_id\n",
    "                data['customer_id'] = 'CUST_' + pd.Series(range(len(data))).astype(str)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_sample_data(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•\"\"\"\n",
    "        logger.info(\"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...\")\n",
    "\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # ç”Ÿæˆæ—¥æœŸåºåˆ—\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "        # ä¸ºæ¯å¤©ç”Ÿæˆå¤šæ¡è®°å½•\n",
    "        data_list = []\n",
    "        customer_ids = [f'CUST_{str(i).zfill(4)}' for i in range(1, 101)]  # 100ä¸ªå®¢æˆ·\n",
    "        items = ['å•†å“A', 'å•†å“B', 'å•†å“C', 'å•†å“D', 'å•†å“E']\n",
    "        categories = ['ç”µå­äº§å“', 'æœè£…', 'é£Ÿå“', 'å®¶å±…', 'å›¾ä¹¦']\n",
    "        locations = ['åº—é“º1', 'åº—é“º2', 'åº—é“º3']\n",
    "\n",
    "        for date in date_range:\n",
    "            # æ¯å¤©éšæœºç”Ÿæˆ10-50æ¡è®°å½•\n",
    "            daily_records = np.random.randint(10, 51)\n",
    "\n",
    "            for _ in range(daily_records):\n",
    "                data_list.append({\n",
    "                    'customer_id': np.random.choice(customer_ids),\n",
    "                    'item_name': np.random.choice(items),\n",
    "                    'category_name': np.random.choice(categories),\n",
    "                    'location_id': np.random.choice(locations),\n",
    "                    'revenue': np.random.lognormal(4, 0.5),  # å¯¹æ•°æ­£æ€åˆ†å¸ƒç”Ÿæˆä»·æ ¼\n",
    "                    'quantity': np.random.randint(1, 5),\n",
    "                    'date': date.date(),\n",
    "                    'hour': np.random.randint(9, 22)  # è¥ä¸šæ—¶é—´9-22ç‚¹\n",
    "                })\n",
    "\n",
    "        sample_data = pd.DataFrame(data_list)\n",
    "        logger.info(f\"åˆ›å»ºäº† {len(sample_data)} æ¡æ¨¡æ‹Ÿæ•°æ®\")\n",
    "\n",
    "        return sample_data\n",
    "\n",
    "    def _get_customer_data(self) -> pd.DataFrame:\n",
    "        \"\"\"è·å–å®¢æˆ·æ•°æ®\"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ç°æœ‰çš„repositoryæ–¹æ³•\n",
    "            customer_data = self.customer_repo.get_customer_segments()\n",
    "\n",
    "            if not customer_data.empty:\n",
    "                logger.info(f\"ä»repositoryè·å–å®¢æˆ·æ•°æ®ï¼Œå½¢çŠ¶: {customer_data.shape}\")\n",
    "                logger.info(f\"å®¢æˆ·æ•°æ®åˆ—å: {list(customer_data.columns)}\")\n",
    "                customer_data = self._standardize_column_names(customer_data)\n",
    "            else:\n",
    "                logger.info(\"Repositoryè¿”å›ç©ºå®¢æˆ·æ•°æ®ï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢...\")\n",
    "                # å°è¯•ç›´æ¥æŸ¥è¯¢å®¢æˆ·ä¿¡æ¯\n",
    "                queries = [\n",
    "                    \"\"\"\n",
    "                    SELECT\n",
    "                        customer_id,\n",
    "                        CASE\n",
    "                            WHEN high_value_customer = 1 THEN 'high_value'\n",
    "                            WHEN key_development_customer = 1 THEN 'key_development'\n",
    "                            WHEN regular_customer = 1 THEN 'regular'\n",
    "                            WHEN critical_win_back_customer = 1 THEN 'win_back'\n",
    "                            ELSE 'other'\n",
    "                        END as customer_segment,\n",
    "                        order_final_total_amt as lifetime_value,\n",
    "                        order_final_avg_amt as avg_order_value,\n",
    "                        order_final_total_cnt as total_orders,\n",
    "                        CASE WHEN high_value_customer = 1 THEN 1 ELSE 0 END as is_high_value\n",
    "                    FROM ads.customer_profile\n",
    "                    WHERE order_final_total_cnt > 0\n",
    "                    LIMIT 1000\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    SELECT * FROM ads.customer_profile LIMIT 10\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    DESCRIBE TABLE ads.customer_profile\n",
    "                    \"\"\"\n",
    "                ]\n",
    "\n",
    "                for i, query in enumerate(queries):\n",
    "                    try:\n",
    "                        logger.info(f\"å°è¯•å®¢æˆ·æ•°æ®æŸ¥è¯¢{i+1}...\")\n",
    "                        result = self.ch_connector.query_df(query)\n",
    "\n",
    "                        if not result.empty:\n",
    "                            logger.info(f\"å®¢æˆ·æ•°æ®æŸ¥è¯¢{i+1}æˆåŠŸï¼Œå½¢çŠ¶: {result.shape}\")\n",
    "                            logger.info(f\"åˆ—å: {list(result.columns)}\")\n",
    "\n",
    "                            if i == 2:  # è¡¨ç»“æ„æŸ¥è¯¢\n",
    "                                logger.info(\"å®¢æˆ·è¡¨ç»“æ„:\")\n",
    "                                for _, row in result.iterrows():\n",
    "                                    logger.info(f\"  {row}\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                customer_data = self._standardize_column_names(result)\n",
    "                                break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"å®¢æˆ·æ•°æ®æŸ¥è¯¢{i+1}å¤±è´¥: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºDataFrame\n",
    "            if customer_data.empty:\n",
    "                logger.warning(\"æ— æ³•è·å–å®¢æˆ·æ•°æ®ï¼Œå°†åœ¨åç»­æ­¥éª¤ä¸­åˆ›å»ºé»˜è®¤å®¢æˆ·ä¿¡æ¯\")\n",
    "\n",
    "            return customer_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è·å–å®¢æˆ·æ•°æ®å¤±è´¥: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _get_item_data(self) -> pd.DataFrame:\n",
    "        \"\"\"è·å–å•†å“æ•°æ®\"\"\"\n",
    "        try:\n",
    "            # è·å–å•†å“è¡¨ç°æ•°æ®\n",
    "            item_data = self.order_repo.get_item_performance(days=30)\n",
    "\n",
    "            if not item_data.empty:\n",
    "                logger.info(f\"ä»repositoryè·å–å•†å“æ•°æ®ï¼Œå½¢çŠ¶: {item_data.shape}\")\n",
    "                logger.info(f\"å•†å“æ•°æ®åˆ—å: {list(item_data.columns)}\")\n",
    "                item_data = self._standardize_column_names(item_data)\n",
    "            else:\n",
    "                logger.info(\"Repositoryè¿”å›ç©ºå•†å“æ•°æ®ï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢...\")\n",
    "                # ç›´æ¥æŸ¥è¯¢å•†å“ä¿¡æ¯\n",
    "                queries = [\n",
    "                    \"\"\"\n",
    "                    SELECT\n",
    "                        item_name,\n",
    "                        category_name,\n",
    "                        AVG(order_final_total_amt) as avg_item_revenue,\n",
    "                        COUNT(DISTINCT customer_id) as unique_buyers,\n",
    "                        SUM(quantity) as total_quantity\n",
    "                    FROM dw.fact_order_item_variations\n",
    "                    WHERE created_at_pt >= today() - 30\n",
    "                        AND pay_status = 'COMPLETED'\n",
    "                    GROUP BY item_name, category_name\n",
    "                    LIMIT 100\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    SELECT DISTINCT item_name, category_name\n",
    "                    FROM dw.fact_order_item_variations\n",
    "                    LIMIT 50\n",
    "                    \"\"\"\n",
    "                ]\n",
    "\n",
    "                for i, query in enumerate(queries):\n",
    "                    try:\n",
    "                        logger.info(f\"å°è¯•å•†å“æ•°æ®æŸ¥è¯¢{i+1}...\")\n",
    "                        result = self.ch_connector.query_df(query)\n",
    "\n",
    "                        if not result.empty:\n",
    "                            logger.info(f\"å•†å“æ•°æ®æŸ¥è¯¢{i+1}æˆåŠŸï¼Œå½¢çŠ¶: {result.shape}\")\n",
    "                            item_data = self._standardize_column_names(result)\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"å•†å“æ•°æ®æŸ¥è¯¢{i+1}å¤±è´¥: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return item_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è·å–å•†å“æ•°æ®å¤±è´¥: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _merge_and_clean_data(self, sales_data: pd.DataFrame,\n",
    "                             customer_data: pd.DataFrame,\n",
    "                             item_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"åˆå¹¶å’Œæ¸…ç†æ•°æ®\"\"\"\n",
    "        if sales_data.empty:\n",
    "            logger.warning(\"é”€å”®æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        logger.info(f\"å¼€å§‹åˆå¹¶æ•°æ® - é”€å”®æ•°æ®: {sales_data.shape}\")\n",
    "        merged_data = sales_data.copy()\n",
    "\n",
    "        # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨\n",
    "        if 'customer_id' not in merged_data.columns:\n",
    "            logger.error(\"é”€å”®æ•°æ®ä¸­ç¼ºå°‘customer_idå­—æ®µ\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if 'revenue' not in merged_data.columns:\n",
    "            # å°è¯•å¯»æ‰¾æ”¶å…¥ç›¸å…³å­—æ®µ\n",
    "            revenue_cols = [col for col in merged_data.columns if any(keyword in col.lower() for keyword in ['amount', 'revenue', 'total', 'price'])]\n",
    "            if revenue_cols:\n",
    "                merged_data['revenue'] = merged_data[revenue_cols[0]]\n",
    "                logger.info(f\"ä½¿ç”¨ {revenue_cols[0]} ä½œä¸ºrevenueå­—æ®µ\")\n",
    "            else:\n",
    "                logger.error(\"æ— æ³•æ‰¾åˆ°æ”¶å…¥ç›¸å…³å­—æ®µ\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        # åˆå¹¶å®¢æˆ·æ•°æ®\n",
    "        if not customer_data.empty and 'customer_id' in customer_data.columns:\n",
    "            logger.info(f\"åˆå¹¶å®¢æˆ·æ•°æ®: {customer_data.shape}\")\n",
    "            try:\n",
    "                merged_data = merged_data.merge(\n",
    "                    customer_data,\n",
    "                    on='customer_id',\n",
    "                    how='left'\n",
    "                )\n",
    "                logger.info(f\"åˆå¹¶åå½¢çŠ¶: {merged_data.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"å®¢æˆ·æ•°æ®åˆå¹¶å¤±è´¥: {e}\")\n",
    "        else:\n",
    "            logger.info(\"å®¢æˆ·æ•°æ®ä¸ºç©ºæˆ–ç¼ºå°‘customer_idï¼Œåˆ›å»ºé»˜è®¤å®¢æˆ·åˆ†ç¾¤\")\n",
    "            # åˆ›å»ºé»˜è®¤å®¢æˆ·åˆ†ç¾¤\n",
    "            merged_data['customer_segment'] = 'regular'\n",
    "            merged_data['is_high_value'] = 0\n",
    "\n",
    "        # åˆå¹¶å•†å“æ•°æ®\n",
    "        if not item_data.empty:\n",
    "            merge_keys = []\n",
    "            if 'item_name' in merged_data.columns and 'item_name' in item_data.columns:\n",
    "                merge_keys.append('item_name')\n",
    "            if 'category_name' in merged_data.columns and 'category_name' in item_data.columns:\n",
    "                merge_keys.append('category_name')\n",
    "\n",
    "            if merge_keys:\n",
    "                logger.info(f\"ä½¿ç”¨ {merge_keys} åˆå¹¶å•†å“æ•°æ®\")\n",
    "                try:\n",
    "                    merged_data = merged_data.merge(\n",
    "                        item_data[merge_keys + ['avg_item_revenue']] if 'avg_item_revenue' in item_data.columns else item_data[merge_keys],\n",
    "                        on=merge_keys,\n",
    "                        how='left'\n",
    "                    )\n",
    "                    logger.info(f\"å•†å“æ•°æ®åˆå¹¶åå½¢çŠ¶: {merged_data.shape}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"å•†å“æ•°æ®åˆå¹¶å¤±è´¥: {e}\")\n",
    "\n",
    "        # æ•°æ®æ¸…ç†\n",
    "        logger.info(\"å¼€å§‹æ•°æ®æ¸…ç†...\")\n",
    "        original_count = len(merged_data)\n",
    "\n",
    "        # ç§»é™¤ç¼ºå¤±å…³é”®å­—æ®µçš„è®°å½•\n",
    "        required_fields = ['customer_id', 'revenue']\n",
    "        available_fields = [field for field in required_fields if field in merged_data.columns]\n",
    "\n",
    "        if available_fields:\n",
    "            merged_data = merged_data.dropna(subset=available_fields)\n",
    "            logger.info(f\"ç§»é™¤ç¼ºå¤±å€¼å: {len(merged_data)} æ¡è®°å½• (åŸå§‹: {original_count})\")\n",
    "\n",
    "        # ç§»é™¤å¼‚å¸¸å€¼ï¼ˆæ¯”å¦‚è´Ÿæ”¶å…¥æˆ–0æ”¶å…¥ï¼‰\n",
    "        if 'revenue' in merged_data.columns:\n",
    "            before_filter = len(merged_data)\n",
    "            merged_data = merged_data[merged_data['revenue'] > 0]\n",
    "            logger.info(f\"ç§»é™¤å¼‚å¸¸æ”¶å…¥å: {len(merged_data)} æ¡è®°å½• (è¿‡æ»¤å‰: {before_filter})\")\n",
    "\n",
    "        # å¡«å……ç¼ºå¤±å€¼\n",
    "        if 'location_id' in merged_data.columns:\n",
    "            merged_data['location_id'] = merged_data['location_id'].fillna('unknown')\n",
    "        else:\n",
    "            merged_data['location_id'] = 'unknown'\n",
    "\n",
    "        if 'category_name' in merged_data.columns:\n",
    "            merged_data['category_name'] = merged_data['category_name'].fillna('other')\n",
    "        else:\n",
    "            merged_data['category_name'] = 'other'\n",
    "\n",
    "        if 'item_name' in merged_data.columns:\n",
    "            merged_data['item_name'] = merged_data['item_name'].fillna('unknown_item')\n",
    "        else:\n",
    "            merged_data['item_name'] = 'unknown_item'\n",
    "\n",
    "        logger.info(f\"æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {merged_data.shape}\")\n",
    "        return merged_data\n",
    "\n",
    "    def _create_treatment_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºå¤„ç†å˜é‡\"\"\"\n",
    "        # 1. ä¿ƒé”€æ´»åŠ¨æ ‡è®°ï¼ˆåŸºäºæ”¶å…¥å¼‚å¸¸æ£€æµ‹ï¼‰\n",
    "        # è®¡ç®—æ¯æ—¥å¹³å‡æ”¶å…¥\n",
    "        daily_revenue = data.groupby('date')['revenue'].sum().reset_index()\n",
    "        daily_avg = daily_revenue['revenue'].mean()\n",
    "        daily_std = daily_revenue['revenue'].std()\n",
    "\n",
    "        # é«˜æ”¶å…¥æ—¥æœŸå¯èƒ½æœ‰ä¿ƒé”€æ´»åŠ¨\n",
    "        promotion_threshold = daily_avg + 1.5 * daily_std\n",
    "        promotion_dates = daily_revenue[daily_revenue['revenue'] > promotion_threshold]['date'].tolist()\n",
    "\n",
    "        data['has_promotion'] = data['date'].isin(promotion_dates).astype(int)\n",
    "\n",
    "        # 2. æŠ˜æ‰£æ´»åŠ¨æ ‡è®°ï¼ˆåŸºäºä½äºå¹³å‡å•†å“ä»·æ ¼ï¼‰\n",
    "        if 'avg_item_revenue' in data.columns:\n",
    "            data['has_discount'] = (data['revenue'] < data['avg_item_revenue'] * 0.8).astype(int)\n",
    "        else:\n",
    "            data['has_discount'] = 0\n",
    "\n",
    "        # 3. ä¼šå‘˜ä¸“äº«æ´»åŠ¨ï¼ˆé’ˆå¯¹é«˜ä»·å€¼å®¢æˆ·ï¼‰\n",
    "        data['vip_treatment'] = ((data.get('is_high_value', 0) == 1) &\n",
    "                                (data['has_promotion'] == 1)).astype(int)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_confounding_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºæ··æ·†å˜é‡\"\"\"\n",
    "        # 1. æ—¶é—´ç›¸å…³å˜é‡\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['is_weekend'] = data['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        data['month'] = data['date'].dt.month\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "        if 'hour' in data.columns:\n",
    "            data['hour_of_day'] = data['hour']\n",
    "        else:\n",
    "            data['hour_of_day'] = 12  # é»˜è®¤ä¸­åˆ\n",
    "\n",
    "        # 2. åœ°ç†ä½ç½®\n",
    "        data['location_encoded'] = pd.Categorical(data['location_id']).codes\n",
    "\n",
    "        # 3. å•†å“ç±»åˆ«\n",
    "        data['category_encoded'] = pd.Categorical(data['category_name']).codes\n",
    "\n",
    "        # 4. å®¢æˆ·ç‰¹å¾\n",
    "        data['customer_segment_encoded'] = pd.Categorical(\n",
    "            data.get('customer_segment', 'regular')\n",
    "        ).codes\n",
    "\n",
    "        # 5. å†å²è´­ä¹°è¡Œä¸ºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n",
    "        if 'total_orders' in data.columns:\n",
    "            data['is_frequent_buyer'] = (data['total_orders'] > data['total_orders'].median()).astype(int)\n",
    "        else:\n",
    "            data['is_frequent_buyer'] = 0\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_outcome_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºç»“æœå˜é‡\"\"\"\n",
    "        # 1. ä¸»è¦ç»“æœå˜é‡ - æ”¶å…¥\n",
    "        data['log_revenue'] = np.log1p(data['revenue'])  # å¯¹æ•°å˜æ¢å‡å°‘åæ€\n",
    "\n",
    "        # 2. è®¢å•é‡ï¼ˆå¦‚æœæœ‰quantityå­—æ®µï¼‰\n",
    "        if 'quantity' in data.columns:\n",
    "            data['order_quantity'] = data['quantity']\n",
    "        else:\n",
    "            data['order_quantity'] = 1  # é»˜è®¤æ¯ä¸ªè®°å½•ä¸º1ä¸ªè®¢å•\n",
    "\n",
    "        # 3. å®¢æˆ·ä»·å€¼ç›¸å…³\n",
    "        if 'avg_order_value' in data.columns:\n",
    "            data['customer_value_change'] = data['revenue'] - data['avg_order_value']\n",
    "        else:\n",
    "            customer_avg = data.groupby('customer_id')['revenue'].transform('mean')\n",
    "            data['customer_value_change'] = data['revenue'] - customer_avg\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _validate_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"éªŒè¯æ•°æ®è´¨é‡\"\"\"\n",
    "        if data.empty:\n",
    "            logger.error(\"æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒéªŒè¯\")\n",
    "            return data\n",
    "\n",
    "        logger.info(f\"æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:\")\n",
    "        logger.info(f\"- æ€»è®°å½•æ•°: {len(data)}\")\n",
    "        logger.info(f\"- æ€»åˆ—æ•°: {len(data.columns)}\")\n",
    "        logger.info(f\"- åˆ—å: {list(data.columns)}\")\n",
    "\n",
    "        # æ£€æŸ¥customer_id\n",
    "        if 'customer_id' in data.columns:\n",
    "            logger.info(f\"- å”¯ä¸€å®¢æˆ·æ•°: {data['customer_id'].nunique()}\")\n",
    "        else:\n",
    "            logger.warning(\"- ç¼ºå°‘customer_idå­—æ®µ\")\n",
    "\n",
    "        # æ£€æŸ¥æ—¥æœŸèŒƒå›´\n",
    "        if 'date' in data.columns:\n",
    "            try:\n",
    "                data['date'] = pd.to_datetime(data['date'])\n",
    "                logger.info(f\"- æ—¥æœŸèŒƒå›´: {data['date'].min()} åˆ° {data['date'].max()}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"æ—¥æœŸå­—æ®µå¤„ç†å¤±è´¥: {e}\")\n",
    "        else:\n",
    "            logger.warning(\"- ç¼ºå°‘dateå­—æ®µ\")\n",
    "\n",
    "        # æ£€æŸ¥å…³é”®å˜é‡\n",
    "        required_vars = ['customer_id', 'revenue']\n",
    "        available_vars = [var for var in required_vars if var in data.columns]\n",
    "        missing_vars = [var for var in required_vars if var not in data.columns]\n",
    "\n",
    "        if available_vars:\n",
    "            logger.info(f\"- å¯ç”¨å…³é”®å˜é‡: {available_vars}\")\n",
    "        if missing_vars:\n",
    "            logger.warning(f\"- ç¼ºå°‘å…³é”®å˜é‡: {missing_vars}\")\n",
    "\n",
    "        # æ£€æŸ¥å¤„ç†å˜é‡åˆ†å¸ƒ\n",
    "        if 'has_promotion' in data.columns:\n",
    "            promo_dist = data['has_promotion'].value_counts()\n",
    "            logger.info(f\"- ä¿ƒé”€åˆ†å¸ƒ: {promo_dist.to_dict()}\")\n",
    "        else:\n",
    "            logger.info(\"- ä¿ƒé”€å˜é‡å°šæœªåˆ›å»º\")\n",
    "\n",
    "        # æ£€æŸ¥æ”¶å…¥åˆ†å¸ƒ\n",
    "        if 'revenue' in data.columns:\n",
    "            logger.info(f\"- æ”¶å…¥ç»Ÿè®¡: å‡å€¼={data['revenue'].mean():.2f}, ä¸­ä½æ•°={data['revenue'].median():.2f}\")\n",
    "            logger.info(f\"- æ”¶å…¥èŒƒå›´: {data['revenue'].min():.2f} - {data['revenue'].max():.2f}\")\n",
    "\n",
    "        # ç§»é™¤å®Œå…¨é‡å¤çš„è®°å½•\n",
    "        before_count = len(data)\n",
    "        data = data.drop_duplicates()\n",
    "        after_count = len(data)\n",
    "\n",
    "        if before_count != after_count:\n",
    "            logger.info(f\"- ç§»é™¤äº† {before_count - after_count} æ¡é‡å¤è®°å½•\")\n",
    "\n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå› æœæ¨æ–­\n",
    "        if len(data) < 100:\n",
    "            logger.warning(\"æ•°æ®é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“å› æœæ¨æ–­çš„å‡†ç¡®æ€§\")\n",
    "        elif len(data) < 1000:\n",
    "            logger.info(\"æ•°æ®é‡é€‚ä¸­ï¼Œå¯ä»¥è¿›è¡ŒåŸºç¡€å› æœæ¨æ–­\")\n",
    "        else:\n",
    "            logger.info(\"æ•°æ®é‡å……è¶³ï¼Œå¯ä»¥è¿›è¡Œç¨³å¥çš„å› æœæ¨æ–­\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_dataset(self, data: pd.DataFrame, output_path: str = \"data/causal_dataset.csv\") -> None:\n",
    "        \"\"\"ä¿å­˜æ•°æ®é›†\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        data.to_csv(output_path, index=False)\n",
    "        logger.info(f\"æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}\")\n",
    "\n",
    "        # ä¿å­˜æ•°æ®å­—å…¸\n",
    "        dict_path = output_path.replace('.csv', '_dictionary.txt')\n",
    "        with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"å› æœæ¨æ–­æ•°æ®é›†å­—å…¸\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"å¤„ç†å˜é‡ (Treatment Variables):\\n\")\n",
    "            f.write(\"- has_promotion: æ˜¯å¦æœ‰ä¿ƒé”€æ´»åŠ¨ (0/1)\\n\")\n",
    "            f.write(\"- has_discount: æ˜¯å¦æœ‰æŠ˜æ‰£ (0/1)\\n\")\n",
    "            f.write(\"- vip_treatment: VIPä¸“äº«æ´»åŠ¨ (0/1)\\n\\n\")\n",
    "\n",
    "            f.write(\"ç»“æœå˜é‡ (Outcome Variables):\\n\")\n",
    "            f.write(\"- revenue: æ”¶å…¥é‡‘é¢\\n\")\n",
    "            f.write(\"- log_revenue: å¯¹æ•°æ”¶å…¥\\n\")\n",
    "            f.write(\"- order_quantity: è®¢å•æ•°é‡\\n\\n\")\n",
    "\n",
    "            f.write(\"æ··æ·†å˜é‡ (Confounding Variables):\\n\")\n",
    "            f.write(\"- is_weekend: æ˜¯å¦å‘¨æœ« (0/1)\\n\")\n",
    "            f.write(\"- hour_of_day: å°æ—¶\\n\")\n",
    "            f.write(\"- location_encoded: åœ°ç†ä½ç½®ç¼–ç \\n\")\n",
    "            f.write(\"- category_encoded: å•†å“ç±»åˆ«ç¼–ç \\n\")\n",
    "            f.write(\"- customer_segment_encoded: å®¢æˆ·åˆ†ç¾¤ç¼–ç \\n\")\n",
    "\n",
    "        logger.info(f\"æ•°æ®å­—å…¸å·²ä¿å­˜åˆ°: {dict_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    try:\n",
    "        logger.info(\"å¼€å§‹åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨...\")\n",
    "\n",
    "        # åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨\n",
    "        try:\n",
    "            preparator = CausalDataPreparator()\n",
    "            logger.info(\"æ•°æ®å‡†å¤‡å™¨åˆå§‹åŒ–æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"æ•°æ®å‡†å¤‡å™¨åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            logger.info(\"è¿™å¯èƒ½æ˜¯å› ä¸ºé…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è¿æ¥é—®é¢˜\")\n",
    "            return\n",
    "\n",
    "        # å‡†å¤‡æ•°æ®é›†\n",
    "        logger.info(\"å¼€å§‹å‡†å¤‡æ•°æ®é›†...\")\n",
    "        dataset = preparator.prepare_causal_dataset(days_back=180)\n",
    "\n",
    "        if not dataset.empty:\n",
    "            # ä¿å­˜æ•°æ®é›†\n",
    "            logger.info(\"ä¿å­˜æ•°æ®é›†...\")\n",
    "            preparator.save_dataset(dataset)\n",
    "\n",
    "            # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"æ•°æ®é›†å‡†å¤‡å®Œæˆ!\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"æ•°æ®å½¢çŠ¶: {dataset.shape}\")\n",
    "            print(f\"åˆ—å: {list(dataset.columns)}\")\n",
    "\n",
    "            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ\n",
    "            print(f\"\\nå‰5è¡Œé¢„è§ˆ:\")\n",
    "            print(dataset.head())\n",
    "\n",
    "            # æ˜¾ç¤ºæ•°æ®ç±»å‹\n",
    "            print(f\"\\næ•°æ®ç±»å‹:\")\n",
    "            print(dataset.dtypes)\n",
    "\n",
    "            # æ˜¾ç¤ºç¼ºå¤±å€¼ç»Ÿè®¡\n",
    "            print(f\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "            missing_stats = dataset.isnull().sum()\n",
    "            print(missing_stats[missing_stats > 0])\n",
    "\n",
    "            print(f\"\\næ•°æ®å·²ä¿å­˜åˆ°: data/causal_dataset.csv\")\n",
    "            print(f\"æ•°æ®å­—å…¸å·²ä¿å­˜åˆ°: data/causal_dataset_dictionary.txt\")\n",
    "\n",
    "        else:\n",
    "            logger.error(\"æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜:\")\n",
    "            logger.error(\"1. æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸\")\n",
    "            logger.error(\"2. è¡¨åå’Œå­—æ®µåæ˜¯å¦æ­£ç¡®\")\n",
    "            logger.error(\"3. æ•°æ®æ˜¯å¦å­˜åœ¨äºæŒ‡å®šçš„æ—¶é—´èŒƒå›´å†…\")\n",
    "            logger.error(\"4. æƒé™æ˜¯å¦è¶³å¤Ÿè®¿é—®ç›¸å…³è¡¨\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"æ•°æ®å‡†å¤‡å¤±è´¥: {e}\", exc_info=True)\n",
    "        logger.info(\"\\nè°ƒè¯•å»ºè®®:\")\n",
    "        logger.info(\"1. æ£€æŸ¥config/config.jsonä¸­çš„æ•°æ®åº“é…ç½®\")\n",
    "        logger.info(\"2. ç¡®è®¤æ•°æ®åº“è¡¨æ˜¯å¦å­˜åœ¨å¹¶å¯è®¿é—®\")\n",
    "        logger.info(\"3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®\")\n",
    "        logger.info(\"4. æŸ¥çœ‹å®Œæ•´çš„é”™è¯¯å †æ ˆä¿¡æ¯\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "3e41167cb30f70a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 14:05:14,334 - __main__ - INFO - å¼€å§‹åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨...\n",
      "2025-07-31 14:05:14,335 - config.settings - INFO - Config loaded from /Users/xander/PycharmProjects/FBR_AI_Agine/config/config.json\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - æ•°æ®å‡†å¤‡å™¨åˆå§‹åŒ–æˆåŠŸ\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - å¼€å§‹å‡†å¤‡æ•°æ®é›†...\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - å¼€å§‹å‡†å¤‡å› æœæ¨æ–­æ•°æ®é›†ï¼Œå›æº¯180å¤©\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - è·å–é”€å”®æ•°æ®...\n",
      "2025-07-31 14:05:16,662 - src.data.connectors - INFO - Connected to ClickHouse using clickhouse_connect: clickhouse-0-0.umetea.net:443\n",
      "2025-07-31 14:05:17,577 - __main__ - INFO - ä»repositoryè·å–åˆ°æ•°æ®ï¼Œå½¢çŠ¶: (4061, 8)\n",
      "2025-07-31 14:05:17,578 - __main__ - INFO - åˆ—å: ['date', 'location_id', 'order_count', 'customer_count', 'total_revenue', 'avg_order_value', 'new_customer_count', 'repeat_customer_count']\n",
      "2025-07-31 14:05:17,583 - __main__ - INFO - æœªæ‰¾åˆ°customer_idï¼Œä½¿ç”¨ location_id ä½œä¸ºcustomer_id\n",
      "2025-07-31 14:05:17,585 - __main__ - INFO - è·å–å®¢æˆ·æ•°æ®...\n",
      "2025-07-31 14:05:21,732 - src.data.connectors - INFO - Connected to ClickHouse using clickhouse_connect: clickhouse-0-0.umetea.net:443\n",
      "2025-07-31 14:05:22,043 - __main__ - INFO - ä»repositoryè·å–å®¢æˆ·æ•°æ®ï¼Œå½¢çŠ¶: (5, 5)\n",
      "2025-07-31 14:05:22,044 - __main__ - INFO - å®¢æˆ·æ•°æ®åˆ—å: ['segment', 'customer_count', 'total_revenue', 'avg_order_value', 'avg_order_count']\n",
      "2025-07-31 14:05:22,045 - __main__ - WARNING - æœªæ‰¾åˆ°ä»»ä½•å®¢æˆ·IDåˆ—ï¼Œåˆ›å»ºè™šæ‹Ÿcustomer_id\n",
      "2025-07-31 14:05:22,049 - __main__ - INFO - è·å–å•†å“æ•°æ®...\n",
      "2025-07-31 14:05:22,863 - __main__ - INFO - ä»repositoryè·å–å•†å“æ•°æ®ï¼Œå½¢çŠ¶: (380, 9)\n",
      "2025-07-31 14:05:22,864 - __main__ - INFO - å•†å“æ•°æ®åˆ—å: ['item_id', 'item_name', 'category_name', 'order_count', 'units_sold', 'revenue', 'avg_price', 'total_discount', 'unique_buyers']\n",
      "2025-07-31 14:05:22,864 - __main__ - INFO - æœªæ‰¾åˆ°customer_idï¼Œä½¿ç”¨ item_id ä½œä¸ºcustomer_id\n",
      "2025-07-31 14:05:22,865 - __main__ - INFO - åˆå¹¶å’Œæ¸…ç†æ•°æ®...\n",
      "2025-07-31 14:05:22,865 - __main__ - INFO - å¼€å§‹åˆå¹¶æ•°æ® - é”€å”®æ•°æ®: (4061, 9)\n",
      "2025-07-31 14:05:22,866 - __main__ - INFO - ä½¿ç”¨ total_revenue ä½œä¸ºrevenueå­—æ®µ\n",
      "2025-07-31 14:05:22,866 - __main__ - INFO - åˆå¹¶å®¢æˆ·æ•°æ®: (5, 6)\n",
      "2025-07-31 14:05:22,876 - __main__ - INFO - åˆå¹¶åå½¢çŠ¶: (4061, 15)\n",
      "2025-07-31 14:05:22,876 - __main__ - INFO - å¼€å§‹æ•°æ®æ¸…ç†...\n",
      "2025-07-31 14:05:22,879 - __main__ - INFO - ç§»é™¤ç¼ºå¤±å€¼å: 4061 æ¡è®°å½• (åŸå§‹: 4061)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - ç§»é™¤å¼‚å¸¸æ”¶å…¥å: 4057 æ¡è®°å½• (è¿‡æ»¤å‰: 4061)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: (4057, 17)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - åˆ›å»ºå¤„ç†å˜é‡...\n",
      "2025-07-31 14:05:22,885 - __main__ - ERROR - åˆ›å»ºå¤„ç†å˜é‡å¤±è´¥: unsupported operand type(s) for -: 'float' and 'decimal.Decimal'\n",
      "2025-07-31 14:05:22,886 - __main__ - INFO - åˆ›å»ºæ··æ·†å˜é‡...\n",
      "2025-07-31 14:05:22,893 - __main__ - ERROR - åˆ›å»ºæ··æ·†å˜é‡å¤±è´¥: Categorical input must be list-like\n",
      "2025-07-31 14:05:22,893 - __main__ - INFO - åˆ›å»ºç»“æœå˜é‡...\n",
      "2025-07-31 14:05:22,894 - __main__ - ERROR - åˆ›å»ºç»“æœå˜é‡å¤±è´¥: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "2025-07-31 14:05:22,894 - __main__ - ERROR - æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "AttributeError: 'decimal.Decimal' object has no attribute 'log1p'. Did you mean: 'log10'?\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 107, in prepare_causal_dataset\n",
      "    merged_data = self._create_outcome_variables(merged_data)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 599, in _create_outcome_variables\n",
      "    data['log_revenue'] = np.log1p(data['revenue'])  # å¯¹æ•°å˜æ¢å‡å°‘åæ€\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py\", line 2190, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/arraylike.py\", line 399, in array_ufunc\n",
      "    result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "AttributeError: 'decimal.Decimal' object has no attribute 'log1p'. Did you mean: 'log10'?\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 112, in prepare_causal_dataset\n",
      "    merged_data['log_revenue'] = np.log1p(merged_data['revenue'])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py\", line 2190, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/arraylike.py\", line 399, in array_ufunc\n",
      "    result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "2025-07-31 14:05:22,897 - __main__ - ERROR - æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜:\n",
      "2025-07-31 14:05:22,897 - __main__ - ERROR - 1. æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 2. è¡¨åå’Œå­—æ®µåæ˜¯å¦æ­£ç¡®\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 3. æ•°æ®æ˜¯å¦å­˜åœ¨äºæŒ‡å®šçš„æ—¶é—´èŒƒå›´å†…\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 4. æƒé™æ˜¯å¦è¶³å¤Ÿè®¿é—®ç›¸å…³è¡¨\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b9888a15f5f63df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
