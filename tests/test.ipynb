{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T02:59:09.527624Z",
     "start_time": "2025-07-31T02:59:08.295408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.causal_inference.causal_engine import FBRUSCausalAnalyzer,logger\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ],
   "id": "3c3a123e3e2111b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T02:59:40.010890Z",
     "start_time": "2025-07-31T02:59:16.067453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 主程序\n",
    "# def main():\n",
    "\"\"\"主函数\"\"\"\n",
    "# 定义门店位置（美国主要城市的邮编）\n",
    "store_locations = [\n",
    "    {'name': 'FBR NYC Manhattan', 'postal_code': '10001'},     # 纽约曼哈顿\n",
    "    {'name': 'FBR LA Downtown', 'postal_code': '90012'},       # 洛杉矶市中心\n",
    "    {'name': 'FBR Chicago Loop', 'postal_code': '60601'},      # 芝加哥\n",
    "    {'name': 'FBR SF Financial', 'postal_code': '94105'},      # 旧金山金融区\n",
    "    {'name': 'FBR Miami Beach', 'postal_code': '33139'}        # 迈阿密海滩\n",
    "]\n",
    "\n",
    "# 创建分析器\n",
    "analyzer = FBRUSCausalAnalyzer(store_locations)\n",
    "\n",
    "try:\n",
    "    # 1. 加载或生成销售数据\n",
    "    sales_data = analyzer.load_sales_data()  # 使用模拟数据\n",
    "    logger.info(f\"销售数据形状: {sales_data.shape}\")\n",
    "\n",
    "    # 2. 获取天气数据\n",
    "    weather_data = analyzer.fetch_weather_data()\n",
    "    logger.info(f\"天气数据形状: {weather_data.shape if weather_data is not None else 'None'}\")\n",
    "\n",
    "    # 3. 获取节假日数据\n",
    "    holiday_data = analyzer.fetch_holiday_data()\n",
    "    logger.info(f\"节假日数据形状: {holiday_data.shape if holiday_data is not None else 'None'}\")\n",
    "\n",
    "    # 4. 合并所有数据\n",
    "    merged_data = analyzer.merge_all_data()\n",
    "    logger.info(f\"合并数据形状: {merged_data.shape}\")\n",
    "\n",
    "    # 5. 执行因果分析\n",
    "    causal_results = analyzer.perform_causal_analysis()\n",
    "\n",
    "    # 6. 执行反事实分析\n",
    "    counterfactual_results = analyzer.perform_counterfactual_analysis()\n",
    "\n",
    "    # 7. 生成可视化报告\n",
    "    analyzer.visualize_results(causal_results, counterfactual_results)\n",
    "\n",
    "    # 8. 打印一些关键结果\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FBR美国门店因果分析 - 关键发现\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 打印促销效果\n",
    "    if 'promotion_effect' in causal_results and 'average_effect' in causal_results['promotion_effect']:\n",
    "        print(f\"\\n📊 促销效果：{causal_results['promotion_effect']['interpretation']}\")\n",
    "\n",
    "    # 打印最优条件\n",
    "    if 'optimal_conditions' in counterfactual_results:\n",
    "        print(f\"\\n🎯 最优销售条件将带来：${counterfactual_results['optimal_conditions']['expected_sales']:.0f} 的销售额\")\n",
    "\n",
    "    # 打印主要风险\n",
    "    if 'risk_scenarios' in counterfactual_results and counterfactual_results['risk_scenarios']:\n",
    "        top_risk = counterfactual_results['risk_scenarios'][0]\n",
    "        print(f\"\\n⚠️  最大风险：{top_risk['scenario']} - 预计损失 ${top_risk['predicted_loss']:.0f}\")\n",
    "\n",
    "    print(\"\\n✅ 分析完成！\")\n",
    "    print(\"📄 报告文件：\")\n",
    "    print(\"   - 可视化报告: fbr_us_causal_analysis_report.html\")\n",
    "    print(\"   - 文字报告: fbr_us_causal_analysis_report.txt\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"分析过程中出错: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# return analyzer, causal_results, counterfactual_results\n",
    "#\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 运行分析\n",
    "#     analyzer, causal_results, counterfactual_results = main()"
   ],
   "id": "e180f1658911d664",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 10:59:16,074 - INFO - 加载销售数据...\n",
      "2025-07-31 10:59:16,076 - INFO - 使用模拟数据...\n",
      "2025-07-31 10:59:16,086 - INFO - 生成了 1830 条销售记录\n",
      "2025-07-31 10:59:16,087 - INFO - 销售数据形状: (1830, 10)\n",
      "2025-07-31 10:59:16,087 - INFO - 获取天气数据...\n",
      "2025-07-31 10:59:16,090 - INFO - Generating weather features for 5 postal codes.\n",
      "2025-07-31 10:59:25,390 - INFO - Finished creating weather features. Total rows: 1830\n",
      "2025-07-31 10:59:25,401 - INFO - 获取了 1830 条天气记录\n",
      "2025-07-31 10:59:25,401 - INFO - 天气数据形状: (1830, 18)\n",
      "2025-07-31 10:59:25,402 - INFO - 获取节假日数据...\n",
      "2025-07-31 10:59:25,405 - INFO - Generating holiday features for 5 postal codes.\n",
      "2025-07-31 10:59:25,537 - INFO - Finished creating holiday features. Total rows: 1830\n",
      "2025-07-31 10:59:25,540 - INFO - 获取了 1830 条节假日记录\n",
      "2025-07-31 10:59:25,541 - INFO - 节假日数据形状: (1830, 9)\n",
      "2025-07-31 10:59:25,541 - INFO - 合并数据...\n",
      "2025-07-31 10:59:25,549 - INFO - 合并后数据集包含 1830 条记录，32 个特征\n",
      "2025-07-31 10:59:25,549 - INFO - 合并数据形状: (1830, 32)\n",
      "2025-07-31 10:59:25,550 - INFO - 开始因果分析...\n",
      "2025-07-31 10:59:25,550 - INFO - 分析天气因果效应...\n",
      "2025-07-31 10:59:25,551 - WARNING - 雨天数据不足，跳过雨天分析\n",
      "2025-07-31 10:59:25,551 - INFO - 分析促销因果效应...\n",
      "2025-07-31 10:59:25,552 - WARNING - Causal Graph not provided. DoWhy will construct a graph based on data inputs.\n",
      "2025-07-31 10:59:25,552 - INFO - Model to find the causal effect of treatment ['has_promotion'] on outcome ['sales_revenue']\n",
      "2025-07-31 10:59:25,553 - WARNING - There are an additional 27 variables in the dataset that are not in the graph. Variable names are: '['date', 'days_since_last_holiday', 'days_until_next_holiday', 'discount_rate', 'holiday_name', 'inventory_level', 'is_extreme_temp', 'is_extreme_weather', 'is_major_holiday', 'is_shopping_season', 'is_snowy', 'is_weekend', 'is_windy', 'orders_count', 'postal_code', 'precipitation_sum', 'snowfall_sum', 'state', 'store_name', 'temp_avg_f', 'temp_max_f', 'temp_min_f', 'temperature_2m_max', 'temperature_2m_min', 'temperature_avg', 'weather_code', 'wind_speed_10m_max']'\n",
      "2025-07-31 10:59:25,554 - INFO - Causal effect can be identified.\n",
      "2025-07-31 10:59:25,555 - INFO - Instrumental variables for treatment and outcome:[]\n",
      "2025-07-31 10:59:25,555 - INFO - Frontdoor variables for treatment and outcome:[]\n",
      "2025-07-31 10:59:25,556 - INFO - Number of general adjustment sets found: 1\n",
      "2025-07-31 10:59:25,556 - INFO - Causal effect can be identified.\n",
      "2025-07-31 10:59:25,557 - INFO - propensity_score_matching\n",
      "2025-07-31 10:59:25,558 - INFO - INFO: Using Propensity Score Matching Estimator\n",
      "2025-07-31 10:59:25,566 - INFO - b: sales_revenue~has_promotion+is_holiday+is_rainy+day_of_week\n",
      "2025-07-31 10:59:25,583 - INFO - 分析节假日因果效应...\n",
      "2025-07-31 10:59:25,590 - INFO - 分析组合因果效应...\n",
      "2025-07-31 10:59:25,607 - INFO - 开始反事实分析...\n",
      "2025-07-31 10:59:39,939 - INFO - 生成可视化...\n",
      "2025-07-31 10:59:40,009 - INFO - 可视化报告已保存至 fbr_us_causal_analysis_report.html\n",
      "2025-07-31 10:59:40,009 - INFO - 文字报告已保存至 fbr_us_causal_analysis_report.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "分析完成！\n",
      "==================================================\n",
      "\n",
      "# FBR美国门店销售数据因果分析报告\n",
      "\n",
      "生成时间：2025-07-31 10:59:40\n",
      "\n",
      "## 一、执行摘要\n",
      "\n",
      "本报告基于FBR美国门店的销售数据，结合天气和节假日信息，进行了深入的因果分析。\n",
      "\n",
      "### 门店覆盖：\n",
      "- FBR NYC Manhattan (邮编: 10001)\n",
      "- FBR LA Downtown (邮编: 90012)\n",
      "- FBR Chicago Loop (邮编: 60601)\n",
      "- FBR SF Financial (邮编: 94105)\n",
      "- FBR Miami Beach (邮编: 33139)\n",
      "\n",
      "### 关键发现：\n",
      "\n",
      "\n",
      "2. **促销效果**：\n",
      "   - 促销平均提升销售额 $10504，ROI约为 -14.6%\n",
      "   - 异质性效应：\n",
      "     * 工作日: $7389\n",
      "     * 周末: $8609\n",
      "     * 晴天: $7517\n",
      "\n",
      "3. **节假日效应**：\n",
      "   - 节假日整体提升销售额 $nan\n",
      "   - 购物季效应：购物季销售额平均减少 $nan\n",
      "\n",
      "## 二、反事实分析\n",
      "\n",
      "### 场景分析：\n",
      "\n",
      "**完美促销日：节假日+好天气+促销**\n",
      "- 预测销售额：$57491\n",
      "- 相比平均值：$+2997\n",
      "\n",
      "**最差情况：极端天气+无促销**\n",
      "- 预测销售额：$50695\n",
      "- 相比平均值：$-3798\n",
      "\n",
      "**雨天促销策略**\n",
      "- 预测销售额：$57491\n",
      "- 相比平均值：$+2997\n",
      "\n",
      "**黑色星期五场景**\n",
      "- 预测销售额：$58168\n",
      "- 相比平均值：$+3675\n",
      "\n",
      "**普通工作日**\n",
      "- 预测销售额：$49648\n",
      "- 相比平均值：$-4845\n",
      "\n",
      "### 最优运营条件：\n",
      "- 预期销售额：$69857\n",
      "- 建议：\n",
      "  * 实施促销活动\n",
      "  * 充分利用节假日流量\n",
      "  * 最佳销售日是周六\n",
      "  * 理想温度约70°F\n",
      "\n",
      "### 风险场景预警：\n",
      "\n",
      "- **连续雨天+工作日**\n",
      "  * 预计损失：$4845\n",
      "  * 风险等级：中\n",
      "  * 缓解措施：雨天专属优惠、加强线上推广、改善店内体验\n",
      "\n",
      "- **极端高温(>100°F)+无促销**\n",
      "  * 预计损失：$3798\n",
      "  * 风险等级：中\n",
      "  * 缓解措施：加强空调、提供冷饮优惠、延长营业时间至晚上\n",
      "\n",
      "- **暴风雪天气**\n",
      "  * 预计损失：$3798\n",
      "  * 风险等级：中\n",
      "  * 缓解措施：提前备货、加强外卖服务、员工安全保障\n",
      "\n",
      "\n",
      "## 三、业务建议\n",
      "\n",
      "### 基于因果分析的行动建议：\n",
      "\n",
      "1. **天气应对策略**：\n",
      "   - 建立天气监测预警系统，提前3-5天预测销售趋势\n",
      "   - 雨天和极端天气时，加强外卖/配送服务\n",
      "   - 在最佳温度区间（舒适天气）时，可以举办户外促销活动\n",
      "\n",
      "2. **促销优化**：\n",
      "   - 基于ROI分析，在周末和节假日加大促销力度\n",
      "   - 雨天促销可以有效缓解天气带来的负面影响\n",
      "   - 建议建立动态促销系统，根据天气和客流自动调整\n",
      "\n",
      "3. **节假日运营**：\n",
      "   - 重点准备主要节假日（感恩节、圣诞节等）的库存\n",
      "   - 购物季（黑色星期五、网络星期一）需要特别准备\n",
      "   - 针对表现较差的节假日，考虑特殊营销策略\n",
      "\n",
      "4. **风险管理**：\n",
      "   - 建立极端天气应急预案\n",
      "   - 优化库存管理，避免因天气导致的损失\n",
      "   - 考虑天气保险等风险对冲工具\n",
      "\n",
      "## 四、数据说明\n",
      "\n",
      "- 分析时间范围：最近365天\n",
      "- 数据记录数：{len(self.merged_data) if self.merged_data is not ...\n",
      "\n",
      "============================================================\n",
      "FBR美国门店因果分析 - 关键发现\n",
      "============================================================\n",
      "\n",
      "📊 促销效果：促销平均提升销售额 $10504，ROI约为 -14.6%\n",
      "\n",
      "🎯 最优销售条件将带来：$69857 的销售额\n",
      "\n",
      "⚠️  最大风险：连续雨天+工作日 - 预计损失 $4845\n",
      "\n",
      "✅ 分析完成！\n",
      "📄 报告文件：\n",
      "   - 可视化报告: fbr_us_causal_analysis_report.html\n",
      "   - 文字报告: fbr_us_causal_analysis_report.txt\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:00:24.928052Z",
     "start_time": "2025-07-31T03:00:24.909982Z"
    }
   },
   "cell_type": "code",
   "source": "sales_data",
   "id": "406d612ae61e4004",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           date         store_name postal_code  sales_revenue  \\\n",
       "0    2024-07-31 10:59:16.076790  FBR NYC Manhattan       10001   52483.570765   \n",
       "1    2024-08-01 10:59:16.076790  FBR NYC Manhattan       10001   57006.632090   \n",
       "2    2024-08-02 10:59:16.076790  FBR NYC Manhattan       10001   53582.658807   \n",
       "3    2024-08-03 10:59:16.076790  FBR NYC Manhattan       10001   77327.500690   \n",
       "4    2024-08-04 10:59:16.076790  FBR NYC Manhattan       10001   59654.862248   \n",
       "...                         ...                ...         ...            ...   \n",
       "1825 2025-07-27 10:59:16.076790    FBR Miami Beach       33139   46710.318381   \n",
       "1826 2025-07-28 10:59:16.076790    FBR Miami Beach       33139   54410.679070   \n",
       "1827 2025-07-29 10:59:16.076790    FBR Miami Beach       33139   55641.624020   \n",
       "1828 2025-07-30 10:59:16.076790    FBR Miami Beach       33139   45789.909738   \n",
       "1829 2025-07-31 10:59:16.076790    FBR Miami Beach       33139   48882.637474   \n",
       "\n",
       "      orders_count  has_promotion  discount_rate  inventory_level  \\\n",
       "0       602.106902              0       0.000000         0.542270   \n",
       "1       711.497004              1       0.228716         0.982927   \n",
       "2       665.164544              0       0.000000         0.748680   \n",
       "3       980.517886              1       0.246318         0.875736   \n",
       "4       782.664900              0       0.000000         0.392767   \n",
       "...            ...            ...            ...              ...   \n",
       "1825    591.819913              0       0.000000         0.984946   \n",
       "1826    698.766808              1       0.196143         0.711903   \n",
       "1827    703.427774              0       0.000000         0.689866   \n",
       "1828    566.120328              0       0.000000         0.759057   \n",
       "1829    595.113190              0       0.000000         0.647917   \n",
       "\n",
       "      day_of_week  is_weekend  \n",
       "0               2       False  \n",
       "1               3       False  \n",
       "2               4       False  \n",
       "3               5        True  \n",
       "4               6        True  \n",
       "...           ...         ...  \n",
       "1825            6        True  \n",
       "1826            0       False  \n",
       "1827            1       False  \n",
       "1828            2       False  \n",
       "1829            3       False  \n",
       "\n",
       "[1830 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>sales_revenue</th>\n",
       "      <th>orders_count</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>inventory_level</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-31 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>52483.570765</td>\n",
       "      <td>602.106902</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542270</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-01 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>57006.632090</td>\n",
       "      <td>711.497004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228716</td>\n",
       "      <td>0.982927</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-02 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>53582.658807</td>\n",
       "      <td>665.164544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.748680</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-03 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>77327.500690</td>\n",
       "      <td>980.517886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.246318</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-04 10:59:16.076790</td>\n",
       "      <td>FBR NYC Manhattan</td>\n",
       "      <td>10001</td>\n",
       "      <td>59654.862248</td>\n",
       "      <td>782.664900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392767</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2025-07-27 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>46710.318381</td>\n",
       "      <td>591.819913</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984946</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>2025-07-28 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>54410.679070</td>\n",
       "      <td>698.766808</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196143</td>\n",
       "      <td>0.711903</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>2025-07-29 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>55641.624020</td>\n",
       "      <td>703.427774</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689866</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>2025-07-30 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>45789.909738</td>\n",
       "      <td>566.120328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.759057</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>2025-07-31 10:59:16.076790</td>\n",
       "      <td>FBR Miami Beach</td>\n",
       "      <td>33139</td>\n",
       "      <td>48882.637474</td>\n",
       "      <td>595.113190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1830 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:09:24.733209Z",
     "start_time": "2025-07-31T03:09:24.714555Z"
    }
   },
   "cell_type": "code",
   "source": "holiday_data.groupby(\"is_holiday\").count()",
   "id": "e8f6bdff132dfdd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            date  postal_code  holiday_name  days_since_last_holiday  \\\n",
       "is_holiday                                                             \n",
       "False       1765         1765          1765                     1765   \n",
       "True          65           65            65                       65   \n",
       "\n",
       "            days_until_next_holiday  state  is_major_holiday  \\\n",
       "is_holiday                                                     \n",
       "False                          1765   1765              1765   \n",
       "True                             65     65                65   \n",
       "\n",
       "            is_shopping_season  \n",
       "is_holiday                      \n",
       "False                     1765  \n",
       "True                        65  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>days_since_last_holiday</th>\n",
       "      <th>days_until_next_holiday</th>\n",
       "      <th>state</th>\n",
       "      <th>is_major_holiday</th>\n",
       "      <th>is_shopping_season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_holiday</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:10:06.434299Z",
     "start_time": "2025-07-31T03:10:06.414106Z"
    }
   },
   "cell_type": "code",
   "source": "holiday_data.groupby(\"is_shopping_season\").count()",
   "id": "4a1423b45c1b46cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    date  postal_code  is_holiday  holiday_name  \\\n",
       "is_shopping_season                                                \n",
       "0                   1817         1817        1817          1817   \n",
       "1                     13           13          13            13   \n",
       "\n",
       "                    days_since_last_holiday  days_until_next_holiday  state  \\\n",
       "is_shopping_season                                                            \n",
       "0                                      1817                     1817   1817   \n",
       "1                                        13                       13     13   \n",
       "\n",
       "                    is_major_holiday  \n",
       "is_shopping_season                    \n",
       "0                               1817  \n",
       "1                                 13  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>days_since_last_holiday</th>\n",
       "      <th>days_until_next_holiday</th>\n",
       "      <th>state</th>\n",
       "      <th>is_major_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_shopping_season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:20:41.075143Z",
     "start_time": "2025-07-31T03:20:41.071456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.repositories import OrderRepository,CustomerRepository,AnalyticsRepository\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "56b4dbcfd08ddcf6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:20:51.097506Z",
     "start_time": "2025-07-31T03:20:51.093422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "order_repository = OrderRepository()\n",
    "customer_repository = CustomerRepository()\n",
    "analytics_repository = AnalyticsRepository()"
   ],
   "id": "5fc21558d61e10b8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:27:31.306713Z",
     "start_time": "2025-07-31T03:27:29.877968Z"
    }
   },
   "cell_type": "code",
   "source": "order_items = order_repository.get_daily_sales(start_date=datetime.now() - timedelta(days=1), end_date=datetime.now())",
   "id": "8f173aa703c90ae2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:28:06.279214Z",
     "start_time": "2025-07-31T03:28:06.250634Z"
    }
   },
   "cell_type": "code",
   "source": "order_items.head()",
   "id": "b10cac2c8eca7bf3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date    location_id  order_count  customer_count total_revenue  \\\n",
       "0 2025-07-30  LVTJHAGAMD2ZY           67              55        999.53   \n",
       "1 2025-07-30  LAFZJ3CHGB11Z           45              28        807.16   \n",
       "2 2025-07-30  LWDYYDAMG4AH5           40              30        575.71   \n",
       "3 2025-07-30  LMF86PJ371T9T          248             157       6948.06   \n",
       "4 2025-07-30  L3T37TFC0S7WB          121              93       1650.83   \n",
       "\n",
       "   avg_order_value  new_customer_count  repeat_customer_count  \n",
       "0        11.622442                  28                     27  \n",
       "1        10.482597                  11                     17  \n",
       "2         9.757797                  14                     16  \n",
       "3        14.942065                  38                    119  \n",
       "4         9.597849                  36                     57  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location_id</th>\n",
       "      <th>order_count</th>\n",
       "      <th>customer_count</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>new_customer_count</th>\n",
       "      <th>repeat_customer_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LVTJHAGAMD2ZY</td>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "      <td>999.53</td>\n",
       "      <td>11.622442</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LAFZJ3CHGB11Z</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>807.16</td>\n",
       "      <td>10.482597</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LWDYYDAMG4AH5</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>575.71</td>\n",
       "      <td>9.757797</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>LMF86PJ371T9T</td>\n",
       "      <td>248</td>\n",
       "      <td>157</td>\n",
       "      <td>6948.06</td>\n",
       "      <td>14.942065</td>\n",
       "      <td>38</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>L3T37TFC0S7WB</td>\n",
       "      <td>121</td>\n",
       "      <td>93</td>\n",
       "      <td>1650.83</td>\n",
       "      <td>9.597849</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T06:05:22.899683Z",
     "start_time": "2025-07-31T06:05:14.313278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "因果推断数据准备脚本\n",
    "利用现有的connectors和repositories准备因果推断所需的数据\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "# project_root = Path(__file__).parent.parent\n",
    "# sys.path.append(str(project_root))\n",
    "\n",
    "from src.data.repositories import OrderRepository, CustomerRepository\n",
    "from src.data.connectors import ClickHouseConnector\n",
    "from config.settings import Settings\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CausalDataPreparator:\n",
    "    \"\"\"因果推断数据准备器\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = Settings()\n",
    "        self.ch_connector = ClickHouseConnector()\n",
    "        self.order_repo = OrderRepository()\n",
    "        self.customer_repo = CustomerRepository()\n",
    "\n",
    "    def prepare_causal_dataset(self, days_back: int = 180) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        准备因果推断数据集\n",
    "\n",
    "        Args:\n",
    "            days_back: 回溯天数，默认180天\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: 准备好的数据集\n",
    "        \"\"\"\n",
    "        logger.info(f\"开始准备因果推断数据集，回溯{days_back}天\")\n",
    "\n",
    "        try:\n",
    "            # 1. 获取基础数据\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=days_back)\n",
    "\n",
    "            # 获取销售数据\n",
    "            logger.info(\"获取销售数据...\")\n",
    "            sales_data = self._get_sales_data(start_date, end_date)\n",
    "\n",
    "            if sales_data.empty:\n",
    "                logger.error(\"无法获取销售数据，流程终止\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # 获取客户数据\n",
    "            logger.info(\"获取客户数据...\")\n",
    "            customer_data = self._get_customer_data()\n",
    "\n",
    "            # 获取商品数据\n",
    "            logger.info(\"获取商品数据...\")\n",
    "            item_data = self._get_item_data()\n",
    "\n",
    "            # 2. 数据合并和清理\n",
    "            logger.info(\"合并和清理数据...\")\n",
    "            merged_data = self._merge_and_clean_data(sales_data, customer_data, item_data)\n",
    "\n",
    "            if merged_data.empty:\n",
    "                logger.error(\"数据合并后为空，流程终止\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # 3. 创建处理变量\n",
    "            logger.info(\"创建处理变量...\")\n",
    "            try:\n",
    "                merged_data = self._create_treatment_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"创建处理变量失败: {e}\")\n",
    "                # 创建最基本的处理变量\n",
    "                merged_data['has_promotion'] = 0\n",
    "                merged_data['has_discount'] = 0\n",
    "                merged_data['vip_treatment'] = 0\n",
    "\n",
    "            # 4. 创建混淆变量\n",
    "            logger.info(\"创建混淆变量...\")\n",
    "            try:\n",
    "                merged_data = self._create_confounding_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"创建混淆变量失败: {e}\")\n",
    "                # 创建最基本的混淆变量\n",
    "                merged_data['is_weekend'] = 0\n",
    "                merged_data['hour_of_day'] = 12\n",
    "                merged_data['location_encoded'] = 0\n",
    "                merged_data['category_encoded'] = 0\n",
    "\n",
    "            # 5. 创建结果变量\n",
    "            logger.info(\"创建结果变量...\")\n",
    "            try:\n",
    "                merged_data = self._create_outcome_variables(merged_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"创建结果变量失败: {e}\")\n",
    "                # 确保至少有基本的结果变量\n",
    "                if 'revenue' in merged_data.columns:\n",
    "                    merged_data['log_revenue'] = np.log1p(merged_data['revenue'])\n",
    "\n",
    "            # 6. 最终数据验证\n",
    "            logger.info(\"数据验证...\")\n",
    "            validated_data = self._validate_data(merged_data)\n",
    "\n",
    "            if not validated_data.empty:\n",
    "                logger.info(f\"数据准备完成，共{len(validated_data)}条记录\")\n",
    "            else:\n",
    "                logger.error(\"数据验证后为空\")\n",
    "\n",
    "            return validated_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据准备过程中发生错误: {e}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _get_sales_data(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"获取销售数据\"\"\"\n",
    "        try:\n",
    "            # 使用现有的repository方法\n",
    "            sales_data = self.order_repo.get_daily_sales(start_date, end_date)\n",
    "\n",
    "            # 打印调试信息\n",
    "            if not sales_data.empty:\n",
    "                logger.info(f\"从repository获取到数据，形状: {sales_data.shape}\")\n",
    "                logger.info(f\"列名: {list(sales_data.columns)}\")\n",
    "                sales_data = self._standardize_column_names(sales_data)\n",
    "            else:\n",
    "                logger.info(\"Repository返回空数据，尝试直接查询...\")\n",
    "                # 尝试多种可能的查询\n",
    "                sales_data = self._try_direct_queries(start_date, end_date)\n",
    "\n",
    "            return sales_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取销售数据失败: {e}\")\n",
    "            logger.info(\"尝试使用模拟数据...\")\n",
    "            return self._create_sample_data(start_date, end_date)\n",
    "\n",
    "    def _try_direct_queries(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"尝试不同的直接查询方式\"\"\"\n",
    "\n",
    "        # 查询1：完整字段查询\n",
    "        queries = [\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "                customer_id,\n",
    "                item_name,\n",
    "                category_name,\n",
    "                location_id,\n",
    "                order_final_total_amt as revenue,\n",
    "                quantity,\n",
    "                toDate(created_at_pt) as date,\n",
    "                toHour(created_at_pt) as hour\n",
    "            FROM dw.fact_order_item_variations\n",
    "            WHERE created_at_pt >= '%(start_date)s'\n",
    "                AND created_at_pt < '%(end_date)s'\n",
    "                AND pay_status = 'COMPLETED'\n",
    "            LIMIT 1000\n",
    "            \"\"\",\n",
    "\n",
    "            # 查询2：简化字段查询\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM dw.fact_order_item_variations\n",
    "            WHERE created_at_pt >= '%(start_date)s'\n",
    "                AND created_at_pt < '%(end_date)s'\n",
    "            LIMIT 10\n",
    "            \"\"\",\n",
    "\n",
    "            # 查询3：表结构查询\n",
    "            \"\"\"\n",
    "            DESCRIBE TABLE dw.fact_order_item_variations\n",
    "            \"\"\"\n",
    "        ]\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "            try:\n",
    "                logger.info(f\"尝试查询{i+1}...\")\n",
    "                formatted_query = query % {\n",
    "                    'start_date': start_date.strftime('%Y-%m-%d'),\n",
    "                    'end_date': end_date.strftime('%Y-%m-%d')\n",
    "                }\n",
    "\n",
    "                result = self.ch_connector.query_df(formatted_query)\n",
    "\n",
    "                if not result.empty:\n",
    "                    logger.info(f\"查询{i+1}成功，形状: {result.shape}\")\n",
    "                    logger.info(f\"列名: {list(result.columns)}\")\n",
    "\n",
    "                    if i == 2:  # 表结构查询\n",
    "                        logger.info(\"表结构:\")\n",
    "                        for _, row in result.iterrows():\n",
    "                            logger.info(f\"  {row}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        return self._standardize_column_names(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"查询{i+1}失败: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def _standardize_column_names(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"标准化列名\"\"\"\n",
    "        # 列名映射字典\n",
    "        column_mapping = {\n",
    "            # customer相关\n",
    "            'customerId': 'customer_id',\n",
    "            'customer': 'customer_id',\n",
    "            'user_id': 'customer_id',\n",
    "            'userId': 'customer_id',\n",
    "\n",
    "            # item相关\n",
    "            'itemName': 'item_name',\n",
    "            'product_name': 'item_name',\n",
    "            'productName': 'item_name',\n",
    "\n",
    "            # category相关\n",
    "            'categoryName': 'category_name',\n",
    "            'category': 'category_name',\n",
    "\n",
    "            # location相关\n",
    "            'locationId': 'location_id',\n",
    "            'location': 'location_id',\n",
    "            'store_id': 'location_id',\n",
    "\n",
    "            # revenue相关\n",
    "            'total_amount': 'revenue',\n",
    "            'amount': 'revenue',\n",
    "            'order_amount': 'revenue',\n",
    "            'order_final_total_amt': 'revenue',\n",
    "\n",
    "            # date相关\n",
    "            'order_date': 'date',\n",
    "            'created_date': 'date',\n",
    "            'purchase_date': 'date'\n",
    "        }\n",
    "\n",
    "        # 应用映射\n",
    "        data = data.rename(columns=column_mapping)\n",
    "\n",
    "        # 如果还是没有customer_id，尝试创建\n",
    "        if 'customer_id' not in data.columns:\n",
    "            # 寻找可能的ID列\n",
    "            id_columns = [col for col in data.columns if 'id' in col.lower()]\n",
    "            if id_columns:\n",
    "                logger.info(f\"未找到customer_id，使用 {id_columns[0]} 作为customer_id\")\n",
    "                data['customer_id'] = data[id_columns[0]]\n",
    "            else:\n",
    "                logger.warning(\"未找到任何客户ID列，创建虚拟customer_id\")\n",
    "                # 创建虚拟customer_id\n",
    "                data['customer_id'] = 'CUST_' + pd.Series(range(len(data))).astype(str)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_sample_data(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"创建示例数据用于测试\"\"\"\n",
    "        logger.info(\"创建模拟数据进行测试...\")\n",
    "\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # 生成日期序列\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "        # 为每天生成多条记录\n",
    "        data_list = []\n",
    "        customer_ids = [f'CUST_{str(i).zfill(4)}' for i in range(1, 101)]  # 100个客户\n",
    "        items = ['商品A', '商品B', '商品C', '商品D', '商品E']\n",
    "        categories = ['电子产品', '服装', '食品', '家居', '图书']\n",
    "        locations = ['店铺1', '店铺2', '店铺3']\n",
    "\n",
    "        for date in date_range:\n",
    "            # 每天随机生成10-50条记录\n",
    "            daily_records = np.random.randint(10, 51)\n",
    "\n",
    "            for _ in range(daily_records):\n",
    "                data_list.append({\n",
    "                    'customer_id': np.random.choice(customer_ids),\n",
    "                    'item_name': np.random.choice(items),\n",
    "                    'category_name': np.random.choice(categories),\n",
    "                    'location_id': np.random.choice(locations),\n",
    "                    'revenue': np.random.lognormal(4, 0.5),  # 对数正态分布生成价格\n",
    "                    'quantity': np.random.randint(1, 5),\n",
    "                    'date': date.date(),\n",
    "                    'hour': np.random.randint(9, 22)  # 营业时间9-22点\n",
    "                })\n",
    "\n",
    "        sample_data = pd.DataFrame(data_list)\n",
    "        logger.info(f\"创建了 {len(sample_data)} 条模拟数据\")\n",
    "\n",
    "        return sample_data\n",
    "\n",
    "    def _get_customer_data(self) -> pd.DataFrame:\n",
    "        \"\"\"获取客户数据\"\"\"\n",
    "        try:\n",
    "            # 使用现有的repository方法\n",
    "            customer_data = self.customer_repo.get_customer_segments()\n",
    "\n",
    "            if not customer_data.empty:\n",
    "                logger.info(f\"从repository获取客户数据，形状: {customer_data.shape}\")\n",
    "                logger.info(f\"客户数据列名: {list(customer_data.columns)}\")\n",
    "                customer_data = self._standardize_column_names(customer_data)\n",
    "            else:\n",
    "                logger.info(\"Repository返回空客户数据，尝试直接查询...\")\n",
    "                # 尝试直接查询客户信息\n",
    "                queries = [\n",
    "                    \"\"\"\n",
    "                    SELECT\n",
    "                        customer_id,\n",
    "                        CASE\n",
    "                            WHEN high_value_customer = 1 THEN 'high_value'\n",
    "                            WHEN key_development_customer = 1 THEN 'key_development'\n",
    "                            WHEN regular_customer = 1 THEN 'regular'\n",
    "                            WHEN critical_win_back_customer = 1 THEN 'win_back'\n",
    "                            ELSE 'other'\n",
    "                        END as customer_segment,\n",
    "                        order_final_total_amt as lifetime_value,\n",
    "                        order_final_avg_amt as avg_order_value,\n",
    "                        order_final_total_cnt as total_orders,\n",
    "                        CASE WHEN high_value_customer = 1 THEN 1 ELSE 0 END as is_high_value\n",
    "                    FROM ads.customer_profile\n",
    "                    WHERE order_final_total_cnt > 0\n",
    "                    LIMIT 1000\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    SELECT * FROM ads.customer_profile LIMIT 10\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    DESCRIBE TABLE ads.customer_profile\n",
    "                    \"\"\"\n",
    "                ]\n",
    "\n",
    "                for i, query in enumerate(queries):\n",
    "                    try:\n",
    "                        logger.info(f\"尝试客户数据查询{i+1}...\")\n",
    "                        result = self.ch_connector.query_df(query)\n",
    "\n",
    "                        if not result.empty:\n",
    "                            logger.info(f\"客户数据查询{i+1}成功，形状: {result.shape}\")\n",
    "                            logger.info(f\"列名: {list(result.columns)}\")\n",
    "\n",
    "                            if i == 2:  # 表结构查询\n",
    "                                logger.info(\"客户表结构:\")\n",
    "                                for _, row in result.iterrows():\n",
    "                                    logger.info(f\"  {row}\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                customer_data = self._standardize_column_names(result)\n",
    "                                break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"客户数据查询{i+1}失败: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # 如果还是没有数据，返回空DataFrame\n",
    "            if customer_data.empty:\n",
    "                logger.warning(\"无法获取客户数据，将在后续步骤中创建默认客户信息\")\n",
    "\n",
    "            return customer_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取客户数据失败: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _get_item_data(self) -> pd.DataFrame:\n",
    "        \"\"\"获取商品数据\"\"\"\n",
    "        try:\n",
    "            # 获取商品表现数据\n",
    "            item_data = self.order_repo.get_item_performance(days=30)\n",
    "\n",
    "            if not item_data.empty:\n",
    "                logger.info(f\"从repository获取商品数据，形状: {item_data.shape}\")\n",
    "                logger.info(f\"商品数据列名: {list(item_data.columns)}\")\n",
    "                item_data = self._standardize_column_names(item_data)\n",
    "            else:\n",
    "                logger.info(\"Repository返回空商品数据，尝试直接查询...\")\n",
    "                # 直接查询商品信息\n",
    "                queries = [\n",
    "                    \"\"\"\n",
    "                    SELECT\n",
    "                        item_name,\n",
    "                        category_name,\n",
    "                        AVG(order_final_total_amt) as avg_item_revenue,\n",
    "                        COUNT(DISTINCT customer_id) as unique_buyers,\n",
    "                        SUM(quantity) as total_quantity\n",
    "                    FROM dw.fact_order_item_variations\n",
    "                    WHERE created_at_pt >= today() - 30\n",
    "                        AND pay_status = 'COMPLETED'\n",
    "                    GROUP BY item_name, category_name\n",
    "                    LIMIT 100\n",
    "                    \"\"\",\n",
    "\n",
    "                    \"\"\"\n",
    "                    SELECT DISTINCT item_name, category_name\n",
    "                    FROM dw.fact_order_item_variations\n",
    "                    LIMIT 50\n",
    "                    \"\"\"\n",
    "                ]\n",
    "\n",
    "                for i, query in enumerate(queries):\n",
    "                    try:\n",
    "                        logger.info(f\"尝试商品数据查询{i+1}...\")\n",
    "                        result = self.ch_connector.query_df(query)\n",
    "\n",
    "                        if not result.empty:\n",
    "                            logger.info(f\"商品数据查询{i+1}成功，形状: {result.shape}\")\n",
    "                            item_data = self._standardize_column_names(result)\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"商品数据查询{i+1}失败: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return item_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取商品数据失败: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _merge_and_clean_data(self, sales_data: pd.DataFrame,\n",
    "                             customer_data: pd.DataFrame,\n",
    "                             item_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"合并和清理数据\"\"\"\n",
    "        if sales_data.empty:\n",
    "            logger.warning(\"销售数据为空，无法继续\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        logger.info(f\"开始合并数据 - 销售数据: {sales_data.shape}\")\n",
    "        merged_data = sales_data.copy()\n",
    "\n",
    "        # 确保基本字段存在\n",
    "        if 'customer_id' not in merged_data.columns:\n",
    "            logger.error(\"销售数据中缺少customer_id字段\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if 'revenue' not in merged_data.columns:\n",
    "            # 尝试寻找收入相关字段\n",
    "            revenue_cols = [col for col in merged_data.columns if any(keyword in col.lower() for keyword in ['amount', 'revenue', 'total', 'price'])]\n",
    "            if revenue_cols:\n",
    "                merged_data['revenue'] = merged_data[revenue_cols[0]]\n",
    "                logger.info(f\"使用 {revenue_cols[0]} 作为revenue字段\")\n",
    "            else:\n",
    "                logger.error(\"无法找到收入相关字段\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        # 合并客户数据\n",
    "        if not customer_data.empty and 'customer_id' in customer_data.columns:\n",
    "            logger.info(f\"合并客户数据: {customer_data.shape}\")\n",
    "            try:\n",
    "                merged_data = merged_data.merge(\n",
    "                    customer_data,\n",
    "                    on='customer_id',\n",
    "                    how='left'\n",
    "                )\n",
    "                logger.info(f\"合并后形状: {merged_data.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"客户数据合并失败: {e}\")\n",
    "        else:\n",
    "            logger.info(\"客户数据为空或缺少customer_id，创建默认客户分群\")\n",
    "            # 创建默认客户分群\n",
    "            merged_data['customer_segment'] = 'regular'\n",
    "            merged_data['is_high_value'] = 0\n",
    "\n",
    "        # 合并商品数据\n",
    "        if not item_data.empty:\n",
    "            merge_keys = []\n",
    "            if 'item_name' in merged_data.columns and 'item_name' in item_data.columns:\n",
    "                merge_keys.append('item_name')\n",
    "            if 'category_name' in merged_data.columns and 'category_name' in item_data.columns:\n",
    "                merge_keys.append('category_name')\n",
    "\n",
    "            if merge_keys:\n",
    "                logger.info(f\"使用 {merge_keys} 合并商品数据\")\n",
    "                try:\n",
    "                    merged_data = merged_data.merge(\n",
    "                        item_data[merge_keys + ['avg_item_revenue']] if 'avg_item_revenue' in item_data.columns else item_data[merge_keys],\n",
    "                        on=merge_keys,\n",
    "                        how='left'\n",
    "                    )\n",
    "                    logger.info(f\"商品数据合并后形状: {merged_data.shape}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"商品数据合并失败: {e}\")\n",
    "\n",
    "        # 数据清理\n",
    "        logger.info(\"开始数据清理...\")\n",
    "        original_count = len(merged_data)\n",
    "\n",
    "        # 移除缺失关键字段的记录\n",
    "        required_fields = ['customer_id', 'revenue']\n",
    "        available_fields = [field for field in required_fields if field in merged_data.columns]\n",
    "\n",
    "        if available_fields:\n",
    "            merged_data = merged_data.dropna(subset=available_fields)\n",
    "            logger.info(f\"移除缺失值后: {len(merged_data)} 条记录 (原始: {original_count})\")\n",
    "\n",
    "        # 移除异常值（比如负收入或0收入）\n",
    "        if 'revenue' in merged_data.columns:\n",
    "            before_filter = len(merged_data)\n",
    "            merged_data = merged_data[merged_data['revenue'] > 0]\n",
    "            logger.info(f\"移除异常收入后: {len(merged_data)} 条记录 (过滤前: {before_filter})\")\n",
    "\n",
    "        # 填充缺失值\n",
    "        if 'location_id' in merged_data.columns:\n",
    "            merged_data['location_id'] = merged_data['location_id'].fillna('unknown')\n",
    "        else:\n",
    "            merged_data['location_id'] = 'unknown'\n",
    "\n",
    "        if 'category_name' in merged_data.columns:\n",
    "            merged_data['category_name'] = merged_data['category_name'].fillna('other')\n",
    "        else:\n",
    "            merged_data['category_name'] = 'other'\n",
    "\n",
    "        if 'item_name' in merged_data.columns:\n",
    "            merged_data['item_name'] = merged_data['item_name'].fillna('unknown_item')\n",
    "        else:\n",
    "            merged_data['item_name'] = 'unknown_item'\n",
    "\n",
    "        logger.info(f\"数据清理完成，最终形状: {merged_data.shape}\")\n",
    "        return merged_data\n",
    "\n",
    "    def _create_treatment_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"创建处理变量\"\"\"\n",
    "        # 1. 促销活动标记（基于收入异常检测）\n",
    "        # 计算每日平均收入\n",
    "        daily_revenue = data.groupby('date')['revenue'].sum().reset_index()\n",
    "        daily_avg = daily_revenue['revenue'].mean()\n",
    "        daily_std = daily_revenue['revenue'].std()\n",
    "\n",
    "        # 高收入日期可能有促销活动\n",
    "        promotion_threshold = daily_avg + 1.5 * daily_std\n",
    "        promotion_dates = daily_revenue[daily_revenue['revenue'] > promotion_threshold]['date'].tolist()\n",
    "\n",
    "        data['has_promotion'] = data['date'].isin(promotion_dates).astype(int)\n",
    "\n",
    "        # 2. 折扣活动标记（基于低于平均商品价格）\n",
    "        if 'avg_item_revenue' in data.columns:\n",
    "            data['has_discount'] = (data['revenue'] < data['avg_item_revenue'] * 0.8).astype(int)\n",
    "        else:\n",
    "            data['has_discount'] = 0\n",
    "\n",
    "        # 3. 会员专享活动（针对高价值客户）\n",
    "        data['vip_treatment'] = ((data.get('is_high_value', 0) == 1) &\n",
    "                                (data['has_promotion'] == 1)).astype(int)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_confounding_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"创建混淆变量\"\"\"\n",
    "        # 1. 时间相关变量\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['is_weekend'] = data['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        data['month'] = data['date'].dt.month\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "        if 'hour' in data.columns:\n",
    "            data['hour_of_day'] = data['hour']\n",
    "        else:\n",
    "            data['hour_of_day'] = 12  # 默认中午\n",
    "\n",
    "        # 2. 地理位置\n",
    "        data['location_encoded'] = pd.Categorical(data['location_id']).codes\n",
    "\n",
    "        # 3. 商品类别\n",
    "        data['category_encoded'] = pd.Categorical(data['category_name']).codes\n",
    "\n",
    "        # 4. 客户特征\n",
    "        data['customer_segment_encoded'] = pd.Categorical(\n",
    "            data.get('customer_segment', 'regular')\n",
    "        ).codes\n",
    "\n",
    "        # 5. 历史购买行为（如果有的话）\n",
    "        if 'total_orders' in data.columns:\n",
    "            data['is_frequent_buyer'] = (data['total_orders'] > data['total_orders'].median()).astype(int)\n",
    "        else:\n",
    "            data['is_frequent_buyer'] = 0\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _create_outcome_variables(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"创建结果变量\"\"\"\n",
    "        # 1. 主要结果变量 - 收入\n",
    "        data['log_revenue'] = np.log1p(data['revenue'])  # 对数变换减少偏态\n",
    "\n",
    "        # 2. 订单量（如果有quantity字段）\n",
    "        if 'quantity' in data.columns:\n",
    "            data['order_quantity'] = data['quantity']\n",
    "        else:\n",
    "            data['order_quantity'] = 1  # 默认每个记录为1个订单\n",
    "\n",
    "        # 3. 客户价值相关\n",
    "        if 'avg_order_value' in data.columns:\n",
    "            data['customer_value_change'] = data['revenue'] - data['avg_order_value']\n",
    "        else:\n",
    "            customer_avg = data.groupby('customer_id')['revenue'].transform('mean')\n",
    "            data['customer_value_change'] = data['revenue'] - customer_avg\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _validate_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"验证数据质量\"\"\"\n",
    "        if data.empty:\n",
    "            logger.error(\"数据集为空，无法进行验证\")\n",
    "            return data\n",
    "\n",
    "        logger.info(f\"数据集基本信息:\")\n",
    "        logger.info(f\"- 总记录数: {len(data)}\")\n",
    "        logger.info(f\"- 总列数: {len(data.columns)}\")\n",
    "        logger.info(f\"- 列名: {list(data.columns)}\")\n",
    "\n",
    "        # 检查customer_id\n",
    "        if 'customer_id' in data.columns:\n",
    "            logger.info(f\"- 唯一客户数: {data['customer_id'].nunique()}\")\n",
    "        else:\n",
    "            logger.warning(\"- 缺少customer_id字段\")\n",
    "\n",
    "        # 检查日期范围\n",
    "        if 'date' in data.columns:\n",
    "            try:\n",
    "                data['date'] = pd.to_datetime(data['date'])\n",
    "                logger.info(f\"- 日期范围: {data['date'].min()} 到 {data['date'].max()}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"日期字段处理失败: {e}\")\n",
    "        else:\n",
    "            logger.warning(\"- 缺少date字段\")\n",
    "\n",
    "        # 检查关键变量\n",
    "        required_vars = ['customer_id', 'revenue']\n",
    "        available_vars = [var for var in required_vars if var in data.columns]\n",
    "        missing_vars = [var for var in required_vars if var not in data.columns]\n",
    "\n",
    "        if available_vars:\n",
    "            logger.info(f\"- 可用关键变量: {available_vars}\")\n",
    "        if missing_vars:\n",
    "            logger.warning(f\"- 缺少关键变量: {missing_vars}\")\n",
    "\n",
    "        # 检查处理变量分布\n",
    "        if 'has_promotion' in data.columns:\n",
    "            promo_dist = data['has_promotion'].value_counts()\n",
    "            logger.info(f\"- 促销分布: {promo_dist.to_dict()}\")\n",
    "        else:\n",
    "            logger.info(\"- 促销变量尚未创建\")\n",
    "\n",
    "        # 检查收入分布\n",
    "        if 'revenue' in data.columns:\n",
    "            logger.info(f\"- 收入统计: 均值={data['revenue'].mean():.2f}, 中位数={data['revenue'].median():.2f}\")\n",
    "            logger.info(f\"- 收入范围: {data['revenue'].min():.2f} - {data['revenue'].max():.2f}\")\n",
    "\n",
    "        # 移除完全重复的记录\n",
    "        before_count = len(data)\n",
    "        data = data.drop_duplicates()\n",
    "        after_count = len(data)\n",
    "\n",
    "        if before_count != after_count:\n",
    "            logger.info(f\"- 移除了 {before_count - after_count} 条重复记录\")\n",
    "\n",
    "        # 检查是否有足够的数据进行因果推断\n",
    "        if len(data) < 100:\n",
    "            logger.warning(\"数据量较少，可能影响因果推断的准确性\")\n",
    "        elif len(data) < 1000:\n",
    "            logger.info(\"数据量适中，可以进行基础因果推断\")\n",
    "        else:\n",
    "            logger.info(\"数据量充足，可以进行稳健的因果推断\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_dataset(self, data: pd.DataFrame, output_path: str = \"data/causal_dataset.csv\") -> None:\n",
    "        \"\"\"保存数据集\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        data.to_csv(output_path, index=False)\n",
    "        logger.info(f\"数据集已保存到: {output_path}\")\n",
    "\n",
    "        # 保存数据字典\n",
    "        dict_path = output_path.replace('.csv', '_dictionary.txt')\n",
    "        with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"因果推断数据集字典\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"处理变量 (Treatment Variables):\\n\")\n",
    "            f.write(\"- has_promotion: 是否有促销活动 (0/1)\\n\")\n",
    "            f.write(\"- has_discount: 是否有折扣 (0/1)\\n\")\n",
    "            f.write(\"- vip_treatment: VIP专享活动 (0/1)\\n\\n\")\n",
    "\n",
    "            f.write(\"结果变量 (Outcome Variables):\\n\")\n",
    "            f.write(\"- revenue: 收入金额\\n\")\n",
    "            f.write(\"- log_revenue: 对数收入\\n\")\n",
    "            f.write(\"- order_quantity: 订单数量\\n\\n\")\n",
    "\n",
    "            f.write(\"混淆变量 (Confounding Variables):\\n\")\n",
    "            f.write(\"- is_weekend: 是否周末 (0/1)\\n\")\n",
    "            f.write(\"- hour_of_day: 小时\\n\")\n",
    "            f.write(\"- location_encoded: 地理位置编码\\n\")\n",
    "            f.write(\"- category_encoded: 商品类别编码\\n\")\n",
    "            f.write(\"- customer_segment_encoded: 客户分群编码\\n\")\n",
    "\n",
    "        logger.info(f\"数据字典已保存到: {dict_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    try:\n",
    "        logger.info(\"开始初始化数据准备器...\")\n",
    "\n",
    "        # 初始化数据准备器\n",
    "        try:\n",
    "            preparator = CausalDataPreparator()\n",
    "            logger.info(\"数据准备器初始化成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据准备器初始化失败: {e}\")\n",
    "            logger.info(\"这可能是因为配置文件或数据库连接问题\")\n",
    "            return\n",
    "\n",
    "        # 准备数据集\n",
    "        logger.info(\"开始准备数据集...\")\n",
    "        dataset = preparator.prepare_causal_dataset(days_back=180)\n",
    "\n",
    "        if not dataset.empty:\n",
    "            # 保存数据集\n",
    "            logger.info(\"保存数据集...\")\n",
    "            preparator.save_dataset(dataset)\n",
    "\n",
    "            # 显示基本统计\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"数据集准备完成!\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"数据形状: {dataset.shape}\")\n",
    "            print(f\"列名: {list(dataset.columns)}\")\n",
    "\n",
    "            # 显示数据预览\n",
    "            print(f\"\\n前5行预览:\")\n",
    "            print(dataset.head())\n",
    "\n",
    "            # 显示数据类型\n",
    "            print(f\"\\n数据类型:\")\n",
    "            print(dataset.dtypes)\n",
    "\n",
    "            # 显示缺失值统计\n",
    "            print(f\"\\n缺失值统计:\")\n",
    "            missing_stats = dataset.isnull().sum()\n",
    "            print(missing_stats[missing_stats > 0])\n",
    "\n",
    "            print(f\"\\n数据已保存到: data/causal_dataset.csv\")\n",
    "            print(f\"数据字典已保存到: data/causal_dataset_dictionary.txt\")\n",
    "\n",
    "        else:\n",
    "            logger.error(\"数据集为空，请检查以下可能的问题:\")\n",
    "            logger.error(\"1. 数据库连接是否正常\")\n",
    "            logger.error(\"2. 表名和字段名是否正确\")\n",
    "            logger.error(\"3. 数据是否存在于指定的时间范围内\")\n",
    "            logger.error(\"4. 权限是否足够访问相关表\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"数据准备失败: {e}\", exc_info=True)\n",
    "        logger.info(\"\\n调试建议:\")\n",
    "        logger.info(\"1. 检查config/config.json中的数据库配置\")\n",
    "        logger.info(\"2. 确认数据库表是否存在并可访问\")\n",
    "        logger.info(\"3. 检查网络连接和防火墙设置\")\n",
    "        logger.info(\"4. 查看完整的错误堆栈信息\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "3e41167cb30f70a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 14:05:14,334 - __main__ - INFO - 开始初始化数据准备器...\n",
      "2025-07-31 14:05:14,335 - config.settings - INFO - Config loaded from /Users/xander/PycharmProjects/FBR_AI_Agine/config/config.json\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - 数据准备器初始化成功\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - 开始准备数据集...\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - 开始准备因果推断数据集，回溯180天\n",
      "2025-07-31 14:05:14,337 - __main__ - INFO - 获取销售数据...\n",
      "2025-07-31 14:05:16,662 - src.data.connectors - INFO - Connected to ClickHouse using clickhouse_connect: clickhouse-0-0.umetea.net:443\n",
      "2025-07-31 14:05:17,577 - __main__ - INFO - 从repository获取到数据，形状: (4061, 8)\n",
      "2025-07-31 14:05:17,578 - __main__ - INFO - 列名: ['date', 'location_id', 'order_count', 'customer_count', 'total_revenue', 'avg_order_value', 'new_customer_count', 'repeat_customer_count']\n",
      "2025-07-31 14:05:17,583 - __main__ - INFO - 未找到customer_id，使用 location_id 作为customer_id\n",
      "2025-07-31 14:05:17,585 - __main__ - INFO - 获取客户数据...\n",
      "2025-07-31 14:05:21,732 - src.data.connectors - INFO - Connected to ClickHouse using clickhouse_connect: clickhouse-0-0.umetea.net:443\n",
      "2025-07-31 14:05:22,043 - __main__ - INFO - 从repository获取客户数据，形状: (5, 5)\n",
      "2025-07-31 14:05:22,044 - __main__ - INFO - 客户数据列名: ['segment', 'customer_count', 'total_revenue', 'avg_order_value', 'avg_order_count']\n",
      "2025-07-31 14:05:22,045 - __main__ - WARNING - 未找到任何客户ID列，创建虚拟customer_id\n",
      "2025-07-31 14:05:22,049 - __main__ - INFO - 获取商品数据...\n",
      "2025-07-31 14:05:22,863 - __main__ - INFO - 从repository获取商品数据，形状: (380, 9)\n",
      "2025-07-31 14:05:22,864 - __main__ - INFO - 商品数据列名: ['item_id', 'item_name', 'category_name', 'order_count', 'units_sold', 'revenue', 'avg_price', 'total_discount', 'unique_buyers']\n",
      "2025-07-31 14:05:22,864 - __main__ - INFO - 未找到customer_id，使用 item_id 作为customer_id\n",
      "2025-07-31 14:05:22,865 - __main__ - INFO - 合并和清理数据...\n",
      "2025-07-31 14:05:22,865 - __main__ - INFO - 开始合并数据 - 销售数据: (4061, 9)\n",
      "2025-07-31 14:05:22,866 - __main__ - INFO - 使用 total_revenue 作为revenue字段\n",
      "2025-07-31 14:05:22,866 - __main__ - INFO - 合并客户数据: (5, 6)\n",
      "2025-07-31 14:05:22,876 - __main__ - INFO - 合并后形状: (4061, 15)\n",
      "2025-07-31 14:05:22,876 - __main__ - INFO - 开始数据清理...\n",
      "2025-07-31 14:05:22,879 - __main__ - INFO - 移除缺失值后: 4061 条记录 (原始: 4061)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - 移除异常收入后: 4057 条记录 (过滤前: 4061)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - 数据清理完成，最终形状: (4057, 17)\n",
      "2025-07-31 14:05:22,881 - __main__ - INFO - 创建处理变量...\n",
      "2025-07-31 14:05:22,885 - __main__ - ERROR - 创建处理变量失败: unsupported operand type(s) for -: 'float' and 'decimal.Decimal'\n",
      "2025-07-31 14:05:22,886 - __main__ - INFO - 创建混淆变量...\n",
      "2025-07-31 14:05:22,893 - __main__ - ERROR - 创建混淆变量失败: Categorical input must be list-like\n",
      "2025-07-31 14:05:22,893 - __main__ - INFO - 创建结果变量...\n",
      "2025-07-31 14:05:22,894 - __main__ - ERROR - 创建结果变量失败: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "2025-07-31 14:05:22,894 - __main__ - ERROR - 数据准备过程中发生错误: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "AttributeError: 'decimal.Decimal' object has no attribute 'log1p'. Did you mean: 'log10'?\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 107, in prepare_causal_dataset\n",
      "    merged_data = self._create_outcome_variables(merged_data)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 599, in _create_outcome_variables\n",
      "    data['log_revenue'] = np.log1p(data['revenue'])  # 对数变换减少偏态\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py\", line 2190, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/arraylike.py\", line 399, in array_ufunc\n",
      "    result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "AttributeError: 'decimal.Decimal' object has no attribute 'log1p'. Did you mean: 'log10'?\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9_/rff4k37d3vvds76m5ltzbqj00000gp/T/ipykernel_42580/3368187267.py\", line 112, in prepare_causal_dataset\n",
      "    merged_data['log_revenue'] = np.log1p(merged_data['revenue'])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py\", line 2190, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/xander/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/arraylike.py\", line 399, in array_ufunc\n",
      "    result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable log1p method\n",
      "2025-07-31 14:05:22,897 - __main__ - ERROR - 数据集为空，请检查以下可能的问题:\n",
      "2025-07-31 14:05:22,897 - __main__ - ERROR - 1. 数据库连接是否正常\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 2. 表名和字段名是否正确\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 3. 数据是否存在于指定的时间范围内\n",
      "2025-07-31 14:05:22,898 - __main__ - ERROR - 4. 权限是否足够访问相关表\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b9888a15f5f63df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
