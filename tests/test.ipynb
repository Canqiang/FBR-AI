{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:52:17.047599Z",
     "start_time": "2025-07-30T09:52:15.112702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.repositories import OrderRepository\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# causal_business_examples.py\n",
    "\"\"\"因果推断在实际业务中的应用示例\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from src.analytics.advanced_causal_engine import (\n",
    "    AdvancedCausalEngine,\n",
    "    CounterfactualScenario,\n",
    "    CausalEffect\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CausalBusinessAnalyzer:\n",
    "    \"\"\"业务场景的因果分析器\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.engine = AdvancedCausalEngine()\n",
    "        self.analysis_results = {}\n",
    "\n",
    "    def analyze_sales_decline(self, business_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"分析销售下滑的因果关系和反事实场景\"\"\"\n",
    "\n",
    "        logger.info(\"开始分析销售下滑问题...\")\n",
    "\n",
    "        # 1. 构建销售因果模型\n",
    "        sales_model = self.engine.create_causal_model(\n",
    "            data=business_data,\n",
    "            treatment=['promotion_active', 'price_level'],\n",
    "            outcome='daily_revenue',\n",
    "            common_causes=[\n",
    "                'day_of_week',\n",
    "                'weather_condition',\n",
    "                'competitor_activity',\n",
    "                'inventory_availability',\n",
    "                'store_traffic'\n",
    "            ],\n",
    "            effect_modifiers=['customer_segment', 'product_type']\n",
    "        )\n",
    "\n",
    "        # 2. 估计当前因果效应\n",
    "        causal_effects = self.engine.estimate_causal_effect(\n",
    "            sales_model,\n",
    "            methods=[\n",
    "                \"backdoor.propensity_score_matching\",\n",
    "                \"backdoor.linear_regression\",\n",
    "                \"backdoor.propensity_score_weighting\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. 反事实分析：如果我们采取不同的策略会怎样？\n",
    "        counterfactual_scenarios = []\n",
    "\n",
    "        # 场景1：如果我们提高促销力度\n",
    "        counterfactual_scenarios.append({\n",
    "            'scenario': CounterfactualScenario(\n",
    "                scenario_name=\"增强促销策略\",\n",
    "                treatment_change={\n",
    "                    'promotion_active': 1.0,  # 全面启动促销\n",
    "                    'price_level': -0.15      # 降价15%\n",
    "                },\n",
    "                context={\n",
    "                    'target_segment': 'all',\n",
    "                    'expected_cost': 50000,\n",
    "                    'duration_days': 7\n",
    "                }\n",
    "            ),\n",
    "            'business_question': \"如果我们实施全场85折促销，销售额会增加多少？\"\n",
    "        })\n",
    "\n",
    "        # 场景2：如果竞争对手没有促销\n",
    "        counterfactual_scenarios.append({\n",
    "            'scenario': CounterfactualScenario(\n",
    "                scenario_name=\"竞争环境改善\",\n",
    "                treatment_change={\n",
    "                    'competitor_activity': -1.0  # 竞争对手停止促销\n",
    "                },\n",
    "                context={\n",
    "                    'market_condition': 'favorable',\n",
    "                    'probability': 0.3\n",
    "                }\n",
    "            ),\n",
    "            'business_question': \"如果竞争对手停止促销活动，我们的销售会恢复多少？\"\n",
    "        })\n",
    "\n",
    "        # 场景3：如果天气转好且库存充足\n",
    "        counterfactual_scenarios.append({\n",
    "            'scenario': CounterfactualScenario(\n",
    "                scenario_name=\"理想运营条件\",\n",
    "                treatment_change={\n",
    "                    'weather_condition': 1.0,      # 好天气\n",
    "                    'inventory_availability': 1.0   # 库存充足\n",
    "                },\n",
    "                context={\n",
    "                    'operational_readiness': 'optimal',\n",
    "                    'feasibility': 'high'\n",
    "                }\n",
    "            ),\n",
    "            'business_question': \"在理想条件下（好天气+充足库存），销售潜力有多大？\"\n",
    "        })\n",
    "\n",
    "        # 4. 执行反事实分析\n",
    "        counterfactual_results = {}\n",
    "        for cf_item in counterfactual_scenarios:\n",
    "            scenario = cf_item['scenario']\n",
    "            result = self.engine.perform_counterfactual_analysis(\n",
    "                sales_model,\n",
    "                scenario,\n",
    "                sample_data=business_data.tail(30)  # 使用最近30天数据\n",
    "            )\n",
    "            result['business_question'] = cf_item['business_question']\n",
    "            counterfactual_results[scenario.scenario_name] = result\n",
    "\n",
    "        # 5. 执行敏感性分析\n",
    "        sensitivity = self.engine.perform_sensitivity_analysis(\n",
    "            sales_model,\n",
    "            list(causal_effects.values())[0]  # 使用第一个估计结果\n",
    "        )\n",
    "\n",
    "        # 6. 生成业务洞察和建议\n",
    "        insights = self._generate_sales_insights(\n",
    "            causal_effects,\n",
    "            counterfactual_results,\n",
    "            sensitivity\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'causal_effects': causal_effects,\n",
    "            'counterfactual_analysis': counterfactual_results,\n",
    "            'sensitivity_analysis': sensitivity,\n",
    "            'business_insights': insights,\n",
    "            'recommended_actions': self._prioritize_actions(counterfactual_results)\n",
    "        }\n",
    "\n",
    "    def analyze_customer_churn(self, customer_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"分析客户流失的因果关系\"\"\"\n",
    "\n",
    "        logger.info(\"开始分析客户流失问题...\")\n",
    "\n",
    "        # 1. 构建客户流失因果模型\n",
    "        churn_model = self.engine.create_causal_model(\n",
    "            data=customer_data,\n",
    "            treatment='received_retention_offer',\n",
    "            outcome='churned',\n",
    "            common_causes=[\n",
    "                'customer_lifetime_value',\n",
    "                'recent_support_tickets',\n",
    "                'days_since_last_purchase',\n",
    "                'total_purchases',\n",
    "                'satisfaction_score'\n",
    "            ],\n",
    "            instruments=['random_campaign_assignment']  # 使用随机分配作为工具变量\n",
    "        )\n",
    "\n",
    "        # 2. 估计挽回策略的因果效应\n",
    "        retention_effects = self.engine.estimate_causal_effect(\n",
    "            churn_model,\n",
    "            methods=[\"iv.instrumental_variable\", \"backdoor.propensity_score_matching\"]\n",
    "        )\n",
    "\n",
    "        # 3. 反事实分析：不同挽回策略的效果\n",
    "        retention_scenarios = [\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"个性化优惠券策略\",\n",
    "                treatment_change={'received_retention_offer': 1.0},\n",
    "                context={\n",
    "                    'offer_type': 'personalized_discount',\n",
    "                    'discount_amount': 0.25,\n",
    "                    'cost_per_customer': 20\n",
    "                }\n",
    "            ),\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"VIP升级策略\",\n",
    "                treatment_change={'received_retention_offer': 1.0},\n",
    "                context={\n",
    "                    'offer_type': 'vip_upgrade',\n",
    "                    'benefits': ['free_shipping', 'exclusive_access'],\n",
    "                    'cost_per_customer': 50\n",
    "                }\n",
    "            ),\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"积分奖励策略\",\n",
    "                treatment_change={'received_retention_offer': 1.0},\n",
    "                context={\n",
    "                    'offer_type': 'bonus_points',\n",
    "                    'points_multiplier': 3,\n",
    "                    'cost_per_customer': 15\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 4. 分析每个策略的效果\n",
    "        strategy_results = {}\n",
    "        for scenario in retention_scenarios:\n",
    "            # 只对高风险客户群体进行分析\n",
    "            high_risk_customers = customer_data[\n",
    "                customer_data['churn_probability'] > 0.7\n",
    "            ]\n",
    "\n",
    "            result = self.engine.perform_counterfactual_analysis(\n",
    "                churn_model,\n",
    "                scenario,\n",
    "                sample_data=high_risk_customers\n",
    "            )\n",
    "\n",
    "            # 计算ROI\n",
    "            prevented_churns = -result['aggregate_impact']['total_change']\n",
    "            revenue_saved = prevented_churns * customer_data['customer_lifetime_value'].mean()\n",
    "            total_cost = len(high_risk_customers) * scenario.context['cost_per_customer']\n",
    "            roi = (revenue_saved - total_cost) / total_cost\n",
    "\n",
    "            result['financial_impact'] = {\n",
    "                'prevented_churns': prevented_churns,\n",
    "                'revenue_saved': revenue_saved,\n",
    "                'total_cost': total_cost,\n",
    "                'roi': roi\n",
    "            }\n",
    "\n",
    "            strategy_results[scenario.scenario_name] = result\n",
    "\n",
    "        return {\n",
    "            'retention_effects': retention_effects,\n",
    "            'strategy_comparison': strategy_results,\n",
    "            'optimal_strategy': self._find_optimal_retention_strategy(strategy_results),\n",
    "            'segmented_recommendations': self._segment_retention_recommendations(\n",
    "                customer_data,\n",
    "                strategy_results\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def analyze_pricing_decisions(self, pricing_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"分析定价决策的因果影响\"\"\"\n",
    "\n",
    "        logger.info(\"开始分析定价策略...\")\n",
    "\n",
    "        # 1. 构建价格弹性模型\n",
    "        pricing_model = self.engine.create_causal_model(\n",
    "            data=pricing_data,\n",
    "            treatment='price',\n",
    "            outcome='units_sold',\n",
    "            common_causes=[\n",
    "                'product_category',\n",
    "                'competitor_price',\n",
    "                'seasonality',\n",
    "                'promotion_active',\n",
    "                'inventory_level'\n",
    "            ],\n",
    "            effect_modifiers=['customer_segment', 'time_of_day']\n",
    "        )\n",
    "\n",
    "        # 2. 估计价格弹性\n",
    "        price_effects = self.engine.estimate_causal_effect(pricing_model)\n",
    "\n",
    "        # 3. What-if 分析：不同定价策略\n",
    "        pricing_scenarios = [\n",
    "            {\n",
    "                'name': '激进降价策略',\n",
    "                'treatment': 'price',\n",
    "                'outcome': 'units_sold',\n",
    "                'changes': {'price': -0.20},  # 降价20%\n",
    "                'confounders': ['competitor_price', 'seasonality']\n",
    "            },\n",
    "            {\n",
    "                'name': '温和涨价策略',\n",
    "                'treatment': 'price',\n",
    "                'outcome': 'units_sold',\n",
    "                'changes': {'price': 0.05},   # 涨价5%\n",
    "                'confounders': ['competitor_price', 'seasonality']\n",
    "            },\n",
    "            {\n",
    "                'name': '动态定价策略',\n",
    "                'treatment': 'price',\n",
    "                'outcome': 'units_sold',\n",
    "                'changes': {'price': 'dynamic'},  # 根据需求动态调整\n",
    "                'confounders': ['competitor_price', 'seasonality', 'time_of_day']\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # 4. 批量What-if分析\n",
    "        what_if_results = self.engine.what_if_analysis(\n",
    "            pricing_data,\n",
    "            pricing_scenarios\n",
    "        )\n",
    "\n",
    "        # 5. 计算最优价格点\n",
    "        optimal_price = self._find_optimal_price(\n",
    "            pricing_data,\n",
    "            price_effects,\n",
    "            what_if_results\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'price_elasticity': price_effects,\n",
    "            'scenario_analysis': what_if_results,\n",
    "            'optimal_price': optimal_price,\n",
    "            'implementation_roadmap': self._create_pricing_roadmap(optimal_price)\n",
    "        }\n",
    "\n",
    "    def analyze_inventory_optimization(self, inventory_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"库存优化的因果分析\"\"\"\n",
    "\n",
    "        logger.info(\"开始分析库存优化策略...\")\n",
    "\n",
    "        # 使用引擎的库存分析方法\n",
    "        inventory_results = self.engine.analyze_inventory_decisions(inventory_data)\n",
    "\n",
    "        # 添加额外的业务分析\n",
    "        inventory_results['seasonal_adjustments'] = self._analyze_seasonal_inventory(\n",
    "            inventory_data\n",
    "        )\n",
    "\n",
    "        inventory_results['supplier_recommendations'] = self._analyze_supplier_impact(\n",
    "            inventory_data\n",
    "        )\n",
    "\n",
    "        return inventory_results\n",
    "\n",
    "    def analyze_marketing_effectiveness(self, marketing_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"营销效果的因果分析\"\"\"\n",
    "\n",
    "        logger.info(\"开始分析营销效果...\")\n",
    "\n",
    "        # 1. 多渠道归因模型\n",
    "        marketing_model = self.engine.create_causal_model(\n",
    "            data=marketing_data,\n",
    "            treatment=['email_sent', 'sms_sent', 'push_notification'],\n",
    "            outcome='purchase_made',\n",
    "            common_causes=[\n",
    "                'customer_segment',\n",
    "                'past_purchase_frequency',\n",
    "                'time_since_last_purchase'\n",
    "            ],\n",
    "            effect_modifiers=['message_content', 'send_time']\n",
    "        )\n",
    "\n",
    "        # 2. 估计各渠道效果\n",
    "        channel_effects = self.engine.estimate_causal_effect(marketing_model)\n",
    "\n",
    "        # 3. 反事实：如果改变营销组合\n",
    "        marketing_mix_scenarios = [\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"仅Email策略\",\n",
    "                treatment_change={\n",
    "                    'email_sent': 1.0,\n",
    "                    'sms_sent': 0.0,\n",
    "                    'push_notification': 0.0\n",
    "                },\n",
    "                context={'cost_per_channel': {'email': 0.1}}\n",
    "            ),\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"全渠道轰炸\",\n",
    "                treatment_change={\n",
    "                    'email_sent': 1.0,\n",
    "                    'sms_sent': 1.0,\n",
    "                    'push_notification': 1.0\n",
    "                },\n",
    "                context={'cost_per_channel': {'email': 0.1, 'sms': 0.5, 'push': 0.2}}\n",
    "            ),\n",
    "            CounterfactualScenario(\n",
    "                scenario_name=\"智能组合策略\",\n",
    "                treatment_change={\n",
    "                    'email_sent': 0.8,\n",
    "                    'sms_sent': 0.3,\n",
    "                    'push_notification': 0.6\n",
    "                },\n",
    "                context={'cost_per_channel': {'email': 0.1, 'sms': 0.5, 'push': 0.2}}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 4. 分析每个策略\n",
    "        mix_results = {}\n",
    "        for scenario in marketing_mix_scenarios:\n",
    "            result = self.engine.perform_counterfactual_analysis(\n",
    "                marketing_model,\n",
    "                scenario\n",
    "            )\n",
    "\n",
    "            # 计算成本效益\n",
    "            total_cost = sum(\n",
    "                scenario.treatment_change[f'{channel}_sent'] *\n",
    "                scenario.context['cost_per_channel'].get(channel, 0)\n",
    "                for channel in ['email', 'sms', 'push']\n",
    "            )\n",
    "\n",
    "            result['cost_effectiveness'] = {\n",
    "                'total_cost': total_cost,\n",
    "                'conversions_per_dollar': result['aggregate_impact']['total_change'] / total_cost if total_cost > 0 else 0\n",
    "            }\n",
    "\n",
    "            mix_results[scenario.scenario_name] = result\n",
    "\n",
    "        return {\n",
    "            'channel_attribution': channel_effects,\n",
    "            'marketing_mix_optimization': mix_results,\n",
    "            'recommended_mix': self._optimize_marketing_mix(mix_results),\n",
    "            'personalization_opportunities': self._identify_personalization_opportunities(\n",
    "                marketing_data,\n",
    "                channel_effects\n",
    "            )\n",
    "        }\n",
    "\n",
    "    # 辅助方法\n",
    "    def _generate_sales_insights(\n",
    "        self,\n",
    "        causal_effects: Dict[str, CausalEffect],\n",
    "        counterfactual_results: Dict[str, Any],\n",
    "        sensitivity: Dict[str, Any]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"生成销售洞察\"\"\"\n",
    "        insights = []\n",
    "\n",
    "        # 1. 促销效果洞察\n",
    "        promotion_effect = next((effect for name, effect in causal_effects.items()\n",
    "                               if 'promotion' in name.lower()), None)\n",
    "        if promotion_effect:\n",
    "            insights.append({\n",
    "                'type': 'promotion_effectiveness',\n",
    "                'finding': f\"促销活动平均提升销售额{promotion_effect.ate*100:.1f}%\",\n",
    "                'confidence': 'high' if sensitivity['overall_robustness']['score'] > 0.7 else 'medium',\n",
    "                'action': \"继续优化促销策略，特别关注高响应客群\"\n",
    "            })\n",
    "\n",
    "        # 2. 反事实洞察\n",
    "        best_scenario = max(\n",
    "            counterfactual_results.items(),\n",
    "            key=lambda x: x[1]['individual_effects']['mean']\n",
    "        )\n",
    "\n",
    "        insights.append({\n",
    "            'type': 'optimal_strategy',\n",
    "            'finding': f\"{best_scenario[0]}可能带来最大收益，\"\n",
    "                      f\"预期销售额增加{best_scenario[1]['aggregate_impact']['total_change']:.0f}元\",\n",
    "            'confidence': f\"{best_scenario[1]['confidence']*100:.0f}%\",\n",
    "            'action': f\"建议实施{best_scenario[0]}，并密切监控效果\"\n",
    "        })\n",
    "\n",
    "        # 3. 风险提示\n",
    "        if sensitivity['overall_robustness']['score'] < 0.6:\n",
    "            insights.append({\n",
    "                'type': 'risk_warning',\n",
    "                'finding': \"分析结果的稳健性较低，存在不确定性\",\n",
    "                'confidence': 'low',\n",
    "                'action': \"建议先进行小规模A/B测试验证\"\n",
    "            })\n",
    "\n",
    "        return insights\n",
    "\n",
    "    def _prioritize_actions(\n",
    "        self,\n",
    "        counterfactual_results: Dict[str, Any]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"优先级排序行动建议\"\"\"\n",
    "        actions = []\n",
    "\n",
    "        for scenario_name, result in counterfactual_results.items():\n",
    "            expected_impact = result['aggregate_impact']['total_change']\n",
    "            confidence = result['confidence']\n",
    "\n",
    "            # 计算优先级分数\n",
    "            priority_score = expected_impact * confidence\n",
    "\n",
    "            # 确定实施难度\n",
    "            if \"增强促销\" in scenario_name:\n",
    "                difficulty = \"medium\"\n",
    "                timeline = \"1-2 days\"\n",
    "            elif \"竞争环境\" in scenario_name:\n",
    "                difficulty = \"high\"\n",
    "                timeline = \"ongoing\"\n",
    "            else:\n",
    "                difficulty = \"low\"\n",
    "                timeline = \"immediate\"\n",
    "\n",
    "            actions.append({\n",
    "                'action': scenario_name,\n",
    "                'expected_impact': expected_impact,\n",
    "                'confidence': confidence,\n",
    "                'priority_score': priority_score,\n",
    "                'difficulty': difficulty,\n",
    "                'timeline': timeline,\n",
    "                'question_answered': result.get('business_question', '')\n",
    "            })\n",
    "\n",
    "        # 按优先级分数排序\n",
    "        actions.sort(key=lambda x: x['priority_score'], reverse=True)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def _find_optimal_retention_strategy(\n",
    "        self,\n",
    "        strategy_results: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"找出最优客户挽回策略\"\"\"\n",
    "        best_roi = -float('inf')\n",
    "        best_strategy = None\n",
    "\n",
    "        for strategy_name, result in strategy_results.items():\n",
    "            roi = result['financial_impact']['roi']\n",
    "            if roi > best_roi:\n",
    "                best_roi = roi\n",
    "                best_strategy = strategy_name\n",
    "\n",
    "        return {\n",
    "            'strategy': best_strategy,\n",
    "            'expected_roi': best_roi,\n",
    "            'implementation_details': strategy_results[best_strategy]\n",
    "        }\n",
    "\n",
    "    def _segment_retention_recommendations(\n",
    "        self,\n",
    "        customer_data: pd.DataFrame,\n",
    "        strategy_results: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"分客群的挽回建议\"\"\"\n",
    "        segments = {}\n",
    "\n",
    "        # VIP客户\n",
    "        vip_mask = customer_data['customer_lifetime_value'] > customer_data['customer_lifetime_value'].quantile(0.8)\n",
    "        segments['vip_customers'] = {\n",
    "            'size': sum(vip_mask),\n",
    "            'recommended_strategy': 'VIP升级策略',\n",
    "            'reason': 'VIP客户对专属权益更敏感'\n",
    "        }\n",
    "\n",
    "        # 价格敏感客户\n",
    "        price_sensitive_mask = customer_data['avg_discount_used'] > 0.2\n",
    "        segments['price_sensitive'] = {\n",
    "            'size': sum(price_sensitive_mask),\n",
    "            'recommended_strategy': '个性化优惠券策略',\n",
    "            'reason': '历史数据显示对折扣响应度高'\n",
    "        }\n",
    "\n",
    "        # 忠诚客户\n",
    "        loyal_mask = customer_data['total_purchases'] > 10\n",
    "        segments['loyal_customers'] = {\n",
    "            'size': sum(loyal_mask),\n",
    "            'recommended_strategy': '积分奖励策略',\n",
    "            'reason': '通过积分强化长期关系'\n",
    "        }\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def _find_optimal_price(\n",
    "        self,\n",
    "        pricing_data: pd.DataFrame,\n",
    "        price_effects: Dict[str, CausalEffect],\n",
    "        what_if_results: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"找出最优价格点\"\"\"\n",
    "        current_price = pricing_data['price'].mean()\n",
    "\n",
    "        # 基于弹性计算最优价格\n",
    "        # 简化假设：利润 = (价格 - 成本) * 销量\n",
    "        cost = current_price * 0.6  # 假设成本是价格的60%\n",
    "\n",
    "        # 使用第一个估计的价格弹性\n",
    "        elasticity = list(price_effects.values())[0].ate\n",
    "\n",
    "        # 最优价格公式（垄断定价）\n",
    "        optimal_price = cost / (1 + 1/elasticity) if elasticity < -1 else current_price * 1.1\n",
    "\n",
    "        return {\n",
    "            'current_price': current_price,\n",
    "            'optimal_price': optimal_price,\n",
    "            'expected_profit_increase': (optimal_price - current_price) * 1000,  # 简化计算\n",
    "            'elasticity': elasticity,\n",
    "            'confidence_interval': list(price_effects.values())[0].confidence_interval\n",
    "        }\n",
    "\n",
    "    def _create_pricing_roadmap(self, optimal_price: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"创建定价实施路线图\"\"\"\n",
    "        current = optimal_price['current_price']\n",
    "        target = optimal_price['optimal_price']\n",
    "\n",
    "        # 分阶段调价\n",
    "        steps = []\n",
    "\n",
    "        if abs(target - current) / current > 0.1:  # 变化超过10%\n",
    "            # 分3步调整\n",
    "            step1 = current + (target - current) * 0.3\n",
    "            step2 = current + (target - current) * 0.6\n",
    "\n",
    "            steps = [\n",
    "                {\n",
    "                    'phase': 1,\n",
    "                    'price': step1,\n",
    "                    'timeline': 'Week 1-2',\n",
    "                    'action': '初步调整，监控市场反应'\n",
    "                },\n",
    "                {\n",
    "                    'phase': 2,\n",
    "                    'price': step2,\n",
    "                    'timeline': 'Week 3-4',\n",
    "                    'action': '根据反馈继续调整'\n",
    "                },\n",
    "                {\n",
    "                    'phase': 3,\n",
    "                    'price': target,\n",
    "                    'timeline': 'Week 5+',\n",
    "                    'action': '达到目标价格，持续优化'\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            steps = [\n",
    "                {\n",
    "                    'phase': 1,\n",
    "                    'price': target,\n",
    "                    'timeline': 'Immediate',\n",
    "                    'action': '一次性调整到位'\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        return steps\n",
    "\n",
    "    def _analyze_seasonal_inventory(\n",
    "        self,\n",
    "        inventory_data: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"分析季节性库存需求\"\"\"\n",
    "        # 简化的季节性分析\n",
    "        seasonal_factors = {\n",
    "            'spring': 1.0,\n",
    "            'summer': 1.3,\n",
    "            'fall': 0.9,\n",
    "            'winter': 0.8\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'seasonal_multipliers': seasonal_factors,\n",
    "            'recommendation': '夏季增加30%安全库存，冬季可降低20%'\n",
    "        }\n",
    "\n",
    "    def _analyze_supplier_impact(\n",
    "        self,\n",
    "        inventory_data: pd.DataFrame\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"分析供应商影响\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'supplier': 'Primary Supplier A',\n",
    "                'lead_time_impact': '缩短1天可减少15%的缺货',\n",
    "                'action': '协商建立VMI（供应商管理库存）'\n",
    "            },\n",
    "            {\n",
    "                'supplier': 'Backup Supplier B',\n",
    "                'lead_time_impact': '作为应急可接受+2天lead time',\n",
    "                'action': '保持战略合作关系'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def _optimize_marketing_mix(\n",
    "        self,\n",
    "        mix_results: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"优化营销组合\"\"\"\n",
    "        best_mix = max(\n",
    "            mix_results.items(),\n",
    "            key=lambda x: x[1]['cost_effectiveness']['conversions_per_dollar']\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'recommended_mix': best_mix[0],\n",
    "            'conversions_per_dollar': best_mix[1]['cost_effectiveness']['conversions_per_dollar'],\n",
    "            'implementation': '建议逐步过渡到智能组合策略，避免客户疲劳'\n",
    "        }\n",
    "\n",
    "    def _identify_personalization_opportunities(\n",
    "        self,\n",
    "        marketing_data: pd.DataFrame,\n",
    "        channel_effects: Dict[str, CausalEffect]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"识别个性化机会\"\"\"\n",
    "        opportunities = []\n",
    "\n",
    "        # 基于客群的渠道偏好\n",
    "        opportunities.append({\n",
    "            'segment': '年轻客户',\n",
    "            'insight': 'App推送效果最好',\n",
    "            'action': '增加App推送频率，减少SMS'\n",
    "        })\n",
    "\n",
    "        opportunities.append({\n",
    "            'segment': '高价值客户',\n",
    "            'insight': 'Email个性化内容转化率高',\n",
    "            'action': '投资邮件内容个性化引擎'\n",
    "        })\n",
    "\n",
    "        opportunities.append({\n",
    "            'segment': '价格敏感客户',\n",
    "            'insight': 'SMS优惠券响应度最高',\n",
    "            'action': 'SMS重点推送限时优惠'\n",
    "        })\n",
    "\n",
    "        return opportunities"
   ],
   "id": "3424454cf38d7a91",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:52:17.477048Z",
     "start_time": "2025-07-30T09:52:17.464595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建分析器\n",
    "analyzer = CausalBusinessAnalyzer()\n",
    "\n",
    "# 生成模拟数据\n",
    "n_samples = 1000\n",
    "\n",
    "# 销售数据\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': pd.date_range(end=datetime.now(), periods=n_samples, freq='D'),\n",
    "    'daily_revenue': np.random.normal(50000, 10000, n_samples),\n",
    "    'promotion_active': np.random.binomial(1, 0.3, n_samples),\n",
    "    'price_level': np.random.normal(100, 10, n_samples),\n",
    "    'day_of_week': np.random.randint(0, 7, n_samples),\n",
    "    'weather_condition': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "    'competitor_activity': np.random.binomial(1, 0.4, n_samples),\n",
    "    'inventory_availability': np.random.uniform(0.5, 1.0, n_samples),\n",
    "    'store_traffic': np.random.normal(1000, 200, n_samples),\n",
    "    'customer_segment': np.random.choice(['regular', 'vip', 'new'], n_samples),\n",
    "    'product_type': np.random.choice(['beverage', 'food', 'snack'], n_samples)\n",
    "})"
   ],
   "id": "23b4a6bd0acf40a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:52:18.264611Z",
     "start_time": "2025-07-30T09:52:18.245310Z"
    }
   },
   "cell_type": "code",
   "source": "sales_data",
   "id": "ab9f3e708c884c16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          date  daily_revenue  promotion_active  price_level  \\\n",
       "0   2022-11-04 17:52:17.469250   57088.332816                 0   102.983178   \n",
       "1   2022-11-05 17:52:17.469250   33903.571192                 0   104.687590   \n",
       "2   2022-11-06 17:52:17.469250   30601.892593                 0    80.966129   \n",
       "3   2022-11-07 17:52:17.469250   40161.213708                 0    80.776156   \n",
       "4   2022-11-08 17:52:17.469250   59491.284872                 0    92.469657   \n",
       "..                         ...            ...               ...          ...   \n",
       "995 2025-07-26 17:52:17.469250   47213.681058                 0   113.919562   \n",
       "996 2025-07-27 17:52:17.469250   62052.391749                 0   119.800106   \n",
       "997 2025-07-28 17:52:17.469250   56046.632820                 0   111.877910   \n",
       "998 2025-07-29 17:52:17.469250   53614.892391                 1   102.457912   \n",
       "999 2025-07-30 17:52:17.469250   50503.031382                 0    90.715690   \n",
       "\n",
       "     day_of_week  weather_condition  competitor_activity  \\\n",
       "0              0                  1                    1   \n",
       "1              5                  1                    1   \n",
       "2              4                  0                    0   \n",
       "3              2                  1                    1   \n",
       "4              6                  1                    1   \n",
       "..           ...                ...                  ...   \n",
       "995            1                  0                    0   \n",
       "996            3                  0                    0   \n",
       "997            3                  1                    0   \n",
       "998            0                  1                    0   \n",
       "999            5                  1                    1   \n",
       "\n",
       "     inventory_availability  store_traffic customer_segment product_type  \n",
       "0                  0.751068     652.492675              new        snack  \n",
       "1                  0.742690    1178.621850          regular        snack  \n",
       "2                  0.942364    1134.277063              new        snack  \n",
       "3                  0.696543    1023.874412              vip     beverage  \n",
       "4                  0.858454    1167.407262          regular         food  \n",
       "..                      ...            ...              ...          ...  \n",
       "995                0.950866     431.436631              new     beverage  \n",
       "996                0.857091    1023.692147              vip        snack  \n",
       "997                0.529126     883.615352              vip     beverage  \n",
       "998                0.536240     635.761100              vip     beverage  \n",
       "999                0.831022    1033.782062              vip        snack  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_revenue</th>\n",
       "      <th>promotion_active</th>\n",
       "      <th>price_level</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>competitor_activity</th>\n",
       "      <th>inventory_availability</th>\n",
       "      <th>store_traffic</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-04 17:52:17.469250</td>\n",
       "      <td>57088.332816</td>\n",
       "      <td>0</td>\n",
       "      <td>102.983178</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751068</td>\n",
       "      <td>652.492675</td>\n",
       "      <td>new</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-05 17:52:17.469250</td>\n",
       "      <td>33903.571192</td>\n",
       "      <td>0</td>\n",
       "      <td>104.687590</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.742690</td>\n",
       "      <td>1178.621850</td>\n",
       "      <td>regular</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-06 17:52:17.469250</td>\n",
       "      <td>30601.892593</td>\n",
       "      <td>0</td>\n",
       "      <td>80.966129</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942364</td>\n",
       "      <td>1134.277063</td>\n",
       "      <td>new</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-07 17:52:17.469250</td>\n",
       "      <td>40161.213708</td>\n",
       "      <td>0</td>\n",
       "      <td>80.776156</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.696543</td>\n",
       "      <td>1023.874412</td>\n",
       "      <td>vip</td>\n",
       "      <td>beverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-08 17:52:17.469250</td>\n",
       "      <td>59491.284872</td>\n",
       "      <td>0</td>\n",
       "      <td>92.469657</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858454</td>\n",
       "      <td>1167.407262</td>\n",
       "      <td>regular</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2025-07-26 17:52:17.469250</td>\n",
       "      <td>47213.681058</td>\n",
       "      <td>0</td>\n",
       "      <td>113.919562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950866</td>\n",
       "      <td>431.436631</td>\n",
       "      <td>new</td>\n",
       "      <td>beverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2025-07-27 17:52:17.469250</td>\n",
       "      <td>62052.391749</td>\n",
       "      <td>0</td>\n",
       "      <td>119.800106</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857091</td>\n",
       "      <td>1023.692147</td>\n",
       "      <td>vip</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2025-07-28 17:52:17.469250</td>\n",
       "      <td>56046.632820</td>\n",
       "      <td>0</td>\n",
       "      <td>111.877910</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>883.615352</td>\n",
       "      <td>vip</td>\n",
       "      <td>beverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2025-07-29 17:52:17.469250</td>\n",
       "      <td>53614.892391</td>\n",
       "      <td>1</td>\n",
       "      <td>102.457912</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.536240</td>\n",
       "      <td>635.761100</td>\n",
       "      <td>vip</td>\n",
       "      <td>beverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2025-07-30 17:52:17.469250</td>\n",
       "      <td>50503.031382</td>\n",
       "      <td>0</td>\n",
       "      <td>90.715690</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831022</td>\n",
       "      <td>1033.782062</td>\n",
       "      <td>vip</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:52:26.909280Z",
     "start_time": "2025-07-30T09:52:25.359531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 销售下滑分析\n",
    "print(\"\\n=== 销售下滑因果分析 ===\")\n",
    "# analyzer.engine.create_causal_model(sales_data)\n",
    "sales_results = analyzer.analyze_sales_decline(sales_data)"
   ],
   "id": "aa3e55676d3e2f28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:开始分析销售下滑问题...\n",
      "INFO:dowhy.causal_model:Model to find the causal effect of treatment ['promotion_active', 'price_level'] on outcome ['daily_revenue']\n",
      "WARNING:dowhy.causal_model:There are an additional 1 variables in the dataset that are not in the graph. Variable names are: '['date']'\n",
      "WARNING:dowhy.causal_identifier.auto_identifier:Adjustment set identification failed.\n",
      "INFO:dowhy.causal_identifier.auto_identifier:Instrumental variables for treatment and outcome:[]\n",
      "INFO:dowhy.causal_identifier.auto_identifier:Frontdoor variables for treatment and outcome:[]\n",
      "INFO:dowhy.causal_identifier.auto_identifier:No adjustment sets found.\n",
      "INFO:dowhy.causal_identifier.auto_identifier:Number of general adjustment sets found: 0\n",
      "WARNING:dowhy.causal_identifier.auto_identifier:Adjustment set identification failed.\n",
      "INFO:dowhy.causal_model:propensity_score_matching\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Matching Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:propensity_score_matching\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Matching Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:propensity_score_matching\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Matching Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "WARNING:src.analytics.advanced_causal_engine:Method backdoor.propensity_score_matching failed: unsupported operand type(s) for *: 'NoneType' and 'float'\n",
      "INFO:dowhy.causal_model:linear_regression\n",
      "INFO:dowhy.causal_estimator:INFO: Using Linear Regression Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:linear_regression\n",
      "INFO:dowhy.causal_estimator:INFO: Using Linear Regression Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:linear_regression\n",
      "INFO:dowhy.causal_estimator:INFO: Using Linear Regression Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "WARNING:src.analytics.advanced_causal_engine:Method backdoor.linear_regression failed: unsupported operand type(s) for *: 'NoneType' and 'float'\n",
      "INFO:dowhy.causal_model:propensity_score_weighting\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Weighting Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:propensity_score_weighting\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Weighting Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "INFO:dowhy.causal_model:propensity_score_weighting\n",
      "INFO:dowhy.causal_estimator:INFO: Using Propensity Score Weighting Estimator\n",
      "ERROR:dowhy.causal_estimator:No valid identified estimand available.\n",
      "WARNING:src.analytics.advanced_causal_engine:Method backdoor.propensity_score_weighting failed: unsupported operand type(s) for *: 'NoneType' and 'float'\n",
      "INFO:src.analytics.advanced_causal_engine:执行反事实分析: 增强促销策略\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 销售下滑因果分析 ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not convert ['regularregularregularvipnewnewvipnewnewvipvipnewregularnewregularnewnewvipvipregularnewregularvipvipregularnewvipvipvipvip'\n 'snackfoodfoodfoodbeveragesnackfoodfoodfoodfoodfoodsnackfoodbeveragebeveragefoodsnackfoodfoodsnacksnackbeveragefoodsnackbeveragebeveragesnackbeveragebeveragesnack'] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== 销售下滑因果分析 ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# analyzer.engine.create_causal_model(sales_data)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m sales_results = analyzer.analyze_sales_decline(sales_data)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 116\u001B[39m, in \u001B[36mCausalBusinessAnalyzer.analyze_sales_decline\u001B[39m\u001B[34m(self, business_data)\u001B[39m\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m cf_item \u001B[38;5;129;01min\u001B[39;00m counterfactual_scenarios:\n\u001B[32m    115\u001B[39m     scenario = cf_item[\u001B[33m'\u001B[39m\u001B[33mscenario\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     result = \u001B[38;5;28mself\u001B[39m.engine.perform_counterfactual_analysis(\n\u001B[32m    117\u001B[39m         sales_model,\n\u001B[32m    118\u001B[39m         scenario,\n\u001B[32m    119\u001B[39m         sample_data=business_data.tail(\u001B[32m30\u001B[39m)  \u001B[38;5;66;03m# 使用最近30天数据\u001B[39;00m\n\u001B[32m    120\u001B[39m     )\n\u001B[32m    121\u001B[39m     result[\u001B[33m'\u001B[39m\u001B[33mbusiness_question\u001B[39m\u001B[33m'\u001B[39m] = cf_item[\u001B[33m'\u001B[39m\u001B[33mbusiness_question\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m    122\u001B[39m     counterfactual_results[scenario.scenario_name] = result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FBR_AI_Agine/src/analytics/advanced_causal_engine.py:247\u001B[39m, in \u001B[36mAdvancedCausalEngine.perform_counterfactual_analysis\u001B[39m\u001B[34m(self, model, scenario, sample_data)\u001B[39m\n\u001B[32m    221\u001B[39m counterfactual_outcome = outcome_model.predict(X_cf)\n\u001B[32m    222\u001B[39m ite = counterfactual_outcome - factual_outcome\n\u001B[32m    223\u001B[39m results = {\n\u001B[32m    224\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mscenario\u001B[39m\u001B[33m\"\u001B[39m: scenario.scenario_name,\n\u001B[32m    225\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtreatment_changes\u001B[39m\u001B[33m\"\u001B[39m: scenario.treatment_change,\n\u001B[32m    226\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindividual_effects\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    227\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m\"\u001B[39m: np.mean(ite),\n\u001B[32m    228\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmedian\u001B[39m\u001B[33m\"\u001B[39m: np.median(ite),\n\u001B[32m    229\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mstd\u001B[39m\u001B[33m\"\u001B[39m: np.std(ite),\n\u001B[32m    230\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmin\u001B[39m\u001B[33m\"\u001B[39m: np.min(ite),\n\u001B[32m    231\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmax\u001B[39m\u001B[33m\"\u001B[39m: np.max(ite),\n\u001B[32m    232\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpercentiles\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    233\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m25\u001B[39m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m: np.percentile(ite, \u001B[32m25\u001B[39m),\n\u001B[32m    234\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m75\u001B[39m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m: np.percentile(ite, \u001B[32m75\u001B[39m),\n\u001B[32m    235\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m95\u001B[39m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m: np.percentile(ite, \u001B[32m95\u001B[39m)\n\u001B[32m    236\u001B[39m         }\n\u001B[32m    237\u001B[39m     },\n\u001B[32m    238\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33maggregate_impact\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    239\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtotal_change\u001B[39m\u001B[33m\"\u001B[39m: np.sum(ite),\n\u001B[32m    240\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33maverage_change\u001B[39m\u001B[33m\"\u001B[39m: np.mean(ite),\n\u001B[32m    241\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpositive_impact_pct\u001B[39m\u001B[33m\"\u001B[39m: np.mean(ite > \u001B[32m0\u001B[39m) * \u001B[32m100\u001B[39m,\n\u001B[32m    242\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33msignificant_impact_pct\u001B[39m\u001B[33m\"\u001B[39m: np.mean(np.abs(ite) > np.std(factual_outcome)) * \u001B[32m100\u001B[39m\n\u001B[32m    243\u001B[39m     },\n\u001B[32m    244\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msubgroup_analysis\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._analyze_subgroups(\n\u001B[32m    245\u001B[39m         sample_data, ite, scenario.context\n\u001B[32m    246\u001B[39m     ),\n\u001B[32m--> \u001B[39m\u001B[32m247\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mconfidence\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._calculate_counterfactual_confidence(\n\u001B[32m    248\u001B[39m         outcome_model, sample_data, counterfactual_data\n\u001B[32m    249\u001B[39m     )\n\u001B[32m    250\u001B[39m }\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FBR_AI_Agine/src/analytics/advanced_causal_engine.py:447\u001B[39m, in \u001B[36mAdvancedCausalEngine._calculate_counterfactual_confidence\u001B[39m\u001B[34m(self, model, factual_data, counterfactual_data)\u001B[39m\n\u001B[32m    441\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_calculate_counterfactual_confidence\u001B[39m(\n\u001B[32m    442\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    443\u001B[39m         model: Any,\n\u001B[32m    444\u001B[39m         factual_data: pd.DataFrame,\n\u001B[32m    445\u001B[39m         counterfactual_data: pd.DataFrame\n\u001B[32m    446\u001B[39m ) -> \u001B[38;5;28mfloat\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m     factual_mean = factual_data.mean()\n\u001B[32m    448\u001B[39m     cf_mean = counterfactual_data.mean()\n\u001B[32m    449\u001B[39m     relative_diff = np.mean(np.abs(cf_mean - factual_mean) / (factual_mean + \u001B[32m1e-10\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/frame.py:11700\u001B[39m, in \u001B[36mDataFrame.mean\u001B[39m\u001B[34m(self, axis, skipna, numeric_only, **kwargs)\u001B[39m\n\u001B[32m  11692\u001B[39m \u001B[38;5;129m@doc\u001B[39m(make_doc(\u001B[33m\"\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m\"\u001B[39m, ndim=\u001B[32m2\u001B[39m))\n\u001B[32m  11693\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmean\u001B[39m(\n\u001B[32m  11694\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m  11698\u001B[39m     **kwargs,\n\u001B[32m  11699\u001B[39m ):\n\u001B[32m> \u001B[39m\u001B[32m11700\u001B[39m     result = \u001B[38;5;28msuper\u001B[39m().mean(axis, skipna, numeric_only, **kwargs)\n\u001B[32m  11701\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, Series):\n\u001B[32m  11702\u001B[39m         result = result.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py:12439\u001B[39m, in \u001B[36mNDFrame.mean\u001B[39m\u001B[34m(self, axis, skipna, numeric_only, **kwargs)\u001B[39m\n\u001B[32m  12432\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmean\u001B[39m(\n\u001B[32m  12433\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m  12434\u001B[39m     axis: Axis | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[32m0\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m  12437\u001B[39m     **kwargs,\n\u001B[32m  12438\u001B[39m ) -> Series | \u001B[38;5;28mfloat\u001B[39m:\n\u001B[32m> \u001B[39m\u001B[32m12439\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stat_function(\n\u001B[32m  12440\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m\"\u001B[39m, nanops.nanmean, axis, skipna, numeric_only, **kwargs\n\u001B[32m  12441\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/generic.py:12396\u001B[39m, in \u001B[36mNDFrame._stat_function\u001B[39m\u001B[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001B[39m\n\u001B[32m  12392\u001B[39m nv.validate_func(name, (), kwargs)\n\u001B[32m  12394\u001B[39m validate_bool_kwarg(skipna, \u001B[33m\"\u001B[39m\u001B[33mskipna\u001B[39m\u001B[33m\"\u001B[39m, none_allowed=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m> \u001B[39m\u001B[32m12396\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._reduce(\n\u001B[32m  12397\u001B[39m     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n\u001B[32m  12398\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/frame.py:11569\u001B[39m, in \u001B[36mDataFrame._reduce\u001B[39m\u001B[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[39m\n\u001B[32m  11565\u001B[39m     df = df.T\n\u001B[32m  11567\u001B[39m \u001B[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001B[39;00m\n\u001B[32m  11568\u001B[39m \u001B[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001B[39;00m\n\u001B[32m> \u001B[39m\u001B[32m11569\u001B[39m res = df._mgr.reduce(blk_func)\n\u001B[32m  11570\u001B[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001B[32m0\u001B[39m]\n\u001B[32m  11571\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m out_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m out.dtype != \u001B[33m\"\u001B[39m\u001B[33mboolean\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001B[39m, in \u001B[36mBlockManager.reduce\u001B[39m\u001B[34m(self, func)\u001B[39m\n\u001B[32m   1498\u001B[39m res_blocks: \u001B[38;5;28mlist\u001B[39m[Block] = []\n\u001B[32m   1499\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n\u001B[32m-> \u001B[39m\u001B[32m1500\u001B[39m     nbs = blk.reduce(func)\n\u001B[32m   1501\u001B[39m     res_blocks.extend(nbs)\n\u001B[32m   1503\u001B[39m index = Index([\u001B[38;5;28;01mNone\u001B[39;00m])  \u001B[38;5;66;03m# placeholder\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/internals/blocks.py:406\u001B[39m, in \u001B[36mBlock.reduce\u001B[39m\u001B[34m(self, func)\u001B[39m\n\u001B[32m    400\u001B[39m \u001B[38;5;129m@final\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func) -> \u001B[38;5;28mlist\u001B[39m[Block]:\n\u001B[32m    402\u001B[39m     \u001B[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001B[39;00m\n\u001B[32m    403\u001B[39m     \u001B[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001B[39;00m\n\u001B[32m    404\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ndim == \u001B[32m2\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m406\u001B[39m     result = func(\u001B[38;5;28mself\u001B[39m.values)\n\u001B[32m    408\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.values.ndim == \u001B[32m1\u001B[39m:\n\u001B[32m    409\u001B[39m         res_values = result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/frame.py:11488\u001B[39m, in \u001B[36mDataFrame._reduce.<locals>.blk_func\u001B[39m\u001B[34m(values, axis)\u001B[39m\n\u001B[32m  11486\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m np.array([result])\n\u001B[32m  11487\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m> \u001B[39m\u001B[32m11488\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m op(values, axis=axis, skipna=skipna, **kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001B[39m, in \u001B[36mbottleneck_switch.__call__.<locals>.f\u001B[39m\u001B[34m(values, axis, skipna, **kwds)\u001B[39m\n\u001B[32m    145\u001B[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001B[39m, in \u001B[36m_datetimelike_compat.<locals>.new_func\u001B[39m\u001B[34m(values, axis, skipna, mask, **kwargs)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m datetimelike \u001B[38;5;129;01mand\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    402\u001B[39m     mask = isna(values)\n\u001B[32m--> \u001B[39m\u001B[32m404\u001B[39m result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m datetimelike:\n\u001B[32m    407\u001B[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001B[39m, in \u001B[36mnanmean\u001B[39m\u001B[34m(values, axis, skipna, mask)\u001B[39m\n\u001B[32m    718\u001B[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001B[32m    719\u001B[39m the_sum = values.sum(axis, dtype=dtype_sum)\n\u001B[32m--> \u001B[39m\u001B[32m720\u001B[39m the_sum = _ensure_numeric(the_sum)\n\u001B[32m    722\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(the_sum, \u001B[33m\"\u001B[39m\u001B[33mndim\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m    723\u001B[39m     count = cast(np.ndarray, count)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/FBR_AI_Engine/lib/python3.12/site-packages/pandas/core/nanops.py:1686\u001B[39m, in \u001B[36m_ensure_numeric\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m   1683\u001B[39m inferred = lib.infer_dtype(x)\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m inferred \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mstring\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmixed\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1686\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not convert \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m to numeric\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1687\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1688\u001B[39m     x = x.astype(np.complex128)\n",
      "\u001B[31mTypeError\u001B[39m: Could not convert ['regularregularregularvipnewnewvipnewnewvipvipnewregularnewregularnewnewvipvipregularnewregularvipvipregularnewvipvipvipvip'\n 'snackfoodfoodfoodbeveragesnackfoodfoodfoodfoodfoodsnackfoodbeveragebeveragefoodsnackfoodfoodsnacksnackbeveragefoodsnackbeveragebeveragesnackbeveragebeveragesnack'] to numeric"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f2c2ffe62cfcadf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "causal_graph = \"\"\"\n",
    "        digraph {\n",
    "            // 外部因素\n",
    "            Weather -> Store_Traffic;\n",
    "            Competitor_Promotion -> Customer_Choice;\n",
    "            Holiday -> Store_Traffic;\n",
    "            Holiday -> Online_Traffic;\n",
    "\n",
    "            // 流量因素\n",
    "            Store_Traffic -> Total_Visitors;\n",
    "            Online_Traffic -> Total_Visitors;\n",
    "            Marketing_Spend -> Online_Traffic;\n",
    "            Marketing_Spend -> Brand_Awareness;\n",
    "            Brand_Awareness -> Store_Traffic;\n",
    "\n",
    "            // 商品和库存\n",
    "            Inventory_Level -> Product_Availability;\n",
    "            Product_Availability -> Conversion_Rate;\n",
    "            Product_Quality -> Customer_Satisfaction;\n",
    "            Product_Quality -> Return_Rate;\n",
    "\n",
    "            // 价格和促销\n",
    "            Price -> Conversion_Rate;\n",
    "            Promotion -> Price;\n",
    "            Promotion -> Conversion_Rate;\n",
    "            Promotion -> Average_Order_Value;\n",
    "            Customer_Choice -> Conversion_Rate;\n",
    "\n",
    "            // 转化和销售\n",
    "            Total_Visitors -> Orders;\n",
    "            Conversion_Rate -> Orders;\n",
    "            Orders -> Revenue;\n",
    "            Average_Order_Value -> Revenue;\n",
    "\n",
    "            // 客户因素\n",
    "            Customer_Satisfaction -> Customer_Retention;\n",
    "            Customer_Retention -> Repeat_Orders;\n",
    "            Repeat_Orders -> Revenue;\n",
    "            Return_Rate -> Customer_Satisfaction;\n",
    "\n",
    "            // 运营因素\n",
    "            Staff_Performance -> Service_Quality;\n",
    "            Service_Quality -> Customer_Satisfaction;\n",
    "            Service_Quality -> Conversion_Rate;\n",
    "\n",
    "            // 隐藏的混淆因素\n",
    "            U1 -> Price;\n",
    "            U1 -> Revenue;  // 未观测的市场因素\n",
    "            U2 -> Marketing_Spend;\n",
    "            U2 -> Revenue;  // 未观测的预算约束\n",
    "        }\n",
    "        \"\"\""
   ],
   "id": "880f430ac0dc31ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a91ac0d99f8bcdd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(\"\\n因果效应估计:\")\n",
    "for method, effect in sales_results['causal_effects'].items():\n",
    "    print(f\"{method}: ATE={effect.ate:.3f}, CI={effect.confidence_interval}\")\n",
    "\n",
    "print(\"\\n反事实分析结果:\")\n",
    "for scenario, result in sales_results['counterfactual_analysis'].items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  问题: {result['business_question']}\")\n",
    "    print(f\"  预期效果: {result['individual_effects']['mean']:.2f}\")\n",
    "    print(f\"  置信度: {result['confidence']*100:.0f}%\")\n",
    "\n",
    "print(\"\\n建议行动优先级:\")\n",
    "for i, action in enumerate(sales_results['recommended_actions'][:3]):\n",
    "    print(f\"{i+1}. {action['action']}\")\n",
    "    print(f\"   预期影响: {action['expected_impact']:.0f}\")\n",
    "    print(f\"   实施时间: {action['timeline']}\")\n",
    "\n",
    "# 2. 客户流失分析\n",
    "print(\"\\n\\n=== 客户流失因果分析 ===\")\n",
    "\n",
    "# 生成客户数据\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1000),\n",
    "    'churned': np.random.binomial(1, 0.2, 1000),\n",
    "    'received_retention_offer': np.random.binomial(1, 0.3, 1000),\n",
    "    'customer_lifetime_value': np.random.gamma(100, 2, 1000),\n",
    "    'recent_support_tickets': np.random.poisson(0.5, 1000),\n",
    "    'days_since_last_purchase': np.random.exponential(30, 1000),\n",
    "    'total_purchases': np.random.poisson(10, 1000),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, 1000),\n",
    "    'random_campaign_assignment': np.random.binomial(1, 0.5, 1000),\n",
    "    'churn_probability': np.random.uniform(0, 1, 1000),\n",
    "    'avg_discount_used': np.random.uniform(0, 0.4, 1000)\n",
    "})\n",
    "\n",
    "churn_results = analyzer.analyze_customer_churn(customer_data)\n",
    "\n",
    "print(\"\\n最优挽回策略:\")\n",
    "optimal = churn_results['optimal_strategy']\n",
    "print(f\"策略: {optimal['strategy']}\")\n",
    "print(f\"预期ROI: {optimal['expected_roi']*100:.0f}%\")\n",
    "\n",
    "print(\"\\n分客群建议:\")\n",
    "for segment, rec in churn_results['segmented_recommendations'].items():\n",
    "    print(f\"{segment}: {rec['recommended_strategy']} ({rec['reason']})\")\n",
    "\n",
    "# 3. 促销场景分析\n",
    "print(\"\\n\\n=== 促销策略What-if分析 ===\")\n",
    "promo_results = analyzer.engine.analyze_promotion_scenarios(sales_data)\n",
    "\n",
    "print(\"\\n场景对比:\")\n",
    "comparison = analyzer.engine._compare_scenarios(promo_results['scenario_analysis'])\n",
    "print(comparison)\n",
    "\n",
    "print(f\"\\n推荐方案: {promo_results['recommendation']['recommended_scenario']}\")\n",
    "print(f\"预期收入增加: ¥{promo_results['recommendation']['expected_revenue_increase']:.0f}\")"
   ],
   "id": "76de947d1ea7df19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:05:39.469094Z",
     "start_time": "2025-07-30T10:05:37.256183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "\t\"latitude\": 52.52,\n",
    "\t\"longitude\": 13.41,\n",
    "\t\"hourly\": \"temperature_2m\",\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates: {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation: {response.Elevation()} m asl\")\n",
    "print(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(\"\\nHourly data\\n\", hourly_dataframe)"
   ],
   "id": "9ad123b2b22dbc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: 52.52000045776367°N 13.419998168945312°E\n",
      "Elevation: 38.0 m asl\n",
      "Timezone difference to GMT+0: 0s\n",
      "\n",
      "Hourly data\n",
      "                          date  temperature_2m\n",
      "0   2025-07-30 00:00:00+00:00       15.095500\n",
      "1   2025-07-30 01:00:00+00:00       15.045500\n",
      "2   2025-07-30 02:00:00+00:00       15.195499\n",
      "3   2025-07-30 03:00:00+00:00       14.595500\n",
      "4   2025-07-30 04:00:00+00:00       14.445499\n",
      "..                        ...             ...\n",
      "163 2025-08-05 19:00:00+00:00       16.969501\n",
      "164 2025-08-05 20:00:00+00:00       16.219501\n",
      "165 2025-08-05 21:00:00+00:00       15.569500\n",
      "166 2025-08-05 22:00:00+00:00       15.169499\n",
      "167 2025-08-05 23:00:00+00:00       14.919499\n",
      "\n",
      "[168 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:05:44.547774Z",
     "start_time": "2025-07-30T10:05:44.542116Z"
    }
   },
   "cell_type": "code",
   "source": "hourly_dataframe",
   "id": "26b432f65f182416",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                         date  temperature_2m\n",
       "0   2025-07-30 00:00:00+00:00       15.095500\n",
       "1   2025-07-30 01:00:00+00:00       15.045500\n",
       "2   2025-07-30 02:00:00+00:00       15.195499\n",
       "3   2025-07-30 03:00:00+00:00       14.595500\n",
       "4   2025-07-30 04:00:00+00:00       14.445499\n",
       "..                        ...             ...\n",
       "163 2025-08-05 19:00:00+00:00       16.969501\n",
       "164 2025-08-05 20:00:00+00:00       16.219501\n",
       "165 2025-08-05 21:00:00+00:00       15.569500\n",
       "166 2025-08-05 22:00:00+00:00       15.169499\n",
       "167 2025-08-05 23:00:00+00:00       14.919499\n",
       "\n",
       "[168 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-30 00:00:00+00:00</td>\n",
       "      <td>15.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-30 01:00:00+00:00</td>\n",
       "      <td>15.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-30 02:00:00+00:00</td>\n",
       "      <td>15.195499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-30 03:00:00+00:00</td>\n",
       "      <td>14.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-30 04:00:00+00:00</td>\n",
       "      <td>14.445499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2025-08-05 19:00:00+00:00</td>\n",
       "      <td>16.969501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2025-08-05 20:00:00+00:00</td>\n",
       "      <td>16.219501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2025-08-05 21:00:00+00:00</td>\n",
       "      <td>15.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2025-08-05 22:00:00+00:00</td>\n",
       "      <td>15.169499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2025-08-05 23:00:00+00:00</td>\n",
       "      <td>14.919499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:06:22.359899Z",
     "start_time": "2025-07-30T10:06:22.354313Z"
    }
   },
   "cell_type": "code",
   "source": "hourly",
   "id": "89a76bf39d6d083",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openmeteo_sdk.VariablesWithTime.VariablesWithTime at 0x103804820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c3a123e3e2111b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
