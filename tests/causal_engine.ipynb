{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dbf1f-8657-448e-82d5-10ac8929e8d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "åŸºäº UMe èŒ¶é¥®é”€å”®æ•°æ®ï¼Œè¯„ä¼°å¤©æ°”ã€èŠ‚å‡æ—¥ã€ä¿ƒé”€ç­‰å› ç´ çš„å› æœå½±å“\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import clickhouse_connect\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# å› æœæ¨æ–­\n",
    "from dowhy import CausalModel\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# EconMLï¼ˆå¼‚è´¨æ•ˆåº”åˆ†æï¼‰\n",
    "from econml.metalearners import TLearner\n",
    "from econml.dml import CausalForestDML, LinearDML\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"  # æˆ–è€… \"notebook\" å¦‚æœä½ å¸Œæœ›ç›´æ¥åœ¨ Jupyter ä¸­æŸ¥çœ‹\n",
    "\n",
    "\n",
    "class FBRCausalInference:\n",
    "    \"\"\"FBR å› æœæ¨æ–­åˆ†æå¼•æ“\"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 0. åˆå§‹åŒ–\n",
    "    # ------------------------------------------------------------\n",
    "    def __init__(self, ch_config: dict):\n",
    "        self.ch_client = clickhouse_connect.get_client(**ch_config)\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. æ•°æ®æŠ½å–\n",
    "    # ------------------------------------------------------------\n",
    "    def load_integrated_data(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"æ‹‰å–æ—¥ Ã— åº—é“ºé”€å”®èšåˆ + ç±»åˆ«ç‰¹å¾\"\"\"\n",
    "        sales_query = f\"\"\"\n",
    "        WITH\n",
    "            toDate(created_at_pt)          AS dt,\n",
    "            toDayOfWeek(created_at_pt)     AS day_of_week,\n",
    "            toHour(created_at_pt)          AS hour_of_day,\n",
    "            substring(location_name, position(location_name,'-')+1, 2) AS state\n",
    "        SELECT\n",
    "            dt  AS date,\n",
    "            location_id,\n",
    "            location_name,\n",
    "            state,\n",
    "            day_of_week,\n",
    "            hour_of_day,\n",
    "            /* æŒ‡æ ‡ */\n",
    "            countDistinct(order_id)                              AS order_count,\n",
    "            sum(item_total_amt)                                  AS total_revenue,\n",
    "            avg(item_total_amt)                                  AS avg_order_value,\n",
    "            sum(item_discount)                                   AS total_discount,\n",
    "            sum(item_discount>0)                                 AS discount_orders,\n",
    "            countDistinct(customer_id)                           AS unique_customers,\n",
    "            sum(is_loyalty)                                      AS loyalty_orders,\n",
    "            sum(arrayExists(x->x='BOGO',assumeNotNull(campaign_names))) AS bogo_orders,\n",
    "            countDistinct(category_name)                         AS category_diversity\n",
    "        FROM dw.fact_order_item_variations\n",
    "        WHERE\n",
    "            created_at_pt >= '{start_date}'\n",
    "            AND created_at_pt <= '{end_date}'\n",
    "            AND pay_status = 'COMPLETED'\n",
    "        GROUP BY\n",
    "            date, location_id, location_name, state, day_of_week, hour_of_day\n",
    "        ORDER BY\n",
    "            date, location_id\n",
    "        \"\"\"\n",
    "        sales_df = self.ch_client.query_df(sales_query)\n",
    "\n",
    "        # â€”â€” ç±»åˆ«ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "        category_query = f\"\"\"\n",
    "        SELECT\n",
    "            toDate(created_at_pt) AS date,\n",
    "            location_id,\n",
    "            category_name,\n",
    "            count()               AS category_orders,\n",
    "            sum(item_total_amt)   AS category_revenue\n",
    "        FROM dw.fact_order_item_variations\n",
    "        WHERE created_at_pt BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND pay_status='COMPLETED'\n",
    "        GROUP BY date, location_id, category_name\n",
    "        \"\"\"\n",
    "        cat_df = self.ch_client.query_df(category_query)\n",
    "\n",
    "        hot_categories = ['Milk Tea', 'Fruit Tea', 'Coffee', 'Snacks']\n",
    "        for cat in hot_categories:\n",
    "            tmp = cat_df[cat_df['category_name'] == cat].copy()\n",
    "            tmp = tmp.rename(columns={\n",
    "                'category_orders': f\"{cat.lower().replace(' ','_')}_orders\",\n",
    "                'category_revenue': f\"{cat.lower().replace(' ','_')}_revenue\"\n",
    "            })\n",
    "            sales_df = sales_df.merge(\n",
    "                tmp[['date','location_id',\n",
    "                     f\"{cat.lower().replace(' ','_')}_orders\",\n",
    "                     f\"{cat.lower().replace(' ','_')}_revenue\"]],\n",
    "                on=['date','location_id'],\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "        return sales_df.fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. è¡ç”Ÿå¤„ç†å˜é‡\n",
    "    # ------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def create_treatment_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['has_promotion']      = (df['total_discount'] > 0).astype(int)\n",
    "        df['promotion_intensity'] = df['total_discount'].astype(float) /(df['total_revenue'].astype(float)  + df['total_discount'].astype(float)  + 1e-3)\n",
    "\n",
    "        df['has_bogo']    = (df['bogo_orders'] > 0).astype(int)\n",
    "        df['is_weekend']  = df['day_of_week'].isin([6,7]).astype(int)\n",
    "        df['is_member_day'] = (df['day_of_week'] == 3).astype(int)\n",
    "        df['is_peak_hour'] = df.groupby('location_id')['order_count'].transform(lambda x: (x > x.quantile(.75)).astype(int))\n",
    "        return df\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. ä¿ƒé”€æ•ˆåº”åˆ†æï¼ˆDoWhy + EconMLï¼‰\n",
    "    # ------------------------------------------------------------\n",
    "    def analyze_promotion_effect(self, df: pd.DataFrame, method='both') -> dict:\n",
    "        \"\"\"\n",
    "        åˆ†æä¿ƒé”€æ•ˆåº”\n",
    "        method: 'dowhy', 'econml', 'both'\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if method in ['dowhy', 'both']:\n",
    "            dowhy_results = self._analyze_promotion_dowhy(df)\n",
    "            results['DoWhy'] = dowhy_results\n",
    "            \n",
    "        if method in ['econml', 'both']:\n",
    "            econml_results = self._analyze_promotion_econml(df)\n",
    "            results['EconML'] = econml_results\n",
    "            \n",
    "        return results\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3.1 DoWhy æ–¹æ³•ï¼ˆä¿®å¤ç‰ˆï¼‰\n",
    "    # ------------------------------------------------------------\n",
    "    def _analyze_promotion_dowhy(self, df: pd.DataFrame) -> dict:\n",
    "        print(\"\\n=== ä¿ƒé”€æ´»åŠ¨å› æœæ•ˆåº”åˆ†æ (DoWhy) ===\\n\")\n",
    "\n",
    "        treatment = 'has_promotion'\n",
    "        outcome   = 'total_revenue'\n",
    "        confs     = ['day_of_week','unique_customers',\n",
    "                     'category_diversity','loyalty_orders','is_weekend']\n",
    "\n",
    "        # æ•°å€¼åŒ–å¤„ç†\n",
    "        df = self._force_numeric(df, [treatment,outcome]+confs).dropna()\n",
    "\n",
    "        # å› æœå›¾å®šä¹‰\n",
    "        graph = \"\"\"\n",
    "        digraph {\n",
    "            is_member_day -> has_promotion;\n",
    "            has_promotion -> total_revenue;\n",
    "\n",
    "            day_of_week -> {has_promotion total_revenue};\n",
    "            unique_customers -> total_revenue;\n",
    "            category_diversity -> total_revenue;\n",
    "            loyalty_orders -> {has_promotion total_revenue};\n",
    "            is_weekend -> {has_promotion total_revenue};\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        model = CausalModel(df, treatment, outcome, graph)\n",
    "        ident = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "        print(\"è¯†åˆ«çš„å› æœæ•ˆåº”:\\n\", ident)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # â‘  PSM\n",
    "        try:\n",
    "            psm = model.estimate_effect(ident,\n",
    "                     method_name=\"backdoor.propensity_score_matching\")\n",
    "            results['PSM'] = round(psm.value,2)\n",
    "            print(f\"å€¾å‘å¾—åˆ†åŒ¹é…ä¼°è®¡: {psm.value:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(\"PSM å¤±è´¥:\", e)\n",
    "\n",
    "        # â‘¡ çº¿æ€§å›å½’\n",
    "        lr = model.estimate_effect(ident,\n",
    "                     method_name=\"backdoor.linear_regression\")\n",
    "        results['LinearRegression'] = round(lr.value,2)\n",
    "        print(f\"çº¿æ€§å›å½’ä¼°è®¡: {lr.value:.2f}\")\n",
    "\n",
    "        # â‘¢ å·¥å…·å˜é‡\n",
    "        try:\n",
    "            iv = model.estimate_effect(ident,\n",
    "                method_name=\"iv.instrumental_variable\",\n",
    "                method_params={'iv_instrument_name':'is_member_day'})\n",
    "            results['IV'] = round(iv.value,2)\n",
    "            print(f\"å·¥å…·å˜é‡ä¼°è®¡: {iv.value:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(\"IV ä¼°è®¡å¤±è´¥:\", e)\n",
    "\n",
    "        # â‘£ åäº‹å®åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰\n",
    "        self._counterfactual_dowhy_fixed(model, ident, df, lr,\n",
    "                                         treatment=treatment, outcome=outcome)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3.2 ä¿®å¤çš„åäº‹å®åˆ†æ\n",
    "    # ------------------------------------------------------------\n",
    "    def _counterfactual_dowhy_fixed(self, model, ident, df, estimator, treatment, outcome):\n",
    "        \"\"\"ä¿®å¤ç‰ˆï¼šä½¿ç”¨å·²ä¼°è®¡çš„æ¨¡å‹è¿›è¡Œåäº‹å®æ¨æ–­\"\"\"\n",
    "        try:\n",
    "            # è·å–å½“å‰å¹³å‡å€¼\n",
    "            y_actual = df[outcome].mean()\n",
    "            \n",
    "            # ä½¿ç”¨ä¼°è®¡å™¨çš„ATEå€¼è¿›è¡Œåäº‹å®è®¡ç®—\n",
    "            ate = estimator.value  # è¿™æ˜¯ä¼°è®¡çš„å¹³å‡å¤„ç†æ•ˆåº”\n",
    "            \n",
    "            # å½“å‰ä¿ƒé”€æ¯”ä¾‹\n",
    "            promo_rate = df[treatment].mean()\n",
    "            \n",
    "            # åäº‹å®è®¡ç®—ï¼š\n",
    "            # 1. å…¨éƒ¨ä¿ƒé”€ï¼šå½“å‰è¥æ”¶ + ATE * (æœªä¿ƒé”€çš„æ¯”ä¾‹)\n",
    "            y_all_promo = y_actual + ate * (1 - promo_rate)\n",
    "            \n",
    "            # 2. å…¨éƒ¨ä¸ä¿ƒé”€ï¼šå½“å‰è¥æ”¶ - ATE * (å·²ä¿ƒé”€çš„æ¯”ä¾‹)\n",
    "            y_no_promo = y_actual - ate * promo_rate\n",
    "            \n",
    "            print(\"\\n=== åäº‹å®æ¨æ–­ (åŸºäºçº¿æ€§å›å½’ ATE) ===\")\n",
    "            print(f\"ä¿ƒé”€è¦†ç›–ç‡          : {promo_rate*100:.1f}%\")\n",
    "            print(f\"ä¼°è®¡çš„ATE          : ${ate:.2f}\")\n",
    "            print(f\"å½“å‰å¹³å‡è¥æ”¶        : ${y_actual:.2f}\")\n",
    "            print(f\"å…¨éƒ¨ä¿ƒé”€ åäº‹å®è¥æ”¶ : ${y_all_promo:.2f}  (+{(y_all_promo/y_actual-1)*100:4.1f}%)\")\n",
    "            print(f\"å…¨éƒ¨åœä¿ƒé”€åäº‹å®è¥æ”¶: ${y_no_promo:.2f}  ({(1-y_no_promo/y_actual)*100:4.1f}% â†“)\")\n",
    "            \n",
    "            # å°è¯•è·å–æ›´è¯¦ç»†çš„æ¨¡å‹ä¿¡æ¯\n",
    "            if hasattr(estimator, 'estimator'):\n",
    "                print(\"\\næ¨¡å‹è¯¦æƒ…:\")\n",
    "                if hasattr(estimator.estimator, 'model'):\n",
    "                    model_obj = estimator.estimator.model\n",
    "                    if hasattr(model_obj, 'params'):\n",
    "                        print(f\"æ¨¡å‹ç³»æ•°: {dict(model_obj.params)}\")\n",
    "                    if hasattr(model_obj, 'summary'):\n",
    "                        print(\"\\nå›å½’æ¨¡å‹æ‘˜è¦:\")\n",
    "                        print(model_obj.summary())\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"åäº‹å®åˆ†æå‡ºé”™: {e}\")\n",
    "            print(f\"é”™è¯¯è¯¦æƒ…: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3.3 EconML æ–¹æ³•\n",
    "    # ------------------------------------------------------------\n",
    "    def _analyze_promotion_econml(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"ä½¿ç”¨EconMLçš„CausalForestè¿›è¡Œä¿ƒé”€æ•ˆåº”åˆ†æ\"\"\"\n",
    "        print(\"\\n=== ä¿ƒé”€æ´»åŠ¨å› æœæ•ˆåº”åˆ†æ (EconML) ===\\n\")\n",
    "        \n",
    "        treatment = 'has_promotion'\n",
    "        outcome = 'total_revenue'\n",
    "        feature_cols = ['day_of_week', 'unique_customers', 'category_diversity',\n",
    "                       'loyalty_orders', 'is_weekend']\n",
    "        \n",
    "        # æ•°æ®é¢„å¤„ç†\n",
    "        df = df.copy()\n",
    "        df[feature_cols + [outcome]] = df[feature_cols + [outcome]].apply(\n",
    "            pd.to_numeric, errors='coerce')\n",
    "        df = df.dropna(subset=feature_cols + [outcome])\n",
    "        \n",
    "        Y = df[outcome].values\n",
    "        T = df[treatment].values.astype(int)  # ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹\n",
    "        X = df[feature_cols].values\n",
    "        \n",
    "        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "        X_tr, X_te, T_tr, T_te, Y_tr, Y_te = train_test_split(\n",
    "            X, T, Y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # è®­ç»ƒCausalForestæ¨¡å‹\n",
    "        # å¯¹äºäºŒå€¼å¤„ç†å˜é‡ï¼Œä½¿ç”¨ discrete_treatment=True\n",
    "        cf = CausalForestDML(\n",
    "            model_t='auto',  # è®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "            model_y=RandomForestRegressor(n_estimators=200, max_depth=6),\n",
    "            discrete_treatment=True,  # æŒ‡å®šå¤„ç†å˜é‡æ˜¯ç¦»æ•£çš„\n",
    "            n_estimators=500,  # å‡å°‘æ ‘çš„æ•°é‡ä»¥åŠ å¿«è®­ç»ƒ\n",
    "            min_samples_leaf=50,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            cf.fit(Y_tr, T_tr, X=X_tr)\n",
    "            \n",
    "            # è®¡ç®—æ•ˆåº”\n",
    "            cate_te = cf.effect(X_te)\n",
    "            ate = float(cate_te.mean())\n",
    "            \n",
    "            # åäº‹å®åˆ†æ\n",
    "            rev_actual = Y_te.mean()\n",
    "            \n",
    "            # å¯¹æœªä¿ƒé”€çš„æ ·æœ¬ï¼Œè®¡ç®—å¦‚æœä¿ƒé”€çš„æ•ˆåº”\n",
    "            no_promo_idx = T_te == 0\n",
    "            promo_effect_on_no_promo = cate_te[no_promo_idx].mean() if no_promo_idx.any() else 0\n",
    "            \n",
    "            # å¯¹å·²ä¿ƒé”€çš„æ ·æœ¬ï¼Œè®¡ç®—å¦‚æœä¸ä¿ƒé”€çš„æ•ˆåº”ï¼ˆè´Ÿæ•ˆåº”ï¼‰\n",
    "            promo_idx = T_te == 1\n",
    "            no_promo_effect_on_promo = -cate_te[promo_idx].mean() if promo_idx.any() else 0\n",
    "            \n",
    "            # è®¡ç®—åäº‹å®åœºæ™¯\n",
    "            promo_rate = T_te.mean()\n",
    "            rev_all_promo = rev_actual + promo_effect_on_no_promo * (1 - promo_rate)\n",
    "            rev_no_promo = rev_actual + no_promo_effect_on_promo * promo_rate\n",
    "            \n",
    "            print(f\"CausalForest ATE: ${ate:.2f}\")\n",
    "            print(f\"\\n=== åäº‹å®æ¨æ–­ (EconML) ===\")\n",
    "            print(f\"å½“å‰å¹³å‡è¥æ”¶        : ${rev_actual:.2f}\")\n",
    "            print(f\"å…¨éƒ¨ä¿ƒé”€ åäº‹å®è¥æ”¶ : ${rev_all_promo:.2f}  (+{(rev_all_promo/rev_actual-1)*100:4.1f}%)\")\n",
    "            print(f\"å…¨éƒ¨åœä¿ƒé”€åäº‹å®è¥æ”¶: ${rev_no_promo:.2f}  ({(1-rev_no_promo/rev_actual)*100:4.1f}% â†“)\")\n",
    "            \n",
    "            results = {\n",
    "                'CausalForest_ATE': ate,\n",
    "                'Actual_Revenue': rev_actual,\n",
    "                'AllPromo_Revenue': rev_all_promo,\n",
    "                'NoPromo_Revenue': rev_no_promo,\n",
    "                'CATE_std': float(cate_te.std())\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"EconMLåˆ†æå¤±è´¥: {e}\")\n",
    "            print(\"å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹...\")\n",
    "            \n",
    "            # é™çº§åˆ°çº¿æ€§DMLæ¨¡å‹\n",
    "            ldml = LinearDML(\n",
    "                model_t='auto',\n",
    "                model_y=RandomForestRegressor(n_estimators=100, max_depth=5),\n",
    "                discrete_treatment=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            ldml.fit(Y_tr, T_tr, X=X_tr)\n",
    "            ate = ldml.ate(X_te)\n",
    "            \n",
    "            print(f\"\\nLinearDML ATE: ${ate:.2f}\")\n",
    "            \n",
    "            return {\n",
    "                'LinearDML_ATE': ate,\n",
    "                'Actual_Revenue': Y_te.mean(),\n",
    "                'Fallback': True\n",
    "            }\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. å¤©æ°”æ•ˆåº”ï¼ˆEconML TLearnerï¼‰\n",
    "    # ------------------------------------------------------------\n",
    "    def analyze_weather_effect(self, sales_df, weather_df):\n",
    "        print(\"\\n=== å¤©æ°”å› æœæ•ˆåº”åˆ†æ (EconML TLearner) ===\\n\")\n",
    "\n",
    "        merged = sales_df.merge(weather_df, on=['date','state'], how='left')\n",
    "        merged['is_hot'] = (merged['temperature_2m_max'] > 30).astype(int)\n",
    "\n",
    "        Y = merged['total_revenue'].values\n",
    "        T = merged['is_hot'].values\n",
    "        X = merged[['day_of_week','unique_customers','category_diversity']].values\n",
    "\n",
    "        t_learner = TLearner(models=RandomForestRegressor(n_estimators=200,\n",
    "                                                          random_state=42))\n",
    "        t_learner.fit(Y, T, X=X)\n",
    "        cate = t_learner.effect(X)\n",
    "\n",
    "        print(f\"é«˜æ¸©å¤©æ°”å¹³å‡å› æœæ•ˆåº”: ${cate.mean():.2f} (Â±{cate.std():.2f})\")\n",
    "        return merged, cate\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. å°å·¥å…·\n",
    "    # ------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _force_numeric(df, cols):\n",
    "        \"\"\"æŠŠ object / Decimal å…¨è½¬ float64ï¼ŒUInt* è½¬ int64\"\"\"\n",
    "        out = df.copy()\n",
    "        for c in cols:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. å¯è§†åŒ–\n",
    "    # ------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def visualize_causal_effects(effects: dict):\n",
    "        \"\"\"å¯è§†åŒ–å› æœæ•ˆåº”å¯¹æ¯”\"\"\"\n",
    "        if 'DoWhy' in effects and 'EconML' in effects:\n",
    "            # ç»„åˆæ˜¾ç¤º\n",
    "            dowhy_vals = effects['DoWhy']\n",
    "            econml_res = effects['EconML']\n",
    "            \n",
    "            if 'Fallback' in econml_res and econml_res['Fallback']:\n",
    "                methods = list(dowhy_vals.keys()) + ['LinearDML']\n",
    "                values = list(dowhy_vals.values()) + [econml_res.get('LinearDML_ATE', 0)]\n",
    "            else:\n",
    "                methods = list(dowhy_vals.keys()) + ['CausalForest']\n",
    "                values = list(dowhy_vals.values()) + [econml_res.get('CausalForest_ATE', 0)]\n",
    "            \n",
    "            fig = go.Figure(go.Bar(\n",
    "                x=methods,\n",
    "                y=values,\n",
    "                text=[f\"${v:.0f}\" for v in values],\n",
    "                textposition='auto'\n",
    "            ))\n",
    "            fig.update_yaxes(title='å¹³å‡å› æœæ•ˆåº” ($)')\n",
    "            fig.update_layout(title='ä¿ƒé”€å› æœæ•ˆåº”ä¼°è®¡å¯¹æ¯” (DoWhy vs EconML)')\n",
    "            \n",
    "        elif 'DoWhy' in effects:\n",
    "            fig = go.Figure(go.Bar(\n",
    "                x=list(effects['DoWhy'].keys()),\n",
    "                y=list(effects['DoWhy'].values()),\n",
    "                text=[f\"${v:.0f}\" for v in effects['DoWhy'].values()],\n",
    "                textposition='auto'\n",
    "            ))\n",
    "            fig.update_yaxes(title='å¹³å‡å› æœæ•ˆåº” ($)')\n",
    "            fig.update_layout(title='ä¿ƒé”€å› æœæ•ˆåº”ä¼°è®¡å¯¹æ¯” (DoWhy)')\n",
    "            \n",
    "        else:\n",
    "            # EconML only\n",
    "            econml_res = effects['EconML']\n",
    "            if 'Fallback' in econml_res and econml_res['Fallback']:\n",
    "                ate = econml_res.get('LinearDML_ATE', 0)\n",
    "                title = 'ä¿ƒé”€å› æœæ•ˆåº” (LinearDML)'\n",
    "                label = 'LinearDML ATE'\n",
    "            else:\n",
    "                ate = econml_res.get('CausalForest_ATE', 0)\n",
    "                title = 'ä¿ƒé”€å› æœæ•ˆåº” (CausalForest)'\n",
    "                label = 'CausalForest ATE'\n",
    "                \n",
    "            fig = go.Figure(go.Bar(\n",
    "                x=[label],\n",
    "                y=[ate],\n",
    "                text=[f\"${ate:.0f}\"],\n",
    "                textposition='auto'\n",
    "            ))\n",
    "            fig.update_yaxes(title='å¹³å‡å› æœæ•ˆåº” ($)')\n",
    "            fig.update_layout(title=title)\n",
    "            \n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_counterfactual_scenarios(results: dict):\n",
    "        \"\"\"å¯è§†åŒ–åäº‹å®åœºæ™¯å¯¹æ¯”\"\"\"\n",
    "        scenarios = ['å½“å‰å®é™…', 'å…¨éƒ¨ä¿ƒé”€', 'å…¨éƒ¨ä¸ä¿ƒé”€']\n",
    "        \n",
    "        if 'EconML' in results:\n",
    "            econml_res = results['EconML']\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æ˜¯ fallback æ¨¡å¼\n",
    "            if 'Fallback' in econml_res and econml_res['Fallback']:\n",
    "                print(\"ä½¿ç”¨ LinearDML ç»“æœï¼Œåäº‹å®åœºæ™¯å¯è§†åŒ–ä¸å¯ç”¨\")\n",
    "                return None\n",
    "                \n",
    "            values = [\n",
    "                econml_res.get('Actual_Revenue', 0),\n",
    "                econml_res.get('AllPromo_Revenue', 0),\n",
    "                econml_res.get('NoPromo_Revenue', 0)\n",
    "            ]\n",
    "        else:\n",
    "            # ä»DoWhyç»“æœæ¨ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "            return None\n",
    "            \n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=scenarios,\n",
    "            y=values,\n",
    "            text=[f\"${v:.0f}\" for v in values],\n",
    "            textposition='auto',\n",
    "            marker_color=['blue', 'green', 'red']\n",
    "        ))\n",
    "        \n",
    "        fig.update_yaxes(title='å¹³å‡è¥æ”¶ ($)')\n",
    "        fig.update_layout(title='åäº‹å®åœºæ™¯åˆ†æ')\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    CLICKHOUSE_CONFIG = dict(\n",
    "        host=\"clickhouse-0-0.umetea.net\",\n",
    "        port=443,\n",
    "        database=\"dw\",\n",
    "        user=\"ml_ume\",\n",
    "        password=\"hDAoDvg8x552bH\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    ci = FBRCausalInference(CLICKHOUSE_CONFIG)\n",
    "\n",
    "    # åŠ è½½æ•°æ®\n",
    "    start_date, end_date = \"2025-06-01\", \"2025-07-31\"\n",
    "    sales = ci.load_integrated_data(start_date, end_date)\n",
    "    sales = ci.create_treatment_variables(sales)\n",
    "\n",
    "    # æ–¹æ³•1ï¼šåˆ†æä¿ƒé”€æ•ˆåº”ï¼ˆä½¿ç”¨ä¸¤ç§æ–¹æ³•ï¼‰\n",
    "    print(\"=\"*60)\n",
    "    print(\"è¿è¡Œå®Œæ•´åˆ†æï¼ˆDoWhy + EconMLï¼‰...\")\n",
    "    print(\"=\"*60)\n",
    "    promo_results = ci.analyze_promotion_effect(sales, method='both')\n",
    "    \n",
    "    # å¯è§†åŒ–ç»“æœ\n",
    "    fig1 = ci.visualize_causal_effects(promo_results)\n",
    "    fig1.show()\n",
    "    \n",
    "    # å¯è§†åŒ–åäº‹å®åœºæ™¯\n",
    "    fig2 = ci.visualize_counterfactual_scenarios(promo_results)\n",
    "    if fig2:\n",
    "        fig2.show()\n",
    "        \n",
    "    # æ–¹æ³•2ï¼šä»…ä½¿ç”¨ EconMLï¼ˆæ¨èï¼Œæ›´ç¨³å®šï¼‰\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ä»…è¿è¡Œ EconML åˆ†æ...\")\n",
    "    print(\"=\"*60)\n",
    "    econml_only = ci.analyze_promotion_effect(sales, method='econml')\n",
    "    fig3 = ci.visualize_causal_effects(econml_only)\n",
    "    fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b3bd1f-5440-4e52-8ad0-1331eb7d1b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T02:15:02.617301Z",
     "start_time": "2025-08-04T02:14:48.933981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UMe èŒ¶é¥®å¢å¼ºç‰ˆå› æœæ¨æ–­åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ åŠ è½½é”€å”®æ•°æ®...\n",
      "åŠ è½½äº† 1362 æ¡é”€å”®è®°å½•\n",
      "\n",
      "2ï¸âƒ£ è·å–å¤©æ°”æ•°æ®...\n",
      "\n",
      "=== è·å–å¤©æ°”æ•°æ® ===\n",
      "è·³è¿‡æœªçŸ¥å·: UM\n",
      "è·³è¿‡æœªçŸ¥å·: Me\n",
      "æˆåŠŸè·å– 183 æ¡å¤©æ°”è®°å½•\n",
      "\n",
      "3ï¸âƒ£ æ·»åŠ æ—¥å†ç‰¹å¾...\n",
      "\n",
      "4ï¸âƒ£ åˆå¹¶å¤©æ°”ç‰¹å¾...\n",
      "\n",
      "5ï¸âƒ£ åˆ›å»ºå¤„ç†å˜é‡...\n",
      "\n",
      "6ï¸âƒ£ æ‰§è¡Œå¤šå› ç´ å› æœåˆ†æ...\n",
      "\n",
      "=== å¤šå› ç´ å› æœæ•ˆåº”åˆ†æ ===\n",
      "\n",
      "\n",
      "--- åˆ†æ ä¿ƒé”€æ´»åŠ¨ çš„å› æœæ•ˆåº” ---\n",
      "ä¿ƒé”€æ´»åŠ¨ åˆ†æç»“æœ:\n",
      "  å› æœæ•ˆåº” (ATE): $358.09\n",
      "  è§‚å¯Ÿåˆ°çš„å·®å¼‚: $2172.52\n",
      "  å¤„ç†ç‡: 93.5%\n",
      "  æ ·æœ¬é‡: 1362\n",
      "\n",
      "--- åˆ†æ é«˜æ¸©å¤©æ°” çš„å› æœæ•ˆåº” ---\n",
      "é«˜æ¸©å¤©æ°” åˆ†æç»“æœ:\n",
      "  å› æœæ•ˆåº” (ATE): $-206.01\n",
      "  è§‚å¯Ÿåˆ°çš„å·®å¼‚: $-1721.68\n",
      "  å¤„ç†ç‡: 12.7%\n",
      "  æ ·æœ¬é‡: 1362\n",
      "\n",
      "--- åˆ†æ é›¨å¤© çš„å› æœæ•ˆåº” ---\n",
      "é›¨å¤© åˆ†æç»“æœ:\n",
      "  å› æœæ•ˆåº” (ATE): $-1142.95\n",
      "  è§‚å¯Ÿåˆ°çš„å·®å¼‚: $-919.79\n",
      "  å¤„ç†ç‡: 5.1%\n",
      "  æ ·æœ¬é‡: 1362\n",
      "\n",
      "--- åˆ†æ èŠ‚å‡æ—¥ çš„å› æœæ•ˆåº” ---\n",
      "èŠ‚å‡æ—¥ åˆ†æç»“æœ:\n",
      "  å› æœæ•ˆåº” (ATE): $-630.23\n",
      "  è§‚å¯Ÿåˆ°çš„å·®å¼‚: $91.02\n",
      "  å¤„ç†ç‡: 3.3%\n",
      "  æ ·æœ¬é‡: 1362\n",
      "\n",
      "--- åˆ†æ å‘¨æœ« çš„å› æœæ•ˆåº” ---\n",
      "å‘¨æœ« åˆ†æç»“æœ:\n",
      "  å› æœæ•ˆåº” (ATE): $-301.11\n",
      "  è§‚å¯Ÿåˆ°çš„å·®å¼‚: $473.88\n",
      "  å¤„ç†ç‡: 28.2%\n",
      "  æ ·æœ¬é‡: 1362\n",
      "\n",
      "7ï¸âƒ£ æ‰§è¡Œäº¤äº’æ•ˆåº”åˆ†æ...\n",
      "\n",
      "=== äº¤äº’æ•ˆåº”åˆ†æ ===\n",
      "\n",
      "\n",
      "--- é›¨å¤©ä¿ƒé”€äº¤äº’æ•ˆåº” ---\n",
      "  is_rainy å•ç‹¬æ•ˆåº”: $276.23\n",
      "  has_promotion å•ç‹¬æ•ˆåº”: $2241.00\n",
      "  äº¤äº’æ•ˆåº”: $-1236.21\n",
      "\n",
      "--- é«˜æ¸©å¤©æ°”å¯¹ä¸åŒäº§å“çš„å½±å“ ---\n",
      "  cold_drink_orders: -72.4 (-59.8%)\n",
      "  hot_drink_orders: +0.0 (+nan%)\n",
      "  food_orders: -40.1 (-83.2%)\n",
      "\n",
      "--- èŠ‚å‡æ—¥å‘¨æœ«äº¤äº’æ•ˆåº” ---\n",
      "  is_holiday å•ç‹¬æ•ˆåº”: $232.30\n",
      "  is_weekend å•ç‹¬æ•ˆåº”: $484.57\n",
      "  äº¤äº’æ•ˆåº”: $nan\n",
      "\n",
      "8ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–...\n",
      "\n",
      "9ï¸âƒ£ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Š\n",
      "================================================================================\n",
      "# UMe èŒ¶é¥®é”€å”®å› æœåˆ†ææ´å¯ŸæŠ¥å‘Š\n",
      "\n",
      "## ğŸ“Š æ€»ä½“æ¦‚è§ˆ\n",
      "- å¹³å‡æ—¥è¥æ”¶: $2291\n",
      "- å¹³å‡æ—¥è®¢å•: 148å•\n",
      "- åˆ†ææœŸé—´: 2025-06-01 00:00:00 è‡³ 2025-07-30 00:00:00\n",
      "\n",
      "## ğŸ¯ ä¸»è¦å½±å“å› ç´ \n",
      "1. **Rainy Weather**: -1143$ (å‘ç”Ÿç‡: 5.1%)\n",
      "2. **Holiday**: -630$ (å‘ç”Ÿç‡: 3.3%)\n",
      "3. **Promotion**: +358$ (å‘ç”Ÿç‡: 93.5%)\n",
      "4. **Weekend**: -301$ (å‘ç”Ÿç‡: 28.2%)\n",
      "5. **Hot Weather**: -206$ (å‘ç”Ÿç‡: 12.7%)\n",
      "\n",
      "## ğŸ” å…³é”®å‘ç°\n",
      "- ä¿ƒé”€æ´»åŠ¨å¹³å‡å¸¦æ¥ $358 çš„è¥æ”¶æå‡\n",
      "- é«˜æ¸©å¤©æ°”å¯¹è¥æ”¶çš„å½±å“: -206$\n",
      "- é›¨å¤©å¯¹è¥æ”¶çš„å½±å“: -1143$\n",
      "- èŠ‚å‡æ—¥å¸¦æ¥è¥æ”¶æå‡: $-630\n",
      "\n",
      "## ğŸ”„ äº¤äº’æ•ˆåº”æ´å¯Ÿ\n",
      "- é›¨å¤©ä¿ƒé”€äº¤äº’æ•ˆåº”: -1236$\n",
      "  â†’ é›¨å¤©ä¿ƒé”€æ•ˆæœä¸å¦‚é¢„æœŸ\n",
      "\n",
      "## ğŸ’¡ è¡ŒåŠ¨å»ºè®®\n",
      "2. **å¤©æ°”é€‚åº”æ€§è¥é”€**: æ ¹æ®å¤©æ°”é¢„æŠ¥è°ƒæ•´äº§å“æ¨å¹¿å’Œåº“å­˜\n",
      "3. **ä¿ƒé”€ç­–ç•¥ä¼˜åŒ–**: å½“å‰ä¿ƒé”€æœ‰æ•ˆï¼Œå¯ä»¥æ‰©å¤§èŒƒå›´\n",
      "\n",
      "âœ… åˆ†æå®Œæˆï¼\n",
      "ğŸ’¡ å»ºè®®ä¿å­˜åˆ†æç»“æœç”¨äºå†³ç­–å‚è€ƒ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "å¢å¼ºç‰ˆ UMe èŒ¶é¥®é”€å”®æ•°æ®å› æœæ¨æ–­åˆ†æå¼•æ“\n",
    "æ–°å¢ï¼šå¤©æ°”ã€èŠ‚å‡æ—¥ã€å‘¨æœ«ç­‰å¤–éƒ¨å› ç´ çš„å› æœå½±å“åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import clickhouse_connect\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, List, Any\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# å› æœæ¨æ–­\n",
    "from dowhy import CausalModel\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# EconMLï¼ˆå¼‚è´¨æ•ˆåº”åˆ†æï¼‰\n",
    "from econml.metalearners import TLearner\n",
    "from econml.dml import CausalForestDML, LinearDML\n",
    "\n",
    "# å¤©æ°”æ•°æ®API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# èŠ‚å‡æ—¥æ•°æ®\n",
    "import holidays\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "\n",
    "class EnhancedFBRCausalInference:\n",
    "    \"\"\"å¢å¼ºç‰ˆ FBR å› æœæ¨æ–­åˆ†æå¼•æ“ - åŒ…å«å¤©æ°”å’ŒèŠ‚å‡æ—¥åˆ†æ\"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 0. åˆå§‹åŒ–\n",
    "    # ------------------------------------------------------------\n",
    "    def __init__(self, ch_config: dict, weather_api_key: str = None):\n",
    "        self.ch_client = clickhouse_connect.get_client(**ch_config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.weather_api_key = weather_api_key\n",
    "\n",
    "        # ç¾å›½èŠ‚å‡æ—¥ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•åˆ°å…¶ä»–å›½å®¶ï¼‰\n",
    "        self.us_holidays = holidays.US()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. æ•°æ®æŠ½å–ï¼ˆå¢å¼ºç‰ˆï¼‰\n",
    "    # ------------------------------------------------------------\n",
    "    def load_integrated_data(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"æ‹‰å–æ—¥ Ã— åº—é“ºé”€å”®èšåˆ + ç±»åˆ«ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆï¼‰\"\"\"\n",
    "        sales_query = f\"\"\"\n",
    "        WITH\n",
    "            toDate(created_at_pt)          AS dt,\n",
    "            toDayOfWeek(created_at_pt)     AS day_of_week,\n",
    "            toHour(created_at_pt)          AS hour_of_day,\n",
    "            substring(location_name, position(location_name,'-')+1, 2) AS state,\n",
    "            toMonth(created_at_pt)         AS month,\n",
    "            toDayOfMonth(created_at_pt)    AS day_of_month\n",
    "        SELECT\n",
    "            dt  AS date,\n",
    "            location_id,\n",
    "            location_name,\n",
    "            state,\n",
    "            day_of_week,\n",
    "            hour_of_day,\n",
    "            month,\n",
    "            day_of_month,\n",
    "            /* åŸºç¡€æŒ‡æ ‡ */\n",
    "            countDistinct(order_id)                              AS order_count,\n",
    "            sum(item_total_amt)                                  AS total_revenue,\n",
    "            avg(item_total_amt)                                  AS avg_order_value,\n",
    "            sum(item_discount)                                   AS total_discount,\n",
    "            sum(item_discount>0)                                 AS discount_orders,\n",
    "            countDistinct(customer_id)                           AS unique_customers,\n",
    "            sum(is_loyalty)                                      AS loyalty_orders,\n",
    "            sum(arrayExists(x->x='BOGO',assumeNotNull(campaign_names))) AS bogo_orders,\n",
    "            countDistinct(category_name)                         AS category_diversity,\n",
    "\n",
    "            /* æ—¶æ®µç›¸å…³æŒ‡æ ‡ */\n",
    "            sum(if(hour_of_day BETWEEN 7 AND 10, 1, 0))         AS morning_orders,\n",
    "            sum(if(hour_of_day BETWEEN 11 AND 14, 1, 0))        AS lunch_orders,\n",
    "            sum(if(hour_of_day BETWEEN 15 AND 17, 1, 0))        AS afternoon_orders,\n",
    "            sum(if(hour_of_day BETWEEN 18 AND 21, 1, 0))        AS evening_orders,\n",
    "\n",
    "            /* äº§å“ç±»å‹æŒ‡æ ‡ */\n",
    "            sum(if(category_name IN ('Milk Tea', 'Fruit Tea'), 1, 0)) AS cold_drink_orders,\n",
    "            sum(if(category_name = 'Coffee', 1, 0))              AS hot_drink_orders,\n",
    "            sum(if(category_name = 'Snacks', 1, 0))              AS food_orders\n",
    "        FROM dw.fact_order_item_variations\n",
    "        WHERE\n",
    "            created_at_pt >= '{start_date}'\n",
    "            AND created_at_pt <= '{end_date}'\n",
    "            AND pay_status = 'COMPLETED'\n",
    "        GROUP BY\n",
    "            date, location_id, location_name, state, day_of_week, hour_of_day, month, day_of_month\n",
    "        ORDER BY\n",
    "            date, location_id\n",
    "        \"\"\"\n",
    "        sales_df = self.ch_client.query_df(sales_query)\n",
    "\n",
    "        # èšåˆåˆ°æ—¥çº§åˆ«\n",
    "        daily_agg = sales_df.groupby(['date', 'location_id', 'location_name', 'state']).agg({\n",
    "            'order_count': 'sum',\n",
    "            'total_revenue': 'sum',\n",
    "            'avg_order_value': 'mean',\n",
    "            'total_discount': 'sum',\n",
    "            'discount_orders': 'sum',\n",
    "            'unique_customers': 'sum',\n",
    "            'loyalty_orders': 'sum',\n",
    "            'bogo_orders': 'sum',\n",
    "            'category_diversity': 'max',\n",
    "            'morning_orders': 'sum',\n",
    "            'lunch_orders': 'sum',\n",
    "            'afternoon_orders': 'sum',\n",
    "            'evening_orders': 'sum',\n",
    "            'cold_drink_orders': 'sum',\n",
    "            'hot_drink_orders': 'sum',\n",
    "            'food_orders': 'sum',\n",
    "            'day_of_week': 'first',\n",
    "            'month': 'first',\n",
    "            'day_of_month': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        # å…³é”®ä¿®å¤ï¼šç«‹å³è½¬æ¢æ•°å€¼ç±»å‹\n",
    "        numeric_cols = [\n",
    "            'order_count', 'total_revenue', 'avg_order_value', 'total_discount',\n",
    "            'discount_orders', 'unique_customers', 'loyalty_orders', 'bogo_orders',\n",
    "            'category_diversity', 'morning_orders', 'lunch_orders', 'afternoon_orders',\n",
    "            'evening_orders', 'cold_drink_orders', 'hot_drink_orders', 'food_orders',\n",
    "            'day_of_week', 'month', 'day_of_month'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in daily_agg.columns:\n",
    "                daily_agg[col] = pd.to_numeric(daily_agg[col], errors='coerce')\n",
    "\n",
    "        return daily_agg.fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. å¤©æ°”æ•°æ®è·å–\n",
    "    # ------------------------------------------------------------\n",
    "    def get_weather_data(self, start_date: str, end_date: str, locations: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"è·å–å¤©æ°”æ•°æ®\"\"\"\n",
    "        print(\"\\n=== è·å–å¤©æ°”æ•°æ® ===\")\n",
    "\n",
    "        weather_data_list = []\n",
    "\n",
    "        # è·å–å„å·çš„ä¸»è¦åŸå¸‚åæ ‡ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“åº—é“ºä½ç½®ï¼‰\n",
    "        state_coords = {\n",
    "            'CA': {'lat': 37.7749, 'lon': -122.4194, 'city': 'San Francisco'},  # åŠ å·\n",
    "            'IL': {'lat': 41.8781, 'lon': -87.6298, 'city': 'Chicago'},       # ä¼Šåˆ©è¯ºä¼Šå·\n",
    "            'AZ': {'lat': 33.4484, 'lon': -112.0740, 'city': 'Phoenix'},      # äºšåˆ©æ¡‘é‚£å·\n",
    "            'TX': {'lat': 29.7604, 'lon': -95.3698, 'city': 'Houston'},       # å¾·å…‹è¨æ–¯å·\n",
    "        }\n",
    "\n",
    "        unique_states = locations['state'].unique()\n",
    "\n",
    "        for state in unique_states:\n",
    "            if state not in state_coords:\n",
    "                print(f\"è·³è¿‡æœªçŸ¥å·: {state}\")\n",
    "                continue\n",
    "\n",
    "            coords = state_coords[state]\n",
    "            weather_data = self._fetch_weather_api(\n",
    "                start_date, end_date,\n",
    "                coords['lat'], coords['lon'], state\n",
    "            )\n",
    "\n",
    "            if weather_data is not None:\n",
    "                weather_data_list.append(weather_data)\n",
    "\n",
    "        if weather_data_list:\n",
    "            weather_df = pd.concat(weather_data_list, ignore_index=True)\n",
    "            print(f\"æˆåŠŸè·å– {len(weather_df)} æ¡å¤©æ°”è®°å½•\")\n",
    "            return weather_df\n",
    "        else:\n",
    "            # å¦‚æœæ— æ³•è·å–çœŸå®å¤©æ°”æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "            print(\"æ— æ³•è·å–çœŸå®å¤©æ°”æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...\")\n",
    "            return self._generate_mock_weather_data(start_date, end_date, unique_states)\n",
    "\n",
    "    def _fetch_weather_api(self, start_date: str, end_date: str, lat: float, lon: float, state: str) -> pd.DataFrame:\n",
    "        \"\"\"ä»å¤©æ°”APIè·å–æ•°æ®ï¼ˆä½¿ç”¨Open-Meteoå…è´¹APIï¼‰\"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ Open-Meteo APIï¼ˆå…è´¹ï¼Œæ— éœ€API keyï¼‰\n",
    "            url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "            params = {\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'daily': [\n",
    "                    'temperature_2m_max', 'temperature_2m_min', 'temperature_2m_mean',\n",
    "                    'precipitation_sum', 'rain_sum', 'snowfall_sum',\n",
    "                    'windspeed_10m_max', 'sunshine_duration'\n",
    "                ],\n",
    "                'timezone': 'America/Los_Angeles'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                weather_df = pd.DataFrame({\n",
    "                    'date': pd.to_datetime(data['daily']['time']),\n",
    "                    'state': state,\n",
    "                    'temperature_max': data['daily']['temperature_2m_max'],\n",
    "                    'temperature_min': data['daily']['temperature_2m_min'],\n",
    "                    'temperature_mean': data['daily']['temperature_2m_mean'],\n",
    "                    'precipitation': data['daily']['precipitation_sum'],\n",
    "                    'rain': data['daily']['rain_sum'],\n",
    "                    'snow': data['daily']['snowfall_sum'],\n",
    "                    'wind_speed': data['daily']['windspeed_10m_max'],\n",
    "                    'sunshine_hours': data['daily']['sunshine_duration']\n",
    "                })\n",
    "\n",
    "                return weather_df\n",
    "            else:\n",
    "                print(f\"å¤©æ°”APIè¯·æ±‚å¤±è´¥: {response.status_code}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"å¤©æ°”æ•°æ®è·å–å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _generate_mock_weather_data(self, start_date: str, end_date: str, states: list) -> pd.DataFrame:\n",
    "        \"\"\"ç”Ÿæˆæ¨¡æ‹Ÿå¤©æ°”æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰\"\"\"\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        weather_data = []\n",
    "\n",
    "        for state in states:\n",
    "            # æ ¹æ®å·è®¾ç½®åŸºç¡€æ¸©åº¦ï¼ˆç®€åŒ–ï¼‰\n",
    "            base_temp = {\n",
    "                'CA': 22, 'IL': 15, 'AZ': 30, 'TX': 25\n",
    "            }.get(state, 20)\n",
    "\n",
    "            for date in date_range:\n",
    "                # ç”Ÿæˆå¸¦å­£èŠ‚æ€§çš„æ¨¡æ‹Ÿå¤©æ°”æ•°æ®\n",
    "                day_of_year = date.timetuple().tm_yday\n",
    "                seasonal_factor = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "                temp_max = base_temp + 8 * seasonal_factor + np.random.normal(0, 3)\n",
    "                temp_min = temp_max - np.random.uniform(5, 15)\n",
    "                temp_mean = (temp_max + temp_min) / 2\n",
    "\n",
    "                weather_data.append({\n",
    "                    'date': date,\n",
    "                    'state': state,\n",
    "                    'temperature_max': temp_max,\n",
    "                    'temperature_min': temp_min,\n",
    "                    'temperature_mean': temp_mean,\n",
    "                    'precipitation': max(0, np.random.exponential(2) - 1),\n",
    "                    'rain': max(0, np.random.exponential(1.5) - 0.5),\n",
    "                    'snow': 0 if state in ['CA', 'AZ', 'TX'] else max(0, np.random.exponential(0.5) - 2),\n",
    "                    'wind_speed': np.random.uniform(5, 25),\n",
    "                    'sunshine_hours': np.random.uniform(4, 12)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(weather_data)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. èŠ‚å‡æ—¥å’Œç‰¹æ®Šæ—¥æœŸæ•°æ®\n",
    "    # ------------------------------------------------------------\n",
    "    def add_calendar_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ·»åŠ æ—¥å†ç‰¹å¾ï¼ˆèŠ‚å‡æ—¥ã€å‘¨æœ«ç­‰ï¼‰\"\"\"\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # åŸºç¡€æ—¥æœŸç‰¹å¾\n",
    "        df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        df['is_monday'] = (df['date'].dt.dayofweek == 0).astype(int)\n",
    "        df['is_friday'] = (df['date'].dt.dayofweek == 4).astype(int)\n",
    "\n",
    "        # èŠ‚å‡æ—¥ç‰¹å¾\n",
    "        df['is_holiday'] = df['date'].apply(lambda x: x.date() in self.us_holidays).astype(int)\n",
    "        df['is_holiday_week'] = df['date'].apply(\n",
    "            lambda x: any((x + timedelta(days=i)).date() in self.us_holidays for i in range(-3, 4))\n",
    "        ).astype(int)\n",
    "\n",
    "        # ç‰¹æ®ŠèŠ‚æ—¥\n",
    "        df['is_valentine'] = ((df['date'].dt.month == 2) & (df['date'].dt.day == 14)).astype(int)\n",
    "        df['is_christmas_season'] = ((df['date'].dt.month == 12) & (df['date'].dt.day >= 15)).astype(int)\n",
    "        df['is_thanksgiving_week'] = df['date'].apply(self._is_thanksgiving_week).astype(int)\n",
    "        df['is_summer'] = df['date'].dt.month.isin([6, 7, 8]).astype(int)\n",
    "        df['is_winter'] = df['date'].dt.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "        # å­¦æœŸç‰¹å¾ï¼ˆå½±å“å­¦ç”Ÿå®¢æµï¼‰\n",
    "        df['is_school_term'] = df['date'].apply(self._is_school_term).astype(int)\n",
    "        df['is_finals_week'] = df['date'].apply(self._is_finals_week).astype(int)\n",
    "\n",
    "        # å‘è–ªæ—¥ç‰¹å¾ï¼ˆå½±å“æ¶ˆè´¹åŠ›ï¼‰\n",
    "        df['is_payday'] = df['date'].apply(self._is_payday).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _is_thanksgiving_week(self, date):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºæ„Ÿæ©èŠ‚å‘¨\"\"\"\n",
    "        # æ„Ÿæ©èŠ‚æ˜¯11æœˆçš„ç¬¬å››ä¸ªæ˜ŸæœŸå››\n",
    "        year = date.year\n",
    "        november_first = datetime(year, 11, 1)\n",
    "        days_to_thursday = (3 - november_first.weekday()) % 7\n",
    "        first_thursday = november_first + timedelta(days=days_to_thursday)\n",
    "        thanksgiving = first_thursday + timedelta(weeks=3)\n",
    "\n",
    "        return abs((date - thanksgiving).days) <= 3\n",
    "\n",
    "    def _is_school_term(self, date):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºå­¦æœŸå†…ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        month = date.month\n",
    "        # ç§‹å­£å­¦æœŸ: 9æœˆ-12æœˆä¸­æ—¬ï¼Œæ˜¥å­£å­¦æœŸ: 1æœˆ-5æœˆ\n",
    "        return month in [1, 2, 3, 4, 5, 9, 10, 11] or (month == 12 and date.day <= 15)\n",
    "\n",
    "    def _is_finals_week(self, date):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºæœŸæœ«è€ƒè¯•å‘¨ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        month, day = date.month, date.day\n",
    "        # ç§‹å­£æœŸæœ«: 12æœˆç¬¬äºŒå‘¨ï¼Œæ˜¥å­£æœŸæœ«: 5æœˆç¬¬äºŒå‘¨\n",
    "        return (month == 12 and 8 <= day <= 15) or (month == 5 and 8 <= day <= 15)\n",
    "\n",
    "    def _is_payday(self, date):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºå‘è–ªæ—¥ï¼ˆé€šå¸¸æ˜¯æ¯æœˆ15æ—¥å’Œæœˆæœ«ï¼‰\"\"\"\n",
    "        day = date.day\n",
    "        month_end = (date + timedelta(days=1)).day == 1  # ä¸‹ä¸€å¤©æ˜¯æ–°æœˆç¬¬ä¸€å¤©\n",
    "        return day == 15 or month_end\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. å¤©æ°”ç‰¹å¾å·¥ç¨‹\n",
    "    # ------------------------------------------------------------\n",
    "    def add_weather_features(self, df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ·»åŠ å¤©æ°”ç‰¹å¾\"\"\"\n",
    "        if weather_df is None or len(weather_df) == 0:\n",
    "            print(\"è­¦å‘Š: æ— å¤©æ°”æ•°æ®ï¼Œè·³è¿‡å¤©æ°”ç‰¹å¾\")\n",
    "            return df\n",
    "\n",
    "        df = df.copy()\n",
    "        weather_df = weather_df.copy()\n",
    "\n",
    "        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "\n",
    "        # åˆå¹¶å¤©æ°”æ•°æ®\n",
    "        merged = df.merge(weather_df, on=['date', 'state'], how='left')\n",
    "\n",
    "        # å¡«å……ç¼ºå¤±çš„å¤©æ°”æ•°æ®\n",
    "        merged = merged.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # å¤©æ°”åˆ†ç±»ç‰¹å¾\n",
    "        merged['is_hot'] = (merged['temperature_max'] > 30).astype(int)  # 30Â°Cä»¥ä¸Šä¸ºçƒ­å¤©\n",
    "        merged['is_cold'] = (merged['temperature_max'] < 10).astype(int)  # 10Â°Cä»¥ä¸‹ä¸ºå†·å¤©\n",
    "        merged['is_mild'] = ((merged['temperature_max'] >= 15) & (merged['temperature_max'] <= 25)).astype(int)\n",
    "\n",
    "        merged['is_rainy'] = (merged['precipitation'] > 2).astype(int)  # 2mmä»¥ä¸Šé™æ°´\n",
    "        merged['is_heavy_rain'] = (merged['precipitation'] > 10).astype(int)  # 10mmä»¥ä¸Šå¤§é›¨\n",
    "        merged['is_snowy'] = (merged['snow'] > 0).astype(int)\n",
    "\n",
    "        merged['is_sunny'] = (merged['sunshine_hours'] > 8).astype(int)  # 8å°æ—¶ä»¥ä¸Šæ—¥ç…§\n",
    "        merged['is_windy'] = (merged['wind_speed'] > 20).astype(int)  # 20km/hä»¥ä¸Šå¤§é£\n",
    "\n",
    "        # èˆ’é€‚åº¦æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        merged['comfort_index'] = (\n",
    "            (merged['temperature_mean'] - 20).abs() * (-0.1) +  # è·ç¦»20Â°Cè¶Šè¿œè¶Šä¸èˆ’é€‚\n",
    "            merged['sunshine_hours'] * 0.1 -  # æ—¥ç…§æ—¶é—´è¶Šé•¿è¶Šèˆ’é€‚\n",
    "            merged['precipitation'] * 0.05 -  # é™æ°´è¶Šå¤šè¶Šä¸èˆ’é€‚\n",
    "            merged['wind_speed'] * 0.02  # é£é€Ÿè¶Šå¤§è¶Šä¸èˆ’é€‚\n",
    "        )\n",
    "\n",
    "        # æ¸©åº¦å˜åŒ–ç‰¹å¾\n",
    "        merged = merged.sort_values(['state', 'date'])\n",
    "        merged['temp_change'] = merged.groupby('state')['temperature_mean'].diff()\n",
    "        merged['temp_volatility'] = merged.groupby('state')['temperature_mean'].rolling(7).std().values\n",
    "\n",
    "        return merged.fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. å¢å¼ºçš„å¤„ç†å˜é‡åˆ›å»º\n",
    "    # ------------------------------------------------------------\n",
    "    def create_enhanced_treatment_variables(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å˜é‡\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # åŸæœ‰çš„ä¿ƒé”€å¤„ç†å˜é‡\n",
    "        df['has_promotion'] = (df['total_discount'] > 0).astype(int)\n",
    "        df['promotion_intensity'] = df['total_discount'].astype(float) / (df['total_revenue'].astype(float) + df['total_discount'].astype(float) + 1e-3)\n",
    "        df['has_bogo'] = (df['bogo_orders'] > 0).astype(int)\n",
    "\n",
    "        # æ—¶é—´ç›¸å…³å¤„ç†å˜é‡\n",
    "        df['is_member_day'] = (df['date'].dt.dayofweek == 2).astype(int)  # å‘¨ä¸‰ä¼šå‘˜æ—¥\n",
    "        df['is_peak_day'] = ((df['date'].dt.dayofweek == 5) | (df['date'].dt.dayofweek == 6)).astype(int)  # å‘¨äº”å‘¨å…­\n",
    "\n",
    "        # å¤©æ°”ç›¸å…³å¤„ç†å˜é‡ï¼ˆå¦‚æœæœ‰å¤©æ°”æ•°æ®ï¼‰\n",
    "        if 'temperature_max' in df.columns:\n",
    "            df['extreme_weather'] = (\n",
    "                # (df['is_very_hot'] == 1) |\n",
    "                # (df['is_very_cold'] == 1) |\n",
    "                (df['is_heavy_rain'] == 1) |\n",
    "                (df['is_snowy'] == 1)\n",
    "            ).astype(int)\n",
    "\n",
    "            df['good_weather'] = (\n",
    "                (df['is_mild'] == 1) &\n",
    "                (df['is_sunny'] == 1) &\n",
    "                (df['is_rainy'] == 0)\n",
    "            ).astype(int)\n",
    "\n",
    "        # ç«äº‰å‹åŠ›ï¼ˆåŸºäºè¥æ”¶è¡¨ç°ï¼‰\n",
    "        df['low_performance'] = df.groupby('location_id')['total_revenue'].transform(\n",
    "            lambda x: (x < x.quantile(0.25))#.astype(int)\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. å¤šå› ç´ å› æœåˆ†æ\n",
    "    # ------------------------------------------------------------\n",
    "    def analyze_multi_factor_effects(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æå¤šå› ç´ ï¼ˆä¿ƒé”€ã€å¤©æ°”ã€èŠ‚å‡æ—¥ï¼‰çš„å› æœæ•ˆåº”\"\"\"\n",
    "        print(\"\\n=== å¤šå› ç´ å› æœæ•ˆåº”åˆ†æ ===\\n\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # 1. ä¿ƒé”€æ•ˆåº”åˆ†æ\n",
    "        if 'has_promotion' in df.columns:\n",
    "            promo_results = self._analyze_factor_effect(\n",
    "                df, 'has_promotion', 'ä¿ƒé”€æ´»åŠ¨',\n",
    "                confounders=['is_weekend', 'is_holiday', 'day_of_week', 'unique_customers']\n",
    "            )\n",
    "            results['promotion'] = promo_results\n",
    "\n",
    "        # 2. å¤©æ°”æ•ˆåº”åˆ†æ\n",
    "        if 'is_hot' in df.columns:\n",
    "            weather_results = self._analyze_factor_effect(\n",
    "                df, 'is_hot', 'é«˜æ¸©å¤©æ°”',\n",
    "                confounders=['is_weekend', 'is_holiday', 'day_of_week', 'unique_customers']\n",
    "            )\n",
    "            results['hot_weather'] = weather_results\n",
    "\n",
    "        if 'is_rainy' in df.columns:\n",
    "            rain_results = self._analyze_factor_effect(\n",
    "                df, 'is_rainy', 'é›¨å¤©',\n",
    "                confounders=['is_weekend', 'is_holiday', 'day_of_week', 'temperature_mean']\n",
    "            )\n",
    "            results['rainy_weather'] = rain_results\n",
    "\n",
    "        # 3. èŠ‚å‡æ—¥æ•ˆåº”åˆ†æ\n",
    "        if 'is_holiday' in df.columns:\n",
    "            holiday_results = self._analyze_factor_effect(\n",
    "                df, 'is_holiday', 'èŠ‚å‡æ—¥',\n",
    "                confounders=['day_of_week', 'unique_customers', 'has_promotion']\n",
    "            )\n",
    "            results['holiday'] = holiday_results\n",
    "\n",
    "        # 4. å‘¨æœ«æ•ˆåº”åˆ†æ\n",
    "        weekend_results = self._analyze_factor_effect(\n",
    "            df, 'is_weekend', 'å‘¨æœ«',\n",
    "            confounders=['is_holiday', 'unique_customers', 'has_promotion']\n",
    "        )\n",
    "        results['weekend'] = weekend_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _analyze_factor_effect(self, df: pd.DataFrame, treatment: str, treatment_name: str,\n",
    "                             confounders: List[str], outcome: str = 'total_revenue') -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æå•ä¸ªå› ç´ çš„å› æœæ•ˆåº”\"\"\"\n",
    "        print(f\"\\n--- åˆ†æ {treatment_name} çš„å› æœæ•ˆåº” ---\")\n",
    "\n",
    "        # æ•°æ®é¢„å¤„ç†\n",
    "        analysis_cols = [treatment, outcome] + confounders\n",
    "        clean_df = self._force_numeric(df, analysis_cols).dropna(subset=analysis_cols)\n",
    "\n",
    "        if len(clean_df) < 50:\n",
    "            print(f\"è­¦å‘Š: {treatment_name} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡åˆ†æ\")\n",
    "            return {'error': 'æ•°æ®ä¸è¶³'}\n",
    "\n",
    "        # ä½¿ç”¨EconMLè¿›è¡Œåˆ†æï¼ˆæ›´ç¨³å®šï¼‰\n",
    "        try:\n",
    "            Y = clean_df[outcome].values\n",
    "            T = clean_df[treatment].values.astype(int)\n",
    "            X = clean_df[confounders].values\n",
    "\n",
    "            # åˆ†å‰²æ•°æ®\n",
    "            X_tr, X_te, T_tr, T_te, Y_tr, Y_te = train_test_split(\n",
    "                X, T, Y, test_size=0.2, random_state=42\n",
    "            )\n",
    "\n",
    "            # ä½¿ç”¨LinearDMLï¼ˆæ›´ç¨³å®šï¼‰\n",
    "            ldml = LinearDML(\n",
    "                model_t='auto',\n",
    "                model_y=RandomForestRegressor(n_estimators=100, max_depth=5),\n",
    "                discrete_treatment=True,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            ldml.fit(Y_tr, T_tr, X=X_tr)\n",
    "            ate = float(ldml.ate(X_te))\n",
    "\n",
    "            # è®¡ç®—å¤„ç†ç»„å’Œæ§åˆ¶ç»„çš„åŸºçº¿\n",
    "            treatment_group_revenue = clean_df[clean_df[treatment] == 1][outcome].mean()\n",
    "            control_group_revenue = clean_df[clean_df[treatment] == 0][outcome].mean()\n",
    "            observed_diff = treatment_group_revenue - control_group_revenue\n",
    "\n",
    "            # è®¡ç®—å¤„ç†ç‡\n",
    "            treatment_rate = clean_df[treatment].mean()\n",
    "\n",
    "            print(f\"{treatment_name} åˆ†æç»“æœ:\")\n",
    "            print(f\"  å› æœæ•ˆåº” (ATE): ${ate:.2f}\")\n",
    "            print(f\"  è§‚å¯Ÿåˆ°çš„å·®å¼‚: ${observed_diff:.2f}\")\n",
    "            print(f\"  å¤„ç†ç‡: {treatment_rate:.1%}\")\n",
    "            print(f\"  æ ·æœ¬é‡: {len(clean_df)}\")\n",
    "\n",
    "            return {\n",
    "                'ate': ate,\n",
    "                'observed_diff': observed_diff,\n",
    "                'treatment_rate': treatment_rate,\n",
    "                'sample_size': len(clean_df),\n",
    "                'treatment_group_mean': treatment_group_revenue,\n",
    "                'control_group_mean': control_group_revenue\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{treatment_name} åˆ†æå¤±è´¥: {e}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7. äº¤äº’æ•ˆåº”åˆ†æ\n",
    "    # ------------------------------------------------------------\n",
    "    def analyze_interaction_effects(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æäº¤äº’æ•ˆåº”ï¼ˆå¦‚ï¼šé›¨å¤© Ã— ä¿ƒé”€æ´»åŠ¨ï¼‰\"\"\"\n",
    "        print(\"\\n=== äº¤äº’æ•ˆåº”åˆ†æ ===\\n\")\n",
    "\n",
    "        interactions = {}\n",
    "\n",
    "        # 1. å¤©æ°” Ã— ä¿ƒé”€äº¤äº’æ•ˆåº”\n",
    "        if all(col in df.columns for col in ['is_rainy', 'has_promotion']):\n",
    "            rain_promo = self._analyze_interaction(\n",
    "                df, 'is_rainy', 'has_promotion', 'é›¨å¤©ä¿ƒé”€äº¤äº’æ•ˆåº”'\n",
    "            )\n",
    "            interactions['rain_promotion'] = rain_promo\n",
    "\n",
    "        # 2. é«˜æ¸© Ã— äº§å“ç±»å‹äº¤äº’æ•ˆåº”\n",
    "        if 'is_hot' in df.columns:\n",
    "            hot_cold_drinks = self._analyze_product_weather_interaction(\n",
    "                df, 'is_hot', 'é«˜æ¸©å¤©æ°”å¯¹ä¸åŒäº§å“çš„å½±å“'\n",
    "            )\n",
    "            interactions['hot_weather_products'] = hot_cold_drinks\n",
    "\n",
    "        # 3. èŠ‚å‡æ—¥ Ã— å‘¨æœ«äº¤äº’æ•ˆåº”\n",
    "        if 'is_holiday' in df.columns:\n",
    "            holiday_weekend = self._analyze_interaction(\n",
    "                df, 'is_holiday', 'is_weekend', 'èŠ‚å‡æ—¥å‘¨æœ«äº¤äº’æ•ˆåº”'\n",
    "            )\n",
    "            interactions['holiday_weekend'] = holiday_weekend\n",
    "\n",
    "        return interactions\n",
    "\n",
    "    def _analyze_interaction(self, df: pd.DataFrame, factor1: str, factor2: str,\n",
    "                           interaction_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æä¸¤ä¸ªå› ç´ çš„äº¤äº’æ•ˆåº”\"\"\"\n",
    "        print(f\"\\n--- {interaction_name} ---\")\n",
    "\n",
    "        # åˆ›å»ºäº¤äº’é¡¹\n",
    "        df_temp = df.copy()\n",
    "        interaction_term = f\"{factor1}_x_{factor2}\"\n",
    "        df_temp[interaction_term] = df_temp[factor1] * df_temp[factor2]\n",
    "\n",
    "        # è®¡ç®—å„ç»„åˆçš„å¹³å‡è¥æ”¶\n",
    "        results = {}\n",
    "        for val1 in [0, 1]:\n",
    "            for val2 in [0, 1]:\n",
    "                mask = (df_temp[factor1] == val1) & (df_temp[factor2] == val2)\n",
    "                group_revenue = df_temp[mask]['total_revenue'].mean()\n",
    "                group_size = mask.sum()\n",
    "                results[f\"{factor1}_{val1}_{factor2}_{val2}\"] = {\n",
    "                    'revenue': group_revenue,\n",
    "                    'count': group_size\n",
    "                }\n",
    "\n",
    "        # è®¡ç®—äº¤äº’æ•ˆåº”\n",
    "        # äº¤äº’æ•ˆåº” = (A=1,B=1çš„æ•ˆåº”) - (A=1,B=0çš„æ•ˆåº”) - (A=0,B=1çš„æ•ˆåº”) + (A=0,B=0çš„æ•ˆåº”)\n",
    "        baseline = results[f\"{factor1}_0_{factor2}_0\"]['revenue']\n",
    "        factor1_effect = results[f\"{factor1}_1_{factor2}_0\"]['revenue'] - baseline\n",
    "        factor2_effect = results[f\"{factor1}_0_{factor2}_1\"]['revenue'] - baseline\n",
    "        combined_effect = results[f\"{factor1}_1_{factor2}_1\"]['revenue'] - baseline\n",
    "\n",
    "        interaction_effect = combined_effect - factor1_effect - factor2_effect\n",
    "\n",
    "        print(f\"  {factor1} å•ç‹¬æ•ˆåº”: ${factor1_effect:.2f}\")\n",
    "        print(f\"  {factor2} å•ç‹¬æ•ˆåº”: ${factor2_effect:.2f}\")\n",
    "        print(f\"  äº¤äº’æ•ˆåº”: ${interaction_effect:.2f}\")\n",
    "\n",
    "        return {\n",
    "            'factor1_effect': factor1_effect,\n",
    "            'factor2_effect': factor2_effect,\n",
    "            'interaction_effect': interaction_effect,\n",
    "            'group_details': results\n",
    "        }\n",
    "\n",
    "    def _analyze_product_weather_interaction(self, df: pd.DataFrame, weather_factor: str,\n",
    "                                           analysis_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æå¤©æ°”å¯¹ä¸åŒäº§å“ç±»å‹çš„å½±å“\"\"\"\n",
    "        print(f\"\\n--- {analysis_name} ---\")\n",
    "\n",
    "        product_effects = {}\n",
    "        product_cols = ['cold_drink_orders', 'hot_drink_orders', 'food_orders']\n",
    "\n",
    "        for product in product_cols:\n",
    "            if product in df.columns:\n",
    "                weather_group = df[df[weather_factor] == 1][product].mean()\n",
    "                normal_group = df[df[weather_factor] == 0][product].mean()\n",
    "                effect = weather_group - normal_group\n",
    "\n",
    "                product_effects[product] = {\n",
    "                    'weather_avg': weather_group,\n",
    "                    'normal_avg': normal_group,\n",
    "                    'effect': effect,\n",
    "                    'effect_pct': (effect / normal_group * 100) if normal_group > 0 else 0\n",
    "                }\n",
    "\n",
    "                print(f\"  {product}: {effect:+.1f} ({effect/normal_group*100:+.1f}%)\")\n",
    "\n",
    "        return product_effects\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 8. å¢å¼ºçš„å¯è§†åŒ–\n",
    "    # ------------------------------------------------------------\n",
    "    def visualize_multi_factor_effects(self, results: Dict[str, Any]) -> go.Figure:\n",
    "        \"\"\"å¯è§†åŒ–å¤šå› ç´ æ•ˆåº”å¯¹æ¯”\"\"\"\n",
    "        factors = []\n",
    "        effects = []\n",
    "        colors = []\n",
    "\n",
    "        color_map = {\n",
    "            'promotion': 'blue',\n",
    "            'hot_weather': 'red',\n",
    "            'rainy_weather': 'gray',\n",
    "            'holiday': 'green',\n",
    "            'weekend': 'orange'\n",
    "        }\n",
    "\n",
    "        for factor_name, result in results.items():\n",
    "            if 'ate' in result:\n",
    "                factors.append(factor_name.replace('_', ' ').title())\n",
    "                effects.append(result['ate'])\n",
    "                colors.append(color_map.get(factor_name, 'purple'))\n",
    "\n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=factors,\n",
    "            y=effects,\n",
    "            text=[f\"${v:.0f}\" for v in effects],\n",
    "            textposition='auto',\n",
    "            marker_color=colors\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='å„å› ç´ å¯¹è¥æ”¶çš„å› æœæ•ˆåº”å¯¹æ¯”',\n",
    "            xaxis_title='å½±å“å› ç´ ',\n",
    "            yaxis_title='å¹³å‡å› æœæ•ˆåº” ($)',\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_weather_impact_by_product(self, interaction_results: Dict[str, Any]) -> go.Figure:\n",
    "        \"\"\"å¯è§†åŒ–å¤©æ°”å¯¹ä¸åŒäº§å“çš„å½±å“\"\"\"\n",
    "        if 'hot_weather_products' not in interaction_results:\n",
    "            return None\n",
    "\n",
    "        weather_effects = interaction_results['hot_weather_products']\n",
    "\n",
    "        products = []\n",
    "        effects_pct = []\n",
    "\n",
    "        for product, data in weather_effects.items():\n",
    "            products.append(product.replace('_', ' ').title())\n",
    "            effects_pct.append(data['effect_pct'])\n",
    "\n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=products,\n",
    "            y=effects_pct,\n",
    "            text=[f\"{v:+.1f}%\" for v in effects_pct],\n",
    "            textposition='auto',\n",
    "            marker_color=['red' if v > 0 else 'blue' for v in effects_pct]\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='é«˜æ¸©å¤©æ°”å¯¹ä¸åŒäº§å“é”€é‡çš„å½±å“',\n",
    "            xaxis_title='äº§å“ç±»å‹',\n",
    "            yaxis_title='é”€é‡å˜åŒ– (%)',\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_seasonal_patterns(self, df: pd.DataFrame) -> go.Figure:\n",
    "        \"\"\"å¯è§†åŒ–å­£èŠ‚æ€§æ¨¡å¼\"\"\"\n",
    "        df_temp = df.copy()\n",
    "        df_temp['date'] = pd.to_datetime(df_temp['date'])\n",
    "        df_temp['month'] = df_temp['date'].dt.month\n",
    "\n",
    "        # æŒ‰æœˆä»½èšåˆ\n",
    "        monthly_data = df_temp.groupby('month').agg({\n",
    "            'total_revenue': 'mean',\n",
    "            'order_count': 'mean',\n",
    "            'cold_drink_orders': 'mean',\n",
    "            'hot_drink_orders': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['æœˆåº¦è¥æ”¶', 'æœˆåº¦è®¢å•é‡', 'å†·é¥®è®¢å•', 'çƒ­é¥®è®¢å•']\n",
    "        )\n",
    "\n",
    "        # è¥æ”¶\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=monthly_data['month'], y=monthly_data['total_revenue'],\n",
    "                      mode='lines+markers', name='è¥æ”¶'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # è®¢å•é‡\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=monthly_data['month'], y=monthly_data['order_count'],\n",
    "                      mode='lines+markers', name='è®¢å•é‡'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        # å†·é¥®\n",
    "        if 'cold_drink_orders' in monthly_data.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=monthly_data['month'], y=monthly_data['cold_drink_orders'],\n",
    "                          mode='lines+markers', name='å†·é¥®è®¢å•'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "        # çƒ­é¥®\n",
    "        if 'hot_drink_orders' in monthly_data.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=monthly_data['month'], y=monthly_data['hot_drink_orders'],\n",
    "                          mode='lines+markers', name='çƒ­é¥®è®¢å•'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='å­£èŠ‚æ€§é”€å”®æ¨¡å¼åˆ†æ',\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 9. ç»¼åˆæ´å¯ŸæŠ¥å‘Š\n",
    "    # ------------------------------------------------------------\n",
    "    def generate_comprehensive_insights(self, df: pd.DataFrame,\n",
    "                                      multi_factor_results: Dict[str, Any],\n",
    "                                      interaction_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š\"\"\"\n",
    "        insights = []\n",
    "        insights.append(\"# UMe èŒ¶é¥®é”€å”®å› æœåˆ†ææ´å¯ŸæŠ¥å‘Š\\n\")\n",
    "\n",
    "        # 1. æ€»ä½“æ¦‚è§ˆ\n",
    "        insights.append(\"## ğŸ“Š æ€»ä½“æ¦‚è§ˆ\")\n",
    "        avg_revenue = df['total_revenue'].mean()\n",
    "        avg_orders = df['order_count'].mean()\n",
    "        insights.append(f\"- å¹³å‡æ—¥è¥æ”¶: ${avg_revenue:.0f}\")\n",
    "        insights.append(f\"- å¹³å‡æ—¥è®¢å•: {avg_orders:.0f}å•\")\n",
    "        insights.append(f\"- åˆ†ææœŸé—´: {df['date'].min()} è‡³ {df['date'].max()}\")\n",
    "        insights.append(\"\")\n",
    "\n",
    "        # 2. ä¸»è¦å½±å“å› ç´ \n",
    "        insights.append(\"## ğŸ¯ ä¸»è¦å½±å“å› ç´ \")\n",
    "        sorted_factors = sorted(\n",
    "            [(k, v) for k, v in multi_factor_results.items() if 'ate' in v],\n",
    "            key=lambda x: abs(x[1]['ate']), reverse=True\n",
    "        )\n",
    "\n",
    "        for i, (factor, result) in enumerate(sorted_factors[:5]):\n",
    "            effect = result['ate']\n",
    "            rate = result['treatment_rate']\n",
    "            insights.append(f\"{i+1}. **{factor.replace('_', ' ').title()}**: {effect:+.0f}$ (å‘ç”Ÿç‡: {rate:.1%})\")\n",
    "\n",
    "        insights.append(\"\")\n",
    "\n",
    "        # 3. å…³é”®å‘ç°\n",
    "        insights.append(\"## ğŸ” å…³é”®å‘ç°\")\n",
    "\n",
    "        # ä¿ƒé”€æ•ˆåº”åˆ†æ\n",
    "        if 'promotion' in multi_factor_results and 'ate' in multi_factor_results['promotion']:\n",
    "            promo_effect = multi_factor_results['promotion']['ate']\n",
    "            if promo_effect > 0:\n",
    "                insights.append(f\"- ä¿ƒé”€æ´»åŠ¨å¹³å‡å¸¦æ¥ ${promo_effect:.0f} çš„è¥æ”¶æå‡\")\n",
    "            else:\n",
    "                insights.append(f\"- ä¿ƒé”€æ´»åŠ¨å¯èƒ½å­˜åœ¨è´Ÿé¢æ•ˆåº”ï¼Œè¥æ”¶ä¸‹é™ ${abs(promo_effect):.0f}\")\n",
    "\n",
    "        # å¤©æ°”æ•ˆåº”åˆ†æ\n",
    "        if 'hot_weather' in multi_factor_results and 'ate' in multi_factor_results['hot_weather']:\n",
    "            hot_effect = multi_factor_results['hot_weather']['ate']\n",
    "            insights.append(f\"- é«˜æ¸©å¤©æ°”å¯¹è¥æ”¶çš„å½±å“: {hot_effect:+.0f}$\")\n",
    "\n",
    "        if 'rainy_weather' in multi_factor_results and 'ate' in multi_factor_results['rainy_weather']:\n",
    "            rain_effect = multi_factor_results['rainy_weather']['ate']\n",
    "            insights.append(f\"- é›¨å¤©å¯¹è¥æ”¶çš„å½±å“: {rain_effect:+.0f}$\")\n",
    "\n",
    "        # èŠ‚å‡æ—¥æ•ˆåº”\n",
    "        if 'holiday' in multi_factor_results and 'ate' in multi_factor_results['holiday']:\n",
    "            holiday_effect = multi_factor_results['holiday']['ate']\n",
    "            insights.append(f\"- èŠ‚å‡æ—¥å¸¦æ¥è¥æ”¶æå‡: ${holiday_effect:.0f}\")\n",
    "\n",
    "        insights.append(\"\")\n",
    "\n",
    "        # 4. äº¤äº’æ•ˆåº”æ´å¯Ÿ\n",
    "        if interaction_results:\n",
    "            insights.append(\"## ğŸ”„ äº¤äº’æ•ˆåº”æ´å¯Ÿ\")\n",
    "\n",
    "            if 'rain_promotion' in interaction_results:\n",
    "                rain_promo = interaction_results['rain_promotion']\n",
    "                interaction_effect = rain_promo['interaction_effect']\n",
    "                insights.append(f\"- é›¨å¤©ä¿ƒé”€äº¤äº’æ•ˆåº”: {interaction_effect:+.0f}$\")\n",
    "                if interaction_effect > 0:\n",
    "                    insights.append(\"  â†’ é›¨å¤©è¿›è¡Œä¿ƒé”€æ´»åŠ¨ç‰¹åˆ«æœ‰æ•ˆ\")\n",
    "                else:\n",
    "                    insights.append(\"  â†’ é›¨å¤©ä¿ƒé”€æ•ˆæœä¸å¦‚é¢„æœŸ\")\n",
    "\n",
    "            insights.append(\"\")\n",
    "\n",
    "        # 5. è¡ŒåŠ¨å»ºè®®\n",
    "        insights.append(\"## ğŸ’¡ è¡ŒåŠ¨å»ºè®®\")\n",
    "\n",
    "        # åŸºäºæœ€å¼ºå½±å“å› ç´ ç»™å‡ºå»ºè®®\n",
    "        if sorted_factors:\n",
    "            strongest_factor, strongest_result = sorted_factors[0]\n",
    "            if strongest_result['ate'] > 0:\n",
    "                insights.append(f\"1. **ä¼˜åŒ– {strongest_factor.replace('_', ' ')}**: è¿™æ˜¯æœ€æœ‰æ•ˆçš„è¥æ”¶æå‡æ‰‹æ®µ\")\n",
    "\n",
    "            # åŸºäºå¤©æ°”ç»™å‡ºå»ºè®®\n",
    "            if 'hot_weather' in multi_factor_results:\n",
    "                insights.append(\"2. **å¤©æ°”é€‚åº”æ€§è¥é”€**: æ ¹æ®å¤©æ°”é¢„æŠ¥è°ƒæ•´äº§å“æ¨å¹¿å’Œåº“å­˜\")\n",
    "\n",
    "            # åŸºäºä¿ƒé”€ç»™å‡ºå»ºè®®\n",
    "            if 'promotion' in multi_factor_results:\n",
    "                promo_effect = multi_factor_results['promotion']['ate']\n",
    "                if promo_effect > 0:\n",
    "                    insights.append(\"3. **ä¿ƒé”€ç­–ç•¥ä¼˜åŒ–**: å½“å‰ä¿ƒé”€æœ‰æ•ˆï¼Œå¯ä»¥æ‰©å¤§èŒƒå›´\")\n",
    "                else:\n",
    "                    insights.append(\"3. **é‡æ–°è¯„ä¼°ä¿ƒé”€ç­–ç•¥**: å½“å‰ä¿ƒé”€å¯èƒ½è¿‡åº¦ï¼Œå»ºè®®ç²¾å‡†åŒ–\")\n",
    "\n",
    "        return \"\\n\".join(insights)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 10. å·¥å…·å‡½æ•°\n",
    "    # ------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _force_numeric(df, cols):\n",
    "        \"\"\"å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹\"\"\"\n",
    "        out = df.copy()\n",
    "        for c in cols:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "        return out\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®\n",
    "    CLICKHOUSE_CONFIG = dict(\n",
    "        host=\"clickhouse-0-0.umetea.net\",\n",
    "        port=443,\n",
    "        database=\"dw\",\n",
    "        user=\"ml_ume\",\n",
    "        password=\"hDAoDvg8x552bH\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    WEATHER_API_KEY = \"your-weather-api-key\"  # å®é™…ä½¿ç”¨æ—¶å¡«å…¥çœŸå®API key\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ†æå¼•æ“\n",
    "    enhanced_ci = EnhancedFBRCausalInference(CLICKHOUSE_CONFIG, WEATHER_API_KEY)\n",
    "\n",
    "    # è®¾ç½®åˆ†ææ—¶é—´èŒƒå›´\n",
    "    start_date, end_date = \"2025-06-01\", \"2025-07-31\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"UMe èŒ¶é¥®å¢å¼ºç‰ˆå› æœæ¨æ–­åˆ†æ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. åŠ è½½é”€å”®æ•°æ®\n",
    "    print(\"\\n1ï¸âƒ£ åŠ è½½é”€å”®æ•°æ®...\")\n",
    "    sales_df = enhanced_ci.load_integrated_data(start_date, end_date)\n",
    "    print(f\"åŠ è½½äº† {len(sales_df)} æ¡é”€å”®è®°å½•\")\n",
    "\n",
    "    # 2. è·å–å¤©æ°”æ•°æ®\n",
    "    print(\"\\n2ï¸âƒ£ è·å–å¤©æ°”æ•°æ®...\")\n",
    "    weather_df = enhanced_ci.get_weather_data(start_date, end_date, sales_df)\n",
    "\n",
    "    # 3. æ·»åŠ æ—¥å†ç‰¹å¾\n",
    "    print(\"\\n3ï¸âƒ£ æ·»åŠ æ—¥å†ç‰¹å¾...\")\n",
    "    sales_df = enhanced_ci.add_calendar_features(sales_df)\n",
    "\n",
    "    # 4. åˆå¹¶å¤©æ°”ç‰¹å¾\n",
    "    print(\"\\n4ï¸âƒ£ åˆå¹¶å¤©æ°”ç‰¹å¾...\")\n",
    "    if weather_df is not None:\n",
    "        enhanced_df = enhanced_ci.add_weather_features(sales_df, weather_df)\n",
    "    else:\n",
    "        enhanced_df = sales_df\n",
    "\n",
    "    # 5. åˆ›å»ºå¤„ç†å˜é‡\n",
    "    print(\"\\n5ï¸âƒ£ åˆ›å»ºå¤„ç†å˜é‡...\")\n",
    "    enhanced_df = enhanced_ci.create_enhanced_treatment_variables(enhanced_df)\n",
    "\n",
    "    # 6. å¤šå› ç´ å› æœåˆ†æ\n",
    "    print(\"\\n6ï¸âƒ£ æ‰§è¡Œå¤šå› ç´ å› æœåˆ†æ...\")\n",
    "    multi_factor_results = enhanced_ci.analyze_multi_factor_effects(enhanced_df)\n",
    "\n",
    "    # 7. äº¤äº’æ•ˆåº”åˆ†æ\n",
    "    print(\"\\n7ï¸âƒ£ æ‰§è¡Œäº¤äº’æ•ˆåº”åˆ†æ...\")\n",
    "    interaction_results = enhanced_ci.analyze_interaction_effects(enhanced_df)\n",
    "\n",
    "    # 8. ç”Ÿæˆå¯è§†åŒ–\n",
    "    print(\"\\n8ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–...\")\n",
    "\n",
    "    # å¤šå› ç´ æ•ˆåº”å¯¹æ¯”\n",
    "    fig1 = enhanced_ci.visualize_multi_factor_effects(multi_factor_results)\n",
    "    fig1.show()\n",
    "\n",
    "    # å¤©æ°”å¯¹äº§å“çš„å½±å“\n",
    "    fig2 = enhanced_ci.visualize_weather_impact_by_product(interaction_results)\n",
    "    if fig2:\n",
    "        fig2.show()\n",
    "\n",
    "    # å­£èŠ‚æ€§æ¨¡å¼\n",
    "    fig3 = enhanced_ci.visualize_seasonal_patterns(enhanced_df)\n",
    "    fig3.show()\n",
    "\n",
    "    # 9. ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š\n",
    "    print(\"\\n9ï¸âƒ£ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\")\n",
    "    insights_report = enhanced_ci.generate_comprehensive_insights(\n",
    "        enhanced_df, multi_factor_results, interaction_results\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Š\")\n",
    "    print(\"=\"*80)\n",
    "    print(insights_report)\n",
    "\n",
    "    print(\"\\nâœ… åˆ†æå®Œæˆï¼\")\n",
    "    print(\"ğŸ’¡ å»ºè®®ä¿å­˜åˆ†æç»“æœç”¨äºå†³ç­–å‚è€ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5c2f764525d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
